Publication Type,Authors,Book Authors,Group Authors,Book Group Authors,Researcher Ids,ORCIDs,Book Editors,Author - Arabic,Grant Principal Investigator,Grant Co Principal Investigator,Article Title,Article Title - SciELO,Article Title - SciELO,Article Title - Chinese,Article Title - Russian,Patent Number,Patent Assignee,Source Title - Arabic,Source Title,Source Title - Korean,Book Series Title,Book Series Subtitle,Volume,Issue,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,Version,Version History,DOI,Book DOI,License Name,License URI,License Description,Early Access Date,Supplement,Document Type,Publication Date,Publication Year,Abstract,Abstract - Foreign,Abstract - English Transliteration,Abstract - Foreign,Abstract - Korean,Conference Title,Conference Date,Conference Sponsor,Conference Location,"Times Cited, WoS Core","Times Cited, CSCD","Times Cited, RSCI","Times Cited, ARCI","Times Cited, BCI","Times Cited, SCIELO","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,ISSN,eISSN,ISBN,Grant Number,No of References,Cited References,Language,Advisor,Committee Member,Copyright,Degree Name,Institution Address,Institution,Dissertation and Thesis Subjects,Author Keywords,Indexed Date,UT (Unique ID),Pubmed Id,
J,"Tang, Gui; Tan, Wuzheng; Cai, Mei",,,,,,,,,,Privacy-Preserving and Trustless Verifiable Fairness Audit of Machine Learning Models,,,,,,,,INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS,,,,14,2,,,822,832,,,,,,,,,,,Article,FEB 2023,2023,"in the big data era, machine learning has devel-oped prominently and is widely used in real-world systems. Yet, machine learning raises fairness concerns, which incurs discrimination against groups determined by sensitive attributes such as gender and race. Many researchers have focused on developing fairness audit technique of machine learning model that enable users to protect themselves from discrimination. Existing solutions, however, rely on additional external trust as-sumptions, either on third-party entities or external components, that significantly lower the security. In this study, we propose a trustless verifiable fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. With succinctness and non-interactive of zero knowledge proof, our framework not only guarantees audit integrity, but also clearly enhance security, enabling fair ML models to be publicly auditable and any client to verify audit results without extra trust assumption. Our evaluation on various machine learning models and real-world datasets shows that our framework achieves practical performance.",,,,,,,,,2,0,0,0,0,0,2,,,2158-107X,2156-5570,,,,,,,,,,"Jinan Univ, Coll Cyber Secur, Guangzhou, Peoples R ChinaJinan Univ Lib, Guangzhou, Peoples R China",,,,2023-07-06,WOS:000958420800001,,
J,"Mittal, Surbhi; Thakral, Kartik; Singh, Richa; Vatsa, Mayank; Glaser, Tamar; Canton Ferrer, Cristian; Hassner, Tal",,,,"Vatsa, Mayank/I-5050-2013; Vatsa, Mayank/AAR-7199-2020; Glaser, Tamar/KZU-5891-2024","Vatsa, Mayank/0000-0001-5952-2274; Thakral, Kartik/0000-0002-2528-9950;",,,,,"On responsible machine learning datasets emphasizing fairness, privacy and regulatory norms with examples in biometrics and healthcare",,,,,,,,NATURE MACHINE INTELLIGENCE,,,,6,8,,,936,949,,,,10.1038/s42256-024-00874-y,,,,,AUG 2024,,Article,AUG 2024,2024,"Artificial Intelligence (AI) has seamlessly integrated into numerous scientific domains, catalysing unparalleled enhancements across a broad spectrum of tasks; however, its integrity and trustworthiness have emerged as notable concerns. The scientific community has focused on the development of trustworthy AI algorithms; however, machine learning and deep learning algorithms, popular in the AI community today, intrinsically rely on the quality of their training data. These algorithms are designed to detect patterns within the data, thereby learning the intended behavioural objectives. Any inadequacy in the data has the potential to translate directly into algorithms. In this study we discuss the importance of responsible machine learning datasets through the lens of fairness, privacy and regulatory compliance, and present a large audit of computer vision datasets. Despite the ubiquity of fairness and privacy challenges across diverse data domains, current regulatory frameworks primarily address human-centric data concerns. We therefore focus our discussion on biometric and healthcare datasets, although the principles we outline are broadly applicable across various domains. The audit is conducted through evaluation of the proposed responsible rubric. After surveying over 100 datasets, our detailed analysis of 60 distinct datasets highlights a universal susceptibility to fairness, privacy and regulatory compliance issues. This finding emphasizes the urgent need for revising dataset creation methodologies within the scientific community, especially in light of global advancements in data protection legislation. We assert that our study is critically relevant in the contemporary AI context, offering insights and recommendations that are both timely and essential for the ongoing evolution of AI technologies.There are pervasive concerns related to fairness, privacy and regulatory compliance in machine learning applications in healthcare, necessitating a reevaluation of dataset creation practices. Mittal et al. examine various computer vision datasets, providing insights to foster responsible AI development.",,,,,,,,,6,1,0,0,1,0,7,,,,2522-5839,,,,,,,,,,"IIT Jodhpur, Dept Comp Sci, Jodhpur, IndiaMeta, Menlo Pk, CA USAWeir PBC, Alameda, CA USA",MetaWeir PBC,,,2024-08-17,WOS:001289381300001,,
J,"Ganescu, Bianca-Mihaela; Passerat-Palmbach, Jonathan",,,,"Passerat-Palmbach, Jonathan/AGG-9688-2022",,,,,,Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2402.06414,,,,,,,preprint,Feb 09 2024,2024,"Generative AI, exemplified by models like transformers, has opened up new possibilities in various domains but also raised concerns about fairness, transparency and reliability, especially in fields like medicine and law. This paper emphasizes the urgency of ensuring fairness and quality in these domains through generative AI. It explores using cryptographic techniques, particularly Zero -Knowledge Proofs (ZKPs), to address concerns regarding performance fairness and accuracy while protecting model privacy. Applying ZKPs to Machine Learning models, known as ZKML (Zero -Knowledge Machine Learning), enables independent validation of AIgenerated content without revealing sensitive model information, promoting transparency and trust. ZKML enhances AI fairness by providing cryptographic audit trails for model predictions and ensuring uniform performance across users. We introduce snarkGPT, a practical ZKML implementation for transformers, to empower users to verify output accuracy and quality while preserving model privacy. We present a series of empirical results studying snarkGPT’s scalability and performance to assess the feasibility and challenges of adopting a ZKML-powered approach to capture quality and performance fairness problems in generative AI models.",,,,,,,,,4,0,0,0,0,0,4,,,,,,,,,,,,,,"Imperial Coll London, London, EnglandFlashbots, London, England",Flashbots,,,2024-05-25,PPRN:87605457,,
J,"Ezeife, Enuma",,,,,,,,,,,,,,,,,,International Journal of Multidisciplinary Research and Growth Evaluation,,,,2,1,,,693,701,,,,10.54660/.ijmrge.2021.2.1.693-701,,,,,,,Article,Jan 01 2021,2021,"The integration of Artificial Intelligence (AI) into tax technology is transforming compliance and efficiency in the U.S. tax system. AI-driven business analytics offers a data-driven approach to enhancing tax administration, reducing compliance burdens, and improving fraud detection. This paper presents a business analytics framework that leverages AI technologies such as machine learning, natural language processing (NLP), robotic process automation (RPA), and blockchain to optimize tax compliance and operational efficiency. Machine learning enhances risk assessment by detecting anomalies and predicting tax fraud patterns, enabling proactive audits. NLP-powered AI systems facilitate real-time interpretation of tax regulations and automate taxpayer assistance, improving service delivery. RPA streamlines tax reporting processes, reducing manual errors and processing times, while blockchain enhances the security and transparency of tax transactions. Additionally, AI-driven tax policy simulations support data-driven decision-making for tax reforms and revenue optimization. Despite its potential, AI-driven tax technology faces challenges, including ethical concerns, data privacy risks, and integration complexities with legacy tax systems. Ensuring transparency, accountability, and fairness in AI-based tax enforcement is critical. Regulatory bodies must establish governance frameworks to oversee AI applications while promoting responsible AI adoption in tax administration. This review highlights key policy recommendations, including AI governance structures, public-private collaborations, and investment in AI literacy for tax professionals. By balancing automation with human oversight, AI-driven tax technology can enhance compliance accuracy, reduce costs, and improve taxpayer engagement. As AI continues to evolve, its role in tax compliance and efficiency will be central to shaping the future of digital tax administration in the United States.",,,,,,,,,0,0,0,0,0,0,2,,,,2582-7138,,,,,,,,,,,,,,2025-07-08,RC:124499782_S24,,
J,"Toreini, Ehsan; Mehrnezhad, Maryam; van Moorsel, Aad",,,,"Toreini, Ehsan/HPG-4769-2023","Mehrnezhad, Maryam/0000-0002-4223-6885",,,,,Fairness as a Service (FaaS): verifiable and privacy-preserving fairness auditing of machine learning systems,,,,,,,,INTERNATIONAL JOURNAL OF INFORMATION SECURITY,,,,23,2,,,981,997,,,,10.1007/s10207-023-00774-z,,,,,NOV 2023,,Article,APR 2024,2024,"Providing trust in machine learning (ML) systems and their fairness is a socio-technical challenge, and while the use of ML continues to rise, there is lack of adequate processes and governance practices to assure their fairness. In this paper, we propose FaaS, a novel privacy-preserving, end-to-end verifiable solution, that audits the algorithmic fairness of ML systems. FaaS offers several features, which are absent from previous designs. The FAAS protocol is model-agnostic and independent of specific fairness metrics and can be utilised as a service by multiple stakeholders. FAAS uses zero knowledge proofs to assure the well-formedness of the cryptograms and provenance in the steps of the protocol. We implement a proof of concept of the FaaS architecture and protocol using off-the-shelf hardware, software, and datasets and run experiments to demonstrate its practical feasibility and to analyse its performance and scalability. Our experiments confirm that our proposed protocol is scalable to large-scale auditing scenarios (e.g. over 1000 participants) and secure against various attack vectors.",,,,,,,,,3,0,0,0,0,0,3,,,1615-5262,1615-5270,,,,,,,,,,"Univ Surrey, Guildford, EnglandRoyal Holloway Univ London, Egham, EnglandUniv Birmingham, Birmingham, England",,,,2023-11-17,WOS:001096258900001,,
J,Julien Kiesse Bahangulu; Louis Owusu-Berko,,,,,,,,,,,,,,,,,,World Journal of Advanced Research and Reviews,,,,25,2,,,1746,1763,,,,10.30574/wjarr.2025.25.2.0571,,,,,Feb 2025,,Article,Feb 28 2025,2025,"The widespread adoption of AI-powered business analytics applications has revolutionized decision-making, yet it has also introduced significant challenges related to algorithmic bias, data ethics, and governance. As organizations increasingly rely on machine learning and big data analytics for customer profiling, credit scoring, hiring decisions, and predictive analytics, concerns about fairness, transparency, and compliance have intensified. Algorithmic biases—often stemming from biased training data, flawed model assumptions, and insufficient diversity in datasets—can result in discriminatory outcomes, reinforcing societal inequalities and reputational risks for businesses. To address these concerns, robust data ethics frameworks must be integrated into AI governance strategies. Ethical AI principles emphasize accountability, explainability, and bias mitigation techniques, ensuring that decision-making algorithms are transparent and justifiable. Organizations must implement bias detection methods, fairness-aware machine learning models, and continuous audits to minimize unintended consequences. Additionally, regulatory frameworks such as GDPR, CCPA, and AI-specific compliance laws necessitate stringent governance practices to protect consumer rights and data privacy. Beyond compliance, fostering public trust in AI-powered analytics requires organizations to adopt ethical data stewardship, ensuring that AI models align with corporate social responsibility (CSR) initiatives and stakeholder expectations. The intersection of data ethics, algorithmic accountability, and regulatory compliance presents both challenges and opportunities for businesses seeking to leverage AI responsibly. This paper examines key strategies for mitigating algorithmic bias, establishing ethical AI governance models, and ensuring fairness in data-driven business applications, providing a roadmap for organizations to enhance transparency, compliance, and equitable AI adoption.",,,,,,,,,3,0,0,0,1,0,4,,,,2581-9615,,,,,,,,,,,,,,2025-07-10,RC:147210996_S24,,
B,"Bhanot, Karan",,,,,,,,,,Synthetic Data Generation and Evaluation for Fairness,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2023,2023,,,,,,,,,,0,0,0,0,0,0,0,,,,,9.79838E+12,,,,,,,,,"Rensselaer Polytechnic Institute, Computer Science, New York, United States",Rensselaer Polytechnic Institute,,,,PQDT:85219726,,
J,Oritsematosan Faith Dudu; Olakunle Babatunde Alao; Enoch O. Alonge,,,,,,,,,,,,,,,,,,International Journal of Frontiers in Engineering and Technology Research,,,,7,2,,,1,10,,,,10.53294/ijfetr.2024.7.2.0045,,,,,Nov 2024,,Article,Nov 30 2024,2024,"This paper comprehensively reviews the integration of Artificial Intelligence (AI) into tax compliance processes within fintech ecosystems. It explores the theoretical foundations of AI technologies, such as machine learning and predictive analytics, and how they can automate tax reporting, auditing, and compliance monitoring. Conceptual models for AI-driven tax compliance are proposed, highlighting the potential for increased efficiency, accuracy, and cost reduction. The paper also examines challenges associated with AI adoption, including algorithmic biases, ethical concerns, data privacy issues, and regulatory hurdles. Strategies for overcoming these challenges and fostering broader adoption are discussed. Finally, the paper offers recommendations for fintech companies and policymakers, emphasizing the need for transparent AI models, bias mitigation, data privacy, and updated regulatory frameworks to ensure fair and effective AI-enabled tax systems.",,,,,,,,,0,0,0,0,0,0,0,,,,2783-0497,,,,,,,,,,,,,,2025-07-11,RC:154898202_S24,,
J,"Brusseau, James; Craveiro, Giovana Meloni",,,,,,,,,,"Why automatic AI ethics evaluations are coming, and how they will work",,,,,,,,Journal of AI Robotics & Workplace Automation,,,,1,4,,,342,342,,,,10.69554/pmum8049,,,,,Jun 2022,,Article,Jun 01 2022,2022,"Ethics evaluations of companies that function with AI at their core are increasingly required by regulation and law in Europe and the US. Investors in artificial intelligence (AI)-intensive companies also seek ethics evaluations as part of the nonfinancial information they gather about corporate performance, especially as it relates to privacy and algorithmic fairness. The result is an increasing demand for the evaluations. The costs and time necessary to perform an AI ethics audit, however, are high, even prohibitive. To solve the problem, natural language processing (NLP) and machine learning (ML) can be employed to automate the process. The proposal is that much of the work of AI evaluating can be accomplished more efficiently by machines than by humans. To show how automated ethics reporting may work, this paper describes a project currently underway at Pace University in New York and the University of Trento in Italy. The project endeavours to apply AI to the task of producing AI ethics evaluations.",,,,,,,,,0,0,0,0,0,0,0,,,,2633-5638,,,,,,,,,,"Graduate, University of São Paulo, BrazilPhilosophy and Religious Studies Department, Pace University, One Pace Plaza, New York, NY 10038, United States",Univ Sao PauloPace Univ,,,2025-07-11,RC:154643173_S24,,
C,"Bergman, A. Stevie; Hendricks, Lisa Anne; Rauh, Maribeth; Wu, Boxi; Agnew, William; Kunesch, Markus; Duan, Isabella; Gabriel, Iason; Isaac, William",,,ASSOC COMPUTING MACHINERY,"; Gabriel, Iason/IST-7093-2023","Wu, Boxi/0009-0007-0641-6065; Bergman, Stevie/0000-0002-4331-1357; Gabriel, Iason/0000-0002-7552-4576;",,,,,Representation in AI Evaluations,,,,,,,,"PROCEEDINGS OF THE 6TH ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2023",,,,,,,,519,533,,,,10.1145/3593013.3594019,,,,,,,Proceedings Paper,2023,2023,"Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with representation or representativeness generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be representative? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities.",,,,,"6th ACM Conference on Fairness, Accountability, and Transparency (FAccT)6th ACM Conference on Fairness, Accountability, and Transparency (FAccT)","JUN 12-15, 2023JUN 12-15, 2023",Assoc Comp MachineryAssoc Comp Machinery,"Chicago, ILChicago, IL",10,0,0,0,0,0,11,,,,,978-1-4503-7252-7,,,,,,,,,"DeepMind, New York, NY 10011 USADeepMind, London, EnglandUniv Washington, Seattle, WA USAUniv Chicago, Chicago, IL USA",,,,2023-10-26,WOS:001062819300049,,
J,Agboola Apooyin,,,,,,,,,,,,,,,,,,International Journal of Science and Research Archive,,,,15,1,,,914,926,,,,10.30574/ijsra.2025.15.1.1099,,,,,,,Article,Apr 30 2025,2025,"Artificial Intelligence (AI) is transforming financial reporting and compliance by automating processes, improving accuracy, and enhancing decision-making. AI-driven technologies such as machine learning, natural language processing, and robotic process automation enable real-time data analysis, fraud detection, and regulatory adherence. These innovations reduce human error, streamline operations, and increase transparency in financial reporting. However, AI integration presents challenges, including data security risks, algorithmic bias, and regulatory uncertainties. The lack of standardized AI governance frameworks raises concerns about accountability and ethical decision-making in financial reporting. Additionally, AI-driven financial models may unintentionally reinforce biases, leading to potential compliance violations and reputational risks. Ethical considerations, such as fairness, transparency, and data privacy, must be addressed to ensure responsible AI adoption. Organizations must implement robust AI governance, conduct regular audits, and ensure human oversight in AI-driven financial processes. The role of regulatory bodies is crucial in establishing guidelines that balance innovation with compliance requirements. Despite the challenges, AI offers immense opportunities for financial reporting, including predictive analytics for risk assessment, automated compliance monitoring, and enhanced fraud detection mechanisms. By leveraging AI responsibly, organizations can achieve greater efficiency, accuracy, and compliance in financial reporting. Future research should focus on developing standardized AI regulatory frameworks, mitigating bias in AI-driven financial models, and enhancing AI transparency.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-8185,,,,,,,,,,,,,,2025-07-31,RC:157106257_S24,,
J,"Yuan, Chih-Cheng Rex; Wang, Bow-Yaw",,,,,,,,,,Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2504.21634,,,,,,,preprint,Apr 30 2025,2025,"Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"Acad Sin, Inst Informat Sci, Taipei, Taiwan",Acad Sin,,,2025-08-07,PPRN:123166262,,
J,"Tadi, Venkata",,"Senior Data Analyst, Frisco, Texas, USA",,,,,,,,,,,,,,,,Journal of Artificial Intelligence & Cloud Computing,,,,,,,,1,10,,,,10.47363/jaicc/2024(3)e104,,,,,Apr 2024,,Article,Apr 30 2024,2024,"The integration of generative artificial intelligence (AI) in business-to-business (B2B) sales processes offers significant opportunities for enhanced efficiency, personalization, and predictive capabilities. However, these advancements come with substantial ethical challenges and risks of biases that can undermine trust and fairness in AI-driven interactions. This paper explores the ethical landscape of generative AI in B2B sales, focusing on data privacy, security, transparency, accountability, and informed consent. It examines the sources of bias in AI algorithms, their impact on customer engagement and satisfaction, and the strategies to mitigate these biases. Through a comprehensive review of current literature and case studies, this research highlights the importance of building and maintaining trust in AI systems and ensuring fair treatment of all customers. Insights from industry leaders and proposed future research directions emphasize the need for continuous adaptation and learning in AI ethics. The findings underscore the critical role of ethical AI practices in fostering sustainable and trustworthy B2B sales environments. This paper aims to contribute to the development of ethical frameworks and guidelines that support fair and transparent AI systems, ensuring that the benefits of AI are realized without compromising ethical standards.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,2025-07-11,RC:155248343_S24,,
J,"-, Rachid Ejjami; -, Khaoula Boussalham",,,,,,,,,,,,,,,,,,International Journal For Multidisciplinary Research,,,,6,4,,,,,,,,10.36948/ijfmr.2024.v06i04.25116,,,,,Jul 2024,,Article,Jul 23 2024,2024,"This integrative literature review investigates the transformative impact of artificial intelligence (AI) on supply chain management, addressing the pressing need for efficiency and robustness through AI-driven predictive maintenance, machine learning (ML), and decision support systems. By examining current literature, the study highlights AI's potential to automate and revolutionize supply chain operations, enhancing speed, accuracy, and risk management capabilities while identifying significant challenges such as bias mitigation, algorithmic transparency, and data privacy. The methodology involves a comprehensive review of scholarly articles, reports, and academic publications, focusing on AI applications in predictive maintenance, risk mitigation, and decision-making processes. The analysis reveals significant improvements in operational efficiency and accuracy due to AI, alongside concerns about biases, transparency, and implementation issues. The findings confirm AI's transformative potential in supply chain management but emphasize the necessity for ongoing supervision, regular audits, and the development of AI models capable of detecting and rectifying operational anomalies. The study proposes creating roles such as AI Supply Chain Oversight Officer (AISCO), AI Supply Chain Compliance Officer (AISCCO), and AI Supply Chain Quality Assurance Officer (AISQAO) to ensure responsible AI utilization, maintaining the integrity and efficiency of supply chain operations while addressing implementation challenges. The review concludes that AI is promising for transforming supply chains; however, careful implementation is crucial to uphold operational integrity and resilience. Future research should prioritize longitudinal studies to evaluate AI's long-term impact, focus on addressing implementation concerns, and ensure fair and transparent integration of AI technologies. These findings have significant implications for practice and policy, underscoring the need for robust frameworks and regulatory measures to guide the effective use of AI in supply chains.",,,,,,,,,4,0,0,0,0,0,5,,,,2582-2160,,,,,,,,,,,,,,2025-07-10,RC:147379935_S24,,
J,"Zhang, Jie",,,,,,,,,,,,,,,,,,International Journal of Artificial Intelligence for Science (IJAI4S),,,,1,2,,,,,,,,10.63619/ijai4s.v1i2.004,,,,,,,Article,Jul 11 2025,2025,"As Artificial Intelligence (AI) technologies become increasingly embedded in critical aspects of modern life—ranging from healthcare diagnostics and financial forecasting to autonomous vehicles, law enforcement, education, and national security—the urgency of addressing their ethical implications has grown exponentially. While AI systems offer unprecedented efficiencies and capabilities, they also present significant risks, including algorithmic bias, opaque decisionmaking processes, data exploitation, invasion of privacy, digital surveillance, job displacement, and the amplification of societal inequalities. These risks are particularly acute in high-stakes domains where errors or unchecked use can result in irreversible harm or systemic injustice. This paper offers a comprehensive examination of the evolving ethical landscape surrounding AI development and deployment. It explores foundational ethical principles such as fairness, accountability, transparency, and human-centered design, alongside contemporary challenges introduced by machine learning models, deep learning algorithms, and autonomous decision systems. Special attention is given to the global regulatory landscape, comparing initiatives such as the European Union’s AI Act, the U.S. Blueprint for an AI Bill of Rights, and guidelines from organizations like UNESCO and the OECD. The paper also examines the growing role of interdisciplinary AI ethics teams, algorithmic auditing, and impact assessments. Ultimately, the paper proposes a strategic roadmap for building ethical AI ecosystems grounded in inclusivity, explainability, legal compliance, and social well-being. It emphasizes that aligning AI development with democratic values, human dignity, and global equity is not merely desirable— but essential—for ensuring that the future of AI serves humanity as a whole, rather than a privileged few.",,,,,,,,,0,0,0,0,0,0,0,,,,3067-3593,,,,,,,,,,,,,,2025-08-04,RC:157867911_S24,,
J,"Muhammad, Tayyab; Yaseen, Asad; Shah, Kriya",,,,,,,,,,,,,,,,,,American Journal of Computing and Engineering,,,,7,4,,,35,49,,,,10.47672/ajce.2423,,,,,Sep 2024,,Article,Sep 13 2024,2024,"Purpose: This study examines the transformative role of Artificial Intelligence (AI) in the financial services industry, particularly in the FinTech sector. By exploring AI applications such as personalized banking, fraud detection, credit scoring, and algorithmic trading, the paper analyzes how AI enhances operational efficiency and customer experience.Material and Methods: Using case studies from leading financial institutions, the paper highlights both opportunities and ethical concerns, such as data privacy and algorithmic bias.Findings: The study found that AI enhances detection by analyzing vast datasets to spot suspicious patterns and anomalies that human auditors may miss, improving compliance and reducing financial crime risks. AI streamlines loan underwriting processes by evaluating a broader range of data, such as payment history and social media behavior, providing more accurate risk assessments. The study also revealed that algorithmic trading uses AI to automate and optimize trades at speeds and scales impossible for human traders. AI systems analyze real-time market data and execute trades within milliseconds, capitalizing on fleeting opportunities in the stock market. By incorporating machine learning, these systems can adapt and improve over time, becoming more effective in predicting market trends and managing risk.Implications to Theory, Practice and Policy: It expands the understanding of how AI can reshape financial interactions, enhancing personalization, fraud detection, and credit assessment. From a practical standpoint, it highlights real-world applications, such as robo-advisors and algorithmic trading, offering insights into how institutions can implement AI responsibly. On a policy level, the study underscores the importance of regulatory frameworks addressing data privacy, algorithmic fairness, and transparency, advocating for collaboration between regulators and financial institutions to ensure ethical AI deployment.",,,,,,,,,0,0,0,0,0,0,0,,,,2790-5586,,,,,,,,,,,,,,2025-07-10,RC:144531434_S24,,
J,"Ahmed, Mohamed Mustaf; Okesanya, Olalekan John; Oweidat, Majd; Othman, Zhinya Kawa; Musa, Shuaibu Saidu; Lucero-Prisno Iii, Don Eliseo",,,,"; Musa, Shuaibu/AAW-7559-2021; Lucero-Prisno, Don Eliseo III/LSJ-4602-2024; Othman, Zhinya/JGE-3058-2023; Oweidat, Majd/LPQ-3057-2024; Ahmed, Mohamed/JEP-2444-2023; Olalekan John, Okesanya/HHY-7224-2022","Ahmed, Mohamed Mustaf/0009-0006-5991-4052; Othman, Zhinya Kawa/0009-0008-3914-0867; Okesanya, Olalekan John/0000-0002-3809-4271; Oweidat, Majd/0009-0009-2564-5781",,,,,"The ethics of data mining in healthcare: challenges, frameworks, and future directions",,,,,,,,BIODATA MINING,,,,18,1,,,,,47,,,10.1186/s13040-025-00461-w,,,,,,,Review,JUL 11 2025,2025,"Data mining in healthcare offers transformative insights yet surfaces multilayered ethical and governance challenges that extend beyond privacy alone. Privacy and consent concerns remain paramount when handling sensitive medical data, particularly as healthcare organizations increasingly share patient information with large digital platforms. The risks of data breaches and unauthorized access are stark: 725 reportable incidents in 2023 alone exposed more than 133 million patient records, and hacking-related breaches surged by 239% since 2018. Algorithmic bias further threatens equity; models trained on historically prejudiced data can reinforce health disparities across protected groups. Therefore, transparency must span three levels-dataset documentation, model interpretability, and post-deployment audit logging-to make algorithmic reasoning and failures traceable. Security vulnerabilities in the Internet of Medical Things (IoMT) and cloud-based health platforms amplify these risks, while corporate data-sharing deals complicate questions of data ownership and patient autonomy. A comprehensive response requires (i) dataset-level artifacts such as datasheets, (ii) model-cards that disclose fairness metrics, and (iii) continuous logging of predictions and LIME/SHAP explanations for independent audits. Technical safeguards must blend differential privacy (with empirically validated noise budgets), homomorphic encryption for high-value queries, and federated learning to maintain the locality of raw data. Governance frameworks must also mandate routine bias and robust audits and harmonized penalties for non-compliance. Regular reassessments, thorough documentation, and active engagement with clinicians, patients, and regulators are critical to accountability. This paper synthesizes current evidence, from a 2019 European re-identification study demonstrating 99.98% uniqueness with 15 quasi-identifiers to recent clinical audits that trimmed false-negative rates via threshold recalibration, and proposes an integrated set of fairness, privacy, and security controls aligned with SPIRIT-AI, CONSORT-AI, and emerging PROBAST-AI guidelines. Implementing these solutions will help healthcare systems harness the benefits of data mining while safeguarding patient rights and sustaining public trust.",,,,,,,,,0,0,0,0,0,0,0,,,1756-0381,,,,,,,,,,,"SIMAD Univ, Fac Med & Hlth Sci, Mogadishu, SomaliaUniv Thessaly, Dept Publ Hlth & Maritime Transport, Volos, GreeceChrisland Univ, Dept Med Lab Sci, Ajebo, Abeokuta, NigeriaHebron Univ, Coll Med, Hebron, PalestineKurdistan Tech Inst, Dept Pharm, Sulaimani, Kurdistan Regio, IraqChulalongkorn Univ, Fac Med, Sch Global Hlth, Bangkok, ThailandAhmadu Bello Univ, Dept Nursing Sci, Zaria, NigeriaLondon Sch Hyg & Trop Med, Dept Global Hlth & Dev, London, EnglandCebu Normal Univ, Ctr Res & Dev, Cebu, PhilippinesUniv Makati, Ctr Univ Res, Makati, Philippines",SIMAD UnivChrisland UnivHebron UnivKurdistan Tech InstUniv Makati,,,2025-07-16,WOS:001526601800001,40646553,
J,Areeba Farooq; Anate Benoit Nicaise Abbey; Ekene Cynthia Onukwulu,,,,,,,,,,,,,,,,,,World Journal of Advanced Research and Reviews,,,,24,3,,,2207,2218,,,,10.30574/wjarr.2024.24.3.3961,,,,,Dec 2024,,Article,Dec 30 2024,2024,"Fraud in public assistance programs, such as the Supplemental Nutrition Assistance Program (SNAP) and Electronic Benefit Transfer (EBT), poses significant economic and social challenges, diverting vital resources from vulnerable populations. Traditional fraud detection methods, including manual audits and static rule-based systems, have proven insufficient to address the complexity and adaptability of modern fraudulent schemes. This paper proposes a conceptual framework for AI-powered fraud detection, emphasizing the use of machine learning, anomaly detection, and predictive analytics to combat fraud effectively. The framework addresses systemic challenges, including evolving fraud tactics, sector-specific issues, and technological barriers such as data privacy and scalability. It highlights the core components of AI-driven systems, ensuring interoperability across public assistance programs and e-commerce platforms. Ethical considerations, such as transparency, fairness, and accountability, are integrated into the framework to prevent algorithmic bias and protect beneficiaries' rights. The paper also explores AI adoption's economic and social implications, outlining the potential for cost savings, operational efficiency, and improved equity in benefit distribution. Finally, strategic recommendations are provided to support the ethical design, sector-agnostic deployment, and continuous improvement of AI-based fraud detection systems. By addressing these challenges, this paper aims to contribute to a more efficient, fair, and transparent approach to public resource protection.",,,,,,,,,0,0,0,0,0,0,0,,,,2581-9615,,,,,,,,,,,,,,2025-07-11,RC:154789801_S24,,
J,"El Arab, Rabie Adel; Al Moosa, Omayma Abdulaziz; Albahrani, Zahraa; Alkhalil, Israa; Somerville, Joel; Abuadas, Fuad",,,,"El Arab, Rabie/AEC-8068-2022; Shaban, Mostafa/GLR-7092-2022","El Arab, Rabie Adel/0000-0002-3822-9236;",,,,,"Integrating Artificial Intelligence into Perinatal Care Pathways: A Scoping Review of Reviews of Applications, Outcomes, and Equity",,,,,,,,NURSING REPORTS,,,,15,8,,,,,281,,,10.3390/nursrep15080281,,,,,,,Review,JUL 31 2025,2025,"Background: Artificial intelligence (AI) and machine learning (ML) have been reshaping maternal, fetal, neonatal, and reproductive healthcare by enhancing risk prediction, diagnostic accuracy, and operational efficiency across the perinatal continuum. However, no comprehensive synthesis has yet been published. Objective: To conduct a scoping review of reviews of AI/ML applications spanning reproductive, prenatal, postpartum, neonatal, and early child-development care. Methods: We searched PubMed, Embase, the Cochrane Library, Web of Science, and Scopus through April 2025. Two reviewers independently screened records, extracted data, and assessed methodological quality using AMSTAR 2 for systematic reviews, ROBIS for bias assessment, SANRA for narrative reviews, and JBI guidance for scoping reviews. Results: Thirty-nine reviews met our inclusion criteria. In preconception and fertility treatment, convolutional neural network-based platforms can identify viable embryos and key sperm parameters with over 90 percent accuracy, and machine-learning models can personalize follicle-stimulating hormone regimens to boost mature oocyte yield while reducing overall medication use. Digital sexual-health chatbots have enhanced patient education, pre-exposure prophylaxis adherence, and safer sexual behaviors, although data-privacy safeguards and bias mitigation remain priorities. During pregnancy, advanced deep-learning models can segment fetal anatomy on ultrasound images with more than 90 percent overlap compared to expert annotations and can detect anomalies with sensitivity exceeding 93 percent. Predictive biometric tools can estimate gestational age within one week with accuracy and fetal weight within approximately 190 g. In the postpartum period, AI-driven decision-support systems and conversational agents can facilitate early screening for depression and can guide follow-up care. Wearable sensors enable remote monitoring of maternal blood pressure and heart rate to support timely clinical intervention. Within neonatal care, the Heart Rate Observation (HeRO) system has reduced mortality among very low-birth-weight infants by roughly 20 percent, and additional AI models can predict neonatal sepsis, retinopathy of prematurity, and necrotizing enterocolitis with area-under-the-curve values above 0.80. From an operational standpoint, automated ultrasound workflows deliver biometric measurements at about 14 milliseconds per frame, and dynamic scheduling in IVF laboratories lowers staff workload and per-cycle costs. Home-monitoring platforms for pregnant women are associated with 7-11 percent reductions in maternal mortality and preeclampsia incidence. Despite these advances, most evidence derives from retrospective, single-center studies with limited external validation. Low-resource settings, especially in Sub-Saharan Africa, remain under-represented, and few AI solutions are fully embedded in electronic health records. Conclusions: AI holds transformative promise for perinatal care but will require prospective multicenter validation, equity-centered design, robust governance, transparent fairness audits, and seamless electronic health record integration to translate these innovations into routine practice and improve maternal and neonatal outcomes.",,,,,,,,,0,0,0,0,0,0,0,,,2039-439X,2039-4403,,,,,,,,,,"Almoosa Coll Hlth Sci, Alhsa 36422, Saudi ArabiaAlmoosa Specialist Hosp, Alhsa 36342, Saudi ArabiaUniv Highlands & Isl, Inverness Coll, Inverness IV2 3JH, ScotlandJouf Univ, Coll Nursing, Dept Community Hlth Nursing, Sakaka 72388, Saudi Arabia",Almoosa Specialist Hosp,,,2025-09-01,WOS:001558062300001,40863668,
J,"Lin, Chengkai",,,,,,,,,,,,,,,,,,Advances in Economics Management and Political Sciences,,,,170,1,,,1,6,,,,10.54254/2754-1169/2025.lh24009,,,,,,,Article,Jun 13 2025,2025,"Recent advancements in artificial intelligence (AI) have fundamentally transformed credit management and risk assessment paradigms within the financial sector. Contemporary research demonstrates that machine learning algorithms, particularly deep neural networks, outperform traditional statistical methods by 18-22% in predictive accuracy metrics (F1-score) across credit scoring applications. This performance advantage stems from AI's capacity to process heterogeneous data streams - including transactional records, alternative credit data, and behavioral patterns - through sophisticated feature extraction techniques. However, the implementation of these systems introduces complex operational challenges. Foremost among these is the substantial data requirement: typical risk assessment models now train on datasets exceeding 10 million observations, raising significant concerns regarding GDPR compliance and consumer privacy protections. Equally problematic is the persistence of algorithmic bias, with recent audits revealing demographic disparities exceeding 15% in approval rates for statistically identical applicants. Emerging mitigation strategies employ multi-objective optimization during model training, incorporating fairness constraints alongside accuracy metrics. Technological solutions such as federated learning architectures and homomorphic encryption show particular promise, enabling decentralized model training while maintaining data confidentiality. The field now faces critical questions regarding model interpretability, with regulators increasingly mandating explainable AI (XAI) standards for financial decision systems. Hybrid approaches combining symbolic AI with neural networks represent a promising research direction. These developments suggest that future AI-driven risk management systems must balance three competing priorities: predictive performance, regulatory compliance, and ethical considerations - a challenge that will require close collaboration between data scientists, policymakers, and financial institutions to resolve effectively.",,,,,,,,,0,0,0,0,0,0,0,,,,2754-1177,,,,,,,,,,Fuzhou Overseas Chinese Middle School,Fuzhou Overseas Chinese Middle Sch,,,2025-08-04,RC:157698629_S24,,
J,"Othman, Esam; Mahafdah, Rund",,,,,,,,,,,,,,,,,,Journal of Posthumanism,,,,5,4,,,,,,,,10.63332/joph.v5i4.1091,,,,,,,Article,Apr 13 2025,2025,"Considering the importance of artificial intelligence (AI) in decision-making processes in various fields such as health, law and finance, the concern for bias and fairness of decision making has increased. This paper presents an extensive discussion of bias-aware machine learning(Ml) such as fairness-aware modeling, detection and mitigation. The paper demonstrates aspects of fairness, different forms of algorithmic bias including intersectional bias and how biased systems impact society. The paper turns to appreciation of dentieth, Trust Dynamics, Legal and Regulatory Frameworks And in the Context of Promoting Transparency: Exploring the Role of Explainable AI (XAI). Taking into account the current advances for combatting bias, also pre-processing, in-processing, and post-processing methods, for instance, draw on examples from major domains of interest. Apart from the improvements AIs have achieved, existing challenges involve little attention to relationship among different identities, poor frameworks in place for implementation and operation in other parts of the world, inadequate abuse detection mechanisms among others. Regarding this, we present some of the research questions that focus on the notions of transparency, privacy protected fairness audits, and shared control with the aim of guiding the growth of fair, responsible, and competent AI systems.",,,,,,,,,0,0,0,0,0,0,0,,,,2634-3584,,,,,,,,,,,,,,2025-07-31,RC:157120513_S24,,
J,"Arcolezi, Heber Hwang; Makhlouf, Karima; Palamidessi, Catuscia",,,,"Makhlouf, Karima/HJY-1851-2023; Arcolezi, Héber/AAV-5099-2020",,,,,,Group fairness under obfuscated sensitive information,,,,,,,,JOURNAL OF COMPUTER SECURITY,,,,33,4,,,,,,,,10.1177/0926227X251330212,,,,,MAY 2025,,Article,JUL 2025,2025,"In the era of Big Data, the development of artificial intelligence (AI) systems presents both opportunities and challenges, particularly concerning privacy and fairness. While differential privacy (DP) has emerged as a robust methodology for preserving privacy in real-world applications, its local variant (LDP) specifically addresses trust issues by removing the reliance on a centralized server. Equally critical, conducting fairness audits of AI systems helps identify and mitigate discriminatory outcomes in machine learning. Although the relationship between DP and fairness is inherently multifaceted, this paper offers a detailed empirical examination of how collecting multi-dimensional sensitive attributes under LDP affects fairness in binary classification tasks. Our findings reveal that LDP can slightly improve fairness without substantially degrading model performance-challenging the notion that DP necessarily exacerbates unfairness. We demonstrate these results by evaluating seven state-of-the-art LDP protocols on three benchmark datasets, using established group fairness metrics. Moreover, we propose a novel privacy budget allocation scheme that incorporates varying domain sizes of sensitive attributes, achieving a superior privacy-utility-fairness trade-off compared to existing solutions.",,,,,,,,,0,0,0,0,0,0,0,,,0926-227X,1875-8924,,,,,,,,,,"Univ Grenoble Alpes, Inria Ctr, Grenoble, FranceInria Saclay & Ecole Polytech IPP, Palaiseau, FranceUniv Doha Sci & Technol UDST, Doha, Qatar",,,,2025-05-19,WOS:001488534400001,,
J,"Pelingon, Charlie E.; Pomperada, Jake R; Madrigal, Dennis V",,,,,,,,,,,,,,,,,,Technium Romanian Journal of Applied Sciences and Technology,,,,30,,,,85,96,,,,10.47577/technium.v30i.12954,,,,,,,Article,Jun 17 2025,2025,"The increasing demand for an efficient and data-driven admission process has necessitated the development of eExamPro: A Web-Based Admission and Program Checker with Machine Learning. This study aims to modernize entrance examinations by automating test administration, real-time scoring, and personalized program recommendations using machine learning algorithms, improving decision-making for both students and institutions. The system provides a comprehensive platform for admissions management, integrating a secure offline examination system with a machine learning-driven course recommendation engine. It streamlines applicant evaluation by automating exam distribution, scoring, and result processing, ensuring consistency and reducing human error while offering institutions insights into student performance trends for data-driven decision-making. eExamPro features an adaptive testing mechanism, a personalized course suggestion system, and real-time data visualization tools for admissions officers. It allows applicants to take exams offline, ensuring accessibility. Administrators can monitor exam statistics, program demand, and student performance trends through dynamic dashboards, enabling improved institutional planning. The system's external interface includes a responsive web-based platform with multi-device compatibility. It supports user authentication, secure communication protocols, and seamless integration with institutional databases. Additionally, it provides customizable role-based access for administrators, program heads, and applicants, ensuring data privacy and a structured, efficient admissions workflow. Beyond functional requirements, eExamPro ensures high performance, security, and usability. It incorporates scalable infrastructure, automated backups, data encryption, and access control policies. The system adheres to educational data privacy regulations, maintaining audit trails and compliance measures for secure handling of student and institutional records. In conclusion, eExamPro revolutionizes the admission process, offering an intelligent, automated, and scalable solution for educational institutions. The system enhances efficiency, accuracy, and fairness in entrance examinations and program selection. Future enhancements include expanded AI capabilities for deeper academic analytics and integration with broader student information systems to support long-term academic planning.",,,,,,,,,0,0,0,0,0,0,0,,,,2668-778X,,,,,,,,,,,,,,2025-08-04,RC:157704980_S24,,
J,"Zafar, Imran; Unar, Ahsanullah; Khan, Najeeb Ullah; Abalkhail, Adil; Jamal, Adil",,,,"Zafar, Imran/JCI-6930-2023; UNAR, AHSANULLAH/JAZ-0526-2023",,,,,,Molecular biology in the exabyte era: Taming the data deluge for biological revelation and clinical transformation,,,,,,,,COMPUTATIONAL BIOLOGY AND CHEMISTRY,,,,119,,,,,,108535,,,10.1016/j.compbiolchem.2025.108535,,,,,,,Article,DEC 2025,2025,"The explosive growth in next-generation high-throughput technologies has driven modern molecular biology into the exabyte era, producing an unparalleled volume of biological data across genomics, proteomics, metabolomics, and biomedical imaging. Although this massive expansion of data can power future biological discoveries and precision medicine, it presents considerable challenges, including computational bottlenecks, fragmented data landscapes, and ethical issues related to privacy and accessibility. We highlight novel contributions, such as the application of blockchain technologies to ensure data integrity and traceability, a relatively underexplored solution in this context. We describe how artificial intelligence (AI), machine learning (ML), and cloud computing fundamentally reshape and provide scalable solutions for these challenges by enabling near real-time pattern recognition, predictive modelling, and integrated data analysis. In particular, the use of federated learning models allows privacy-preserving collaboration across institutions. We emphasise the importance of open science, FAIR principles (Findable, Accessible, Interoperable, and Reusable), and blockchainbased audit trails to enhance global collaboration, reproducibility, and data security. By processing multi-omics datasets in integrated formats, we can enhance our understanding of disease mechanisms, facilitate biomarker discovery, and develop AI-assisted, personalised therapeutics. Addressing these technical and ethical demands requires robust governance frameworks that protect sensitive data without hindering innovation. This paper underscores a shift toward more secure, transparent, and collaborative biomedical research, marking a decisive step toward clinical transformation.",,,,,,,,,1,0,0,0,0,0,1,,,1476-9271,1476-928X,,,,,,,,,,"Univ Faisalabad TUF, Fac Sci, Dept Biochem & Biotechnol, Faisalabad 38000, Punjab, PakistanUniv Campania L Vanvitelli, Dept Precis Med, Naples, ItalyUniv Agr, Inst Biotechnol & Genet Engn, Hlth Div, Peshawar 25130, PakistanQassim Univ, Coll Appl Med Sci, Dept Publ Hlth, Buraydah, Saudi Arabia",Univ Faisalabad TUF,,,2025-06-13,WOS:001504873200001,40466336,
J,"Singh, Prabhakar",,,,,,,,,,,,,,,,,,European Journal of Computer Science and Information Technology,,,,13,34,,,25,39,,,,10.37745/ejcsit.2013/vol13n342539,,,,,May 2025,,Article,May 26 2025,2025,"The gig economy's defining characteristics—real-time fulfillment, decentralized operations, and rapid payment cycles—create ideal conditions for sophisticated fraud schemes. This article examines the architectural frameworks and technical approaches required to implement effective AI-driven fraud prevention systems within gig platforms. Through analysis of the unique fraud landscape in gig environments, it explores multi-layered detection methodologies combining rule-based systems, statistical anomaly detection, machine learning classifiers, and graph analytics to identify fraudulent behaviors. The article details key architectural components including stream processing for live data ingestion, hybrid detection approaches, low-latency model serving infrastructure, decision orchestration, and comprehensive audit trails. Using a food delivery platform implementation as a case study, the article illustrates how these components function cohesively to detect and prevent fraud in real-time. Technical challenges including balancing speed with accuracy, ensuring algorithmic fairness, and scaling with platform growth are addressed alongside practical implementation considerations for data persistence, computational resource management, and API design. Finally, emerging technologies including federated identity solutions, behavioral biometrics, explainable AI, and privacy-preserving computation are evaluated for their potential to transform fraud prevention capabilities in gig economy environments",,,,,,,,,0,0,0,0,0,0,0,,,,2054-0965,,,,,,,,,,,,,,2025-08-04,RC:157562078_S24,,
C,"Aalmoes, Jan; Duddu, Vasisht; Boutet, Antoine",,,,,,"Barhamgi, M; Wang, H; Wang, X",,,,On the Alignment of Group Fairness with Attribute Privacy,,,,,,,,"WEB INFORMATION SYSTEMS ENGINEERING-WISE 2024, PT II",,Lecture Notes in Computer Science,,15437,,,,333,348,,,,10.1007/978-981-96-0567-5_24,,,,,,,Proceedings Paper,2025,2025,"Machine learning (ML) models have been adopted for applications with high-stakes decision-making like healthcare and criminal justice. To ensure trustworthy ML models, the new AI regulations (e.g., AI Act) have established several pillars such as privacy, safety and fairness that model design must take into account. Designing such models requires an understanding of the interactions between fairness definitions with different notions of privacy. Specifically, the interaction of group fairness (i.e., protection against discriminatory behaviour across demographic subgroups) with attribute privacy (i.e., resistance to attribute inference attacks-AIAs), has not been comprehensively studied. In this paper, we study in depth, both theoretically and empirically, the alignment of group fairness with attribute privacy in a blackbox setting. We first propose ADAPTAIA, which outperforms existing AIAs on real-world datasets with class imbalances in sensitive attributes. We then show that group fairness theoretically bounds the success of ADAPTAIA, which depends on the choice of fairness metrics (e.g., demographic parity or equalized odds). Through our empirical study, we show that attribute privacy can be achieved from group fairness at no additional cost other than the already existing trade-off with utility. Our work has several implications: i) group fairness acts as a defense against AIAs, which is currently lacking ii) practitioners do not need to explicitly train models for both fairness and privacy to meet regulatory requirements iii) ADAPTAIA can be used for blackbox auditing of group fairness.",,,,,25th International Conference on Web Information Systems Engineering-WISE-Annual25th International Conference on Web Information Systems Engineering-WISE-Annual,"DEC 02-05, 2024DEC 02-05, 2024",,"Doha, QATARDoha, QATAR",0,0,0,0,0,0,0,,,0302-9743,1611-3349,978-981-96-0566-8; 978-981-96-0567-5,,,,,,,,,"Univ Lyon, INSA Lyon, Inria, CITI, Lyon, FranceUniv Waterloo, Waterloo, ON, Canada",,,,2025-08-16,WOS:001534841000024,,
J,"Foalem, Patrick Loic; Silva, Leuson Da; Khomh, Foutse; Li, Heng; Merlo, Ettore",,,,"; Da Silva, Leuson/JCE-1862-2023","Da Silva, Leuson/0000-0002-9086-9038;",,,,,Logging requirement for continuous auditing of responsible machine learning-based applications,,,,,,,,EMPIRICAL SOFTWARE ENGINEERING,,,,30,3,,,,,97,,,10.1007/s10664-025-10656-8,,,,,,,Article,MAY 2025,2025,"Machine learning (ML) is increasingly used across various industries to automate decision-making processes. However, concerns about the ethical and legal compliance of ML models have arisen due to their lack of transparency, fairness, and accountability. Monitoring, particularly through logging, is a widely used technique in traditional software systems that could be leveraged to assist in auditing ML-based applications. Logs provide a record of an application's behavior, which can be used for continuous auditing, debugging, and analyzing both the behavior and performance of the application. In this study, we investigate the logging practices of ML practitioners to capture responsible ML-related information in ML applications. We analyzed 85 ML projects hosted on GitHub, leveraging 20 responsible ML libraries that span principles such as privacy, transparency & explainability, fairness, and security & safety. Our analysis revealed important differences in the implementation of responsible AI principles. For example, out of 5,733 function calls analyzed, privacy accounted for 89.3% (5,120 calls), while fairness represented only 2.1% (118 calls), highlighting the uneven emphasis on these principles across projects. Furthermore, our manual analysis of 44,877 issue discussions revealed that only 8.1% of the sampled issues addressed responsible AI principles, with transparency & explainability being the most frequently discussed principles (32.2% of all issues related to responsible AI principles). Additionally, a survey conducted with ML practitioners provided direct insights into their perspectives, informing our exploration of ways to enhance logging practices for more effective, responsible ML auditing. We discovered that while privacy, model interpretability & explainability, fairness, and security & safety are commonly considered, there is a gap in how metrics associated with these principles are logged. Specifically, crucial fairness metrics like group and individual fairness, privacy metrics such as epsilon and delta, and explainability metrics like SHAP values are not considered current logging practices. The insights from this study highlight the need for ML practitioners and logging tool developers to adopt enhanced logging strategies that incorporate a broader range of responsible AI metrics. This adjustment will facilitate the development of auditable and ethically responsible ML applications, ensuring they meet emerging regulatory and societal expectations. These specific insights offer actionable guidance for improving the accountability and trustworthiness of ML systems.",,,,,,,,,0,0,0,0,0,0,0,,,1382-3256,1573-7616,,,,,,,,,,"Polytech Montreal, Dept Comp Engn & Software Engn, Montreal, PQ, Canada",,,,2025-04-19,WOS:001466425200003,,
J,"Stewart, Matthew; Warden, Pete; Omri, Yasmine; Prakash, Shvetank; Santos, Joao; Hymel, Shawn; Brown, Benjamin; Macarthur, Jim; Jeffries, Nat; Katti, Sachin; Plancher, Brian; Reddi, Vijay Janapa",,,,"Santos, Joao/B-6135-2009; Plancher, Brian/HKW-7966-2023",,,,,,"Datasheets for Machine Learning Sensors: Towards Transparency, Auditability, and Responsibility for Intelligent Sensing",,,,,,,,Arxiv,,,,,,,,,,,3,,arXiv:2306.08848,,,,,,,preprint,Feb 17 2024,2024,"Machine learning (ML) sensors are enabling intelligence at the edge by empowering end-users with greater control over their data. ML sensors offer a new paradigm for sensing that moves the processing and analysis to the device itself rather than relying on the cloud, bringing benefits like lower latency and greater data privacy. The rise of these intelligent edge devices, while revolutionizing areas like the internet of things (IoT) and healthcare, also throws open critical questions about privacy, security, and the opacity of AI decision-making. As ML sensors become more pervasive, it requires judicious governance regarding transparency, accountability, and fairness. To this end, we introduce a standard datasheet template for these ML sensors and discuss and evaluate the design and motivation for each section of the datasheet in detail including: standard dasheet components like the system’s hardware specifications, IoT and AI components like the ML model and dataset attributes, as well as novel components like end-to-end performance metrics, and expanded environmental impact metrics. To provide a case study of the application of our datasheet template, we also designed and developed two examples for ML sensors performing computer vision-based person detection: one an open-source ML sensor designed and developed in-house, and a second commercial ML sensor developed by our industry collaborators. Together, ML sensors and their datasheets provide greater privacy, security, transparency, explainability, auditability, and user-friendliness for ML-enabled embedded systems. We conclude by emphasizing the need for standardization of datasheets across the broader ML community to ensure the responsible use of sensor data.",,,,,,,,,1,0,0,0,0,0,1,,,,,,,,,,,,,,"Harvard Univ, Cambridge, MA 02138, USAStanford Univ, Stanford, CA 94305, USAEdge Impulse, San Jose, CA, USAColumbia Univ, Barnard Coll, New York City, NY 10027, USA",Harvard UnivEdge ImpulseColumbia Univ,,,2024-03-16,PPRN:73385232,,
J,Mark Joseph Bayona,,,,,,,,,,,,,,,,,,World Journal of Advanced Research and Reviews,,,,25,2,,,2011,2057,,,,10.30574/wjarr.2025.25.2.0514,,,,,Feb 2025,,Article,Feb 28 2025,2025,"AI-driven recruitment systems offer improved hiring outcomes and enhanced efficiency, which transform traditional recruitment methods. Public sector implementation of AI systems raises ethical concerns around transparency, fairness, accountability, and data privacy, crucial in the maintenance of public trust. Bias in AI presents risks to equitable hiring, highlighting the need for regular audits and bias-detection tools. The study proposes a framework for ethical considerations to guide just and fair AI-driven recruitment in the Philippine public sector, emphasizing diversity, social equity, and trust in public service. Ethical considerations ensure adherence to societal values and operational efficiency in public sector hiring.The study adopts Descriptive Research Design to describe the sample characteristics and area of interest. Quantitative research techniques analyzed the survey data to identify correlations between ethical considerations and effective implementation of AI-driven recruitment systems in the public sector. A survey questionnaire was used as the primary data-gathering instrument, which was tested for validity and reliability. Respondents of the study are government agency employees and HR professionals, selected by Purposive Sampling Design. Statistical tools used are percentage, frequency, mean, and standard deviation. Pearson Correlation Analysis was used to measure the significant impact of key ethical considerations on the effectiveness of the potential implementation of AI-Driven recruitment systems in the public sector. Multiple Regression Analysis measured which of the key ethical considerations significantly achieve effectiveness in the potential implementation of AI-Driven recruitment systems in the public sector.Bias, transparency, accountability, fairness, and diversity are key ethical considerations in AI-driven recruitment systems in the public sector. Bias, transparency, and diversity were found significant in the effective achievement of AI-driven recruitment systems. Accountability and fairness were not significant in AI recruitment implementation. As diversity in the Philippines is highly pronounced, considerations on bias become essential to ensure support of AI systems for equitable public service delivery. Through bias mitigation, the Philippine government can effectively promote a fair, diverse, and trusted hiring process, vital in the creation of representative and capable workforce serving the best interests of the public. Transparency is crucial for achieving success in AI-driven recruitment implementation in the Philippine public sector, specifically in the promotion of accountability, public trust, and in providing support for legal compliance, fairness, and adaptability, thus creating a robust framework for effective and ethical AI recruitment practices. The significance of transparency in the Philippine public sector lies in ensuring fairness, fostering of trust, and alignment with regulatory and ethical standards, crucial for legitimacy and accountability of the public sector. In the effective implementation of AI-driven recruitment in the public sector, diversity was found as critical factor, contributing to the promotion of social equity, inclusivity, and fairness in hiring practices. Embracing diversity ensures the alignment of diversity with the government’s mandate for the provision of equal employment opportunities and reflection of the country’s diverse linguistic, cultural, and socioeconomic backgrounds. The Philippine public sector has lower perceptions of the direct impact of accountability on effective AI-driven recruitment systems, as they put priority to efficiency, reliance on legal standards, distributed nature of responsibility, and cultural trust, lessening the immediate need for stringent measures of accountability. Accountability is believed to play a supportive rather than a central role in determining the effectiveness of AI-driven recruitment. The perspective was found not to entirely negate accountability, but rather to contextualize the relative impact on practical outcomes of implementation. AI-driven recruitment systems’ technical complexity, involving the opaque nature of machine learning models, limits traditional accountability measures, making it challenging to assign individual accountability for specific recruitment outcomes. Fairness is seen as non-significant to the effective AI-driven recruitment systems in the Philippine public sector, as efficiency, objective data, resource constraints, transparency, accountability, short-term recruitment outcomes, and merit-based selection, can take precedence over considerations of fairness. The perspective is assumed to balance societal and ethical goals for ensuring a holistic approach in public sector hiring. As the public sector considers operational efficiency as the primary measure of success, fairness becomes secondary in the achievement of recruitment objectives.",,,,,,,,,0,0,0,0,0,0,0,,,,2581-9615,,,,,,,,,,,,,,2025-07-10,RC:147072950_S24,,
J,"Arcolezi, Heber H.; Alishahi, Mina; Bendoukha, Adda-Akram; Kaaniche, Nesrine",,,,"Kaaniche, Nesrine/KZU-4792-2024; Arcolezi, Héber/AAV-5099-2020",,,,,,"Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness",,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2505.07985,,,,,,,preprint,May 12 2025,2025,"Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to four orders of magnitude. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.",,,,,,,,,1,0,0,0,0,0,1,,,,,,,,,,,,,,"Univ Grenoble Alpes, Inria Ctr, Grenoble, FranceOpen Univ, Heerlen, NetherlandsInst Polytech Paris, Samovar, Telecom SudParis, Paris, France",Open UnivInst Polytech Paris,,,2025-08-07,PPRN:123343275,,
C,"Givens, Alexandra Reeve; Morris, Meredith Ringel",,,Assoc Comp Machinery,,,,,,,"Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency",,,,,,,,"FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY",,,,,,,,684,684,,,,10.1145/3351095.3375686,,,,,,,Proceedings Paper,2020,2020,,,,,,"ACM Conference on Fairness, Accountability, and Transparency (FAT)ACM Conference on Fairness, Accountability, and Transparency (FAT)","JAN 27-30, 2020JAN 27-30, 2020",Assoc Comp MachineryAssoc Comp Machinery,"Barcelona, SPAINBarcelona, SPAIN",7,0,0,0,0,0,9,,,,,978-1-4503-6936-7,,,,,,,,,"Georgetown Univ, Inst Tech Law & Policy, Washington, DC 20007 USAMicrosoft Res, Redmond, WA USA",,,,2021-03-02,WOS:000620151400083,,
C,"Lycklama, Hidde; Viand, Alexander; Kuchler, Nicolas; Knabenhans, Christian; Hithnawi, Anwar",,,USENIX,,,,,,,Holding Secrets Accountable: Auditing Privacy-Preserving Machine Learning,,,,,,,,"PROCEEDINGS OF THE 33RD USENIX SECURITY SYMPOSIUM, SECURITY 2024",,,,,,,,1975,1992,,,,,,,,,,,Proceedings Paper,2024,2024,"Recent advancements in privacy-preserving machine learning are paving the way to extend the benefits of ML to highly sensitive data that, until now, has been hard to utilize due to privacy concerns and regulatory constraints. Simultaneously, there is a growing emphasis on enhancing the transparency and accountability of ML, including the ability to audit deployments for aspects such as fairness, accuracy and compliance. Although ML auditing and privacy-preserving machine learning have been extensively researched, they have largely been studied in isolation. However, the integration of these two areas is becoming increasingly important. In this work, we introduce Arc, an MPC framework designed for auditing privacy-preserving machine learning. Arc cryptographically ties together the training, inference, and auditing phases to allow robust and private auditing. At the core of our framework is a new protocol for efficiently verifying inputs against succinct commitments. We evaluate the performance of our framework when instantiated with our consistency protocol and compare it to hashing-based and homomorphic-commitment-based approaches, demonstrating that it is up to 104x faster and up to 106x more concise.",,,,,33rd USENIX Security Symposium33rd USENIX Security Symposium,"AUG 14-16, 2024AUG 14-16, 2024",USENIX; Bloomberg Engn; Futurewei Technologies; Google; NSF; TikTok; Ant Res; IBM; Meta; Microsoft; Technol Innovat Inst; Paloalto Network; QualcommUSENIX; Bloomberg Engn; Futurewei Technologies; Google; NSF; TikTok; Ant Res; IBM; Meta; Microsoft; Technol Innovat Inst; Paloalto Network; Qualcomm,"Philadelphia, PAPhiladelphia, PA",0,0,0,0,0,0,0,,,,,978-1-939133-44-1,,,,,,,,,"Swiss Fed Inst Technol, Zurich, SwitzerlandIntel Labs, Hillsboro, OR USAEcole Polytech Fed Lausanne, Lausanne, Switzerland",,,,2025-02-23,WOS:001333860302020,,
J,"Haley, Patricia",,,,,,,,,,,,,,,,,,Health Economics and Management Review,,,,6,2,,,32,49,,,,10.61093/hem.2025.2-03,,,,,Jul 2025,,Article,Jul 02 2025,2025,"This study provides a critical examination of AI-integrated speed and red-light camera systems through the theoretical lenses of Surveillance Capitalism, the Panopticon Model, Social Control Theory, Technological Determinism, and Structural Violence Theory. While artificial intelligent speed safety cameras demonstrate efficacy in reducing traffic violations and fatalities, this research addresses a critical gap in healthcare literature regarding their broader societal and ethical consequences, including algorithmic bias, data governance failures, and privacy violations that directly impact public trust and health equity. The analysis reveals how machine learning and predictive analytics in automated enforcement create disproportionate burdens on marginalized populations through three specific mechanisms: (1) biased algorithmic design that targets low-income neighborhoods more intensively, (2) punitive traffic fine structures that impose greater relative financial hardship on economically disadvantaged families, and (3) opaque implementation practices that limit community understanding and participation. These patterns perpetuate health disparities by increasing chronic stress, economic instability, and barriers to healthcare access among vulnerable populations. This work’s novel contribution lies in applying four foundational health equity principles to AI-powered traffic enforcement: distributive justice (fair allocation of enforcement across communities), procedural justice (transparent and accountable decision-making processes), recognition justice (acknowledgment of community voices and concerns), and capabilities approach (ensuring enforcement practices do not undermine individuals’ fundamental capabilities for health and wellbeing). Additionally, the study examines three core social justice principles: substantive equality (addressing systemic disadvantages rather than treating all violations identically), participatory parity (ensuring affected communities can participate meaningfully in policy decisions), and non-domination (preventing the arbitrary exercise of state power through automated systems). The study advocates for the development of ethical artificial intelligence governance frameworks that incorporate transparent algorithmic auditing, community driven design processes, and robust oversight mechanisms. These evidence-based recommendations support equitable and trustworthy applications of artificial intelligence that advocate for, rather than undermine, population health and social justice in traffic safety initiatives. A novel contribution of this work lies in its exploration of how artificial intelligence powered speed safety cameras intersect with specific health equity principles in distributive justice, procedural justice, and the capabilities approach, as well as core social justice principles, including substantive equality, participatory parity, and nondomination, in the governance of public infrastructure. The analysis applies distributive justice to examine the fair allocation of enforcement across communities, procedural justice to evaluate transparent decision-making processes, and the capabilities approach to assess whether enforcement practices undermine individuals’ fundamental capabilities for health and wellbeing.",,,,,,,,,0,0,0,0,0,0,0,,,,2786-4634,,,,,,,,,,,,,,2025-08-04,RC:157869370_S24,,
B,"Huang, Yangsibo",,,,,,,,,,"Quantifying, Evaluating, and Mitigating Risks in Modern Machine Learning Systems",,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2025,2025,,,,,,,,,,0,0,0,0,0,0,0,,,,,9.79829E+12,,,,,,,,,"Princeton University, Electrical and Computer Engineering, New Jersey, United States",Princeton University,,,,PQDT:160248373,,
C,"Toreini, Ehsan; Mehrnezhad, Maryam; van Moorsel, Aad",,,,"Toreini, Ehsan/HPG-4769-2023","van Moorsel, Aad/0000-0001-7233-6943; Toreini, Ehsan/0000-0002-5172-2957; Mehrnezhad, Maryam/0000-0002-4223-6885","Katsikas, S; Abie, H; Ranise, S; Verderame, L; Cambiaso, E; Ugarelli, R; Praca, I; Li, W; Meng, W; Furnell, S; Katt, B; Pirbhulal, S; Shukla, A; Ianni, M; Preda, MD; Choo, KKR; Correia, MP; Abhishta, A; Sileno, G; Alishahi, M; Kalutarage, H; Yanai, N",,,,Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems,,,,,,,,"COMPUTER SECURITY. ESORICS 2023 INTERNATIONAL WORKSHOPS, CPS4CIP, PT II",,Lecture Notes in Computer Science,,14399,,,,569,584,,,,10.1007/978-3-031-54129-2_34,,,,,,,Proceedings Paper,2024,2024,"Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model-agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.",,,,,28th European Symposium on Research in Computer Security (ESORICS)28th European Symposium on Research in Computer Security (ESORICS),"SEP 25-29, 2023SEP 25-29, 2023",,"The Hague, NETHERLANDSThe Hague, NETHERLANDS",0,0,0,0,0,0,0,,,0302-9743,1611-3349,978-3-031-54128-5; 978-3-031-54129-2,,,,,,,,,"Univ Surrey, Guildford, Surrey, EnglandRoyal Holloway Univ London, Egham, Surrey, EnglandUniv Birmingham, Birmingham, W Midlands, England",,,,2024-05-23,WOS:001212380000024,,
B,"Franzese, Olive",,,,,,,,,,Cryptographic Verification of Trustworthy Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2024,2024,,,,,,,,,,0,0,0,0,0,0,0,,,,,9.79835E+12,,,,,,,,,"Northwestern University, Computer Science, Illinois, United States",Northwestern University,,,,PQDT:120087463,,
C,"Park, Saerom; Kim, Seongmin; Lim, Yeon-sup",,,ACM,"; Kim, Min/ACN-6827-2022","Kim, Seongmin/0000-0002-8183-0641;",,,,,Fairness Audit of Machine Learning Models with Confidential Computing,,,,,,,,PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22),,,,,,,,3488,3499,,,,10.1145/3485447.3512244,,,,,,,Proceedings Paper,2022,2022,"Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-ofthe-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.",,,,,31st ACM Web Conference (WWW)31st ACM Web Conference (WWW),"APR 25-29, 2022APR 25-29, 2022",Assoc Comp Machinery; ACM SIGWEB; LIRIS; Univ Lyon; Inst Natl Sci Appliquees; Eurecom; Int World Wide Web Conf CommAssoc Comp Machinery; ACM SIGWEB; LIRIS; Univ Lyon; Inst Natl Sci Appliquees; Eurecom; Int World Wide Web Conf Comm,ELECTR NETWORKELECTR NETWORK,13,0,0,0,0,0,15,,,,,978-1-4503-9096-5,,,,,,,,,"Sungshin Womens Univ, Seoul, South Korea",,,,2022-10-27,WOS:000852713003057,,
B,"Dean, Sarah Ankaret Anderson",,,,,,,,,,Reliable Machine Learning in Feedback Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2021,2021,,,,,,,,,,0,0,0,0,0,0,0,,,,,9.79838E+12,,,,,,,,,"University of California, Berkeley, Electrical Engineering & Computer Sciences, California, United States","University of California, Berkeley",,,,PQDT:87747990,,
J,"Weng, Jiasi; Weng, Jian; Zhang, Jilian; Li, Ming; Zhang, Yue; Luo, Weiqi",,,,,"Zhang, Yue/0000-0002-7786-0231",,,,,DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-Based Incentive,,,,,,,,IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING,,,,18,5,,,2438,2455,,,,10.1109/TDSC.2019.2952332,,,,,,,Article,SEPT 1 2021,2021,"Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this article, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a prototype of DeepChain and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.",,,,,,,,,389,39,0,0,7,0,441,,,1545-5971,1941-0018,,,,,,,,,,"Jinan Univ, Coll Informat Sci & Technol, Guangzhou 510632, Peoples R ChinaGuangdong Guangzhou Key Lab Data Secur & Privacy, Guangzhou 510632, Peoples R ChinaNatl Local Joint Engn Res Ctr Cyber Secur Detect, Guangzhou 510632, Peoples R China",Guangdong Guangzhou Key Lab Data Secur & PrivacyNatl Local Joint Engn Res Ctr Cyber Secur Detect,,,2021-09-09,WOS:000690440600028,,
J,"Li, Yueqi; Goel, Sanjay",,,,,,,,,,Artificial intelligence auditability and auditor readiness for auditing artificial intelligence systems,,,,,,,,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,,,,56,,,,,,100739,,,10.1016/j.accinf.2025.100739,,,,,JAN 2025,,Article,DEC 2025,2025,"As the business community races to implement artificial intelligence (AI), there are several challenges that need to be addressed such as fairness and biases, transparency, denial of individual rights, and dilution of privacy. AI audits are expected to ensure that AI systems function lawfully, robustly, and follow ethical standards (e.g., fairness). While the auditability for financial audits and information system audits has been well addressed in the literature, auditability of AI systems has not been sufficiently addressed. AI auditability and auditors' competencies are crucial for ensuring AI audits are conducted with high quality. Research on the auditability of AI and the competencies of AI auditors is gravely lacking leaving risks in AI systems unmitigated. The primary reason is that the field is nascent and the rapid growth has left the audit profession struggling to catch up. Foundational work on establishing parameters for such research would help advance this research. In this paper, we explore AI auditability measures and competencies required for conducting AI audits. We conducted semi-structured interviews with 23 experienced AI professionals who have direct involvement or indirect exposure to AI audits. Based on our findings, we propose a framework of AI auditability and identify the competencies required to conduct AI audits. Our study serves as the first formal attempt to systematically identify and classify auditability measures and auditors' expertise demanded for AI audits based on practitioners' perspectives. Our findings contribute to the AI audit literature, inform AI developers about implementing auditability, guide the training of new AI auditors, and establish a foundation for further research in the field.",,,,,,,,,4,0,0,0,0,0,5,,,1467-0895,1873-4723,,,,,,,,,,"Skidmore Coll, Management & Business Dept, 815 N Broadway, Saratoga Springs, NY 12866 USASUNY Albany, Sch Business, Dept Informat Secur & Digital Forens, 1400 Washington Ave, Albany, NY 12222 USA",,,,2025-02-20,WOS:001416804600001,,
J,"Ferry, Julien; Aivodji, Ulrich; Gambs, Sebastien; Huguet, Marie-Jose; Siala, Mohamed",,,,,,,,,,"Taming the Triangle: On the Interplays Between Fairness, Interpretability, and Privacy in Machine Learning",,,,,,,,COMPUTATIONAL INTELLIGENCE,,,,41,4,,,,,e70113,,,10.1111/coin.70113,,,,,,,Article,AUG 6 2025,2025,"Machine learning techniques are increasingly used for high-stakes decision-making, such as college admissions, loan attribution, or recidivism prediction. Thus, it is crucial to ensure that the models learnt can be audited or understood by human users, do not create or reproduce discrimination or bias and do not leak sensitive information regarding their training data. Indeed, interpretability, fairness, and privacy are key requirements for the development of responsible machine learning, and all three have been studied extensively during the last decade. However, they were mainly considered in isolation, while in practice they interplay with each other, either positively or negatively. In this survey paper, we review the literature on the interactions between these three desiderata. More precisely, for each pairwise interaction, we summarize the identified synergies and tensions. These findings highlight several fundamental theoretical and empirical conflicts, while also demonstrating that jointly considering these different requirements is challenging when one aims at preserving a high level of utility. To solve this issue, we also discuss possible conciliation mechanisms, showing that a careful design can enable to successfully handle these different concerns in practice.",,,,,,,,,1,0,0,0,0,0,1,,,0824-7935,1467-8640,,,,,,,,,,"Polytech Montreal, CIRRELT & SCALE AI Chair Data Driven Supply Chains, Dept Math & Ind Engn, Montreal, PQ, CanadaEcole Technol Super, Montreal, PQ, CanadaUniv Quebec Montreal, Montreal, PQ, CanadaUniv Toulouse, CNRS, LAAS CNRS, INSA, Toulouse, France",,,,2025-08-10,WOS:001544676800001,,
J,"Ferry, Julien; Aivodji, Ulrich; Gambs, Sebastien; Huguet, Marie-Jose; Siala, Mohamed",,,,"SIALA, Mohamed/ACF-7293-2022",,,,,,"SoK: Taming the Triangle - On the Interplays between Fairness, Interpretability and Privacy in Machine Learning",,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2312.16191,,,,,,,preprint,Dec 22 2023,2023,"Machine learning techniques are increasingly used for high-stakes decision-making, such as college admissions, loan attribution or recidivism prediction. Thus, it is crucial to ensure that the models learnt can be audited or understood by human users, do not create or reproduce discrimination or bias, and do not leak sensitive information regarding their training data. Indeed, interpretability, fairness and privacy are key requirements for the development of responsible machine learning, and all three have been studied extensively during the last decade. However, they were mainly considered in isolation, while in practice they interplay with each other, either positively or negatively. In this Systematization of Knowledge (SoK) paper, we survey the literature on the interactions between these three desiderata. More precisely, for each pairwise interaction, we summarize the identified synergies and tensions. These findings highlight several fundamental theoretical and empirical conflicts, while also demonstrating that jointly considering these different requirements is challenging when one aims at preserving a high level of utility. To solve this issue, we also discuss possible conciliation mechanisms, showing that a careful design can enable to successfully handle these different concerns in practice.",,,,,,,,,1,0,0,0,0,0,1,,,,,,,,,,,,,,"Univ Toulouse, LAAS CNRS, CNRS, Toulouse, FranceEcole Technol Super, Montreal, PQ, CanadaUniv Quebeca Montreal, Montreal, PQ, Canada",Univ ToulouseUniv Quebeca Montreal,,,2024-01-04,PPRN:86851072,,
J,"Toreini, Ehsan; Mehrnezhad, Maryam; Van Moorsel, Aad",,,,"Toreini, Ehsan/HPG-4769-2023",,,,,,Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2309.06061,,,,,,,preprint,Sep 12 2023,2023,"Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model–agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"Univ Surrey, Guildford, EnglandRoyal Holloway Univ London, Egham, EnglandBirmingham Univ, Birmingham, England",Royal Holloway Univ LondonBirmingham Univ,,,2024-05-25,PPRN:84979554,,
J,"Kumar, Chandan",,,,,,,,,,,,,,,,,,RESEARCH REVIEW International Journal of Multidisciplinary,,,,10,4,,,63,81,,,,10.31305/rrijm.2025.v10.n4.009,,,,,,,Article,Apr 20 2025,2025,"The integration of Artificial Intelligence (AI) in Human Resource Management (HRM) is revolutionising workforce management by automating recruitment, performance evaluations, and employee engagement processes. However, AI-driven HRM systems raise critical ethical concerns, particularly regarding bias, privacy, and transparency. This study explores the ethical implications of AI adoption in HRM, with a specific focus on the power sector, where automation plays a crucial role in workforce optimisation. The research employs a quantitative approach, analysing responses from 250 employees across various departments in power sector organisations. Using SPSS, key statistical tests—including factor analysis, correlation, regression, and ANOVA—are applied to examine the relationships between AI bias, privacy concerns, transparency, employee trust, and job satisfaction. Findings reveal that AI bias significantly affects workforce diversity, while privacy concerns negatively impact employee trust in AI-driven HR decisions. Moreover, the study highlights that greater transparency in AI decision-making fosters higher employee satisfaction and engagement. The study underscores the need for organisations to implement ethical AI governance frameworks to ensure fair, unbiased, and privacy-compliant AI systems in HRM. It recommends explainable AI models, fairness audits, and hybrid decision-making (AI + human oversight) to enhance trust and acceptance of AI-driven HR practices. These findings contribute to the broader discourse on responsible AI adoption in HRM, offering strategic insights for HR leaders, policymakers, and AI developers in the power sector.",,,,,,,,,0,0,0,0,0,0,0,,,,2455-3085,,,,,,,,,,,,,,2025-08-05,RC:157021980_S24,,
J,"Liu, Haochen; Wang, Yiqi; Fan, Wenqi; Liu, Xiaorui; Li, Yaxin; Jain, Shaili; Liu, Yunhao; Jain, Anil; Tang, Jiliang",,,,"; Liu, Yunhao/NIS-8274-2025; liu, xiaorui/GVU-5497-2022","Liu, Xiaorui/0000-0001-8217-5688; Liu, Haochen/0000-0001-5290-7163; FAN, Wenqi/0000-0002-4049-1233",,,,,Trustworthy AI: A Computational Perspective,,,,,,,,ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY,,,,14,1,,,,,,,,10.1145/3546872,,,,,,,Article,FEB 2023,2023,"In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies.A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",,,,,,,,,97,1,0,0,2,0,110,,,2157-6904,2157-6912,,,,,,,,,,"Michigan State Univ, Engn Bldg 3308,428 South Shaw Lane, E Lansing, MI 48824 USAHong Kong Polytech Univ, Hung Hom, Kowloon, PQ724c,Mong Man Wai Bldg, Hong Kong 999077, Peoples R ChinaTwitter, 1355 Market St 900, San Francisco, CA 94110 USATsinghua Univ, Sch Software, Beijing 100084, Peoples R ChinaTsinghua Univ, TNLIST, Beijing 100084, Peoples R ChinaMichigan State Univ, Engn Bldg 3115,428 South Shaw Lane, E Lansing, MI 48824 USA",,,,2023-04-10,WOS:000944988100004,,
J,"Ueda, Daiju; Kakinuma, Taichi; Fujita, Shohei; Kamagata, Koji; Fushimi, Yasutaka; Ito, Rintaro; Matsui, Yusuke; Nozaki, Taiki; Nakaura, Takeshi; Fujima, Noriyuki; Tatsugami, Fuminari; Yanagawa, Masahiro; Hirata, Kenji; Yamada, Akira; Tsuboyama, Takahiro; Kawamura, Mariko; Fujioka, Tomoyuki; Naganawa, Shinji",,,,"Yanagawa, Masahiro/AAD-2306-2022; 鎌形, 康司/AFL-9072-2022; Ueda, Daiju/AAG-2167-2021; Fushimi, Yasutaka/L-8922-2017; Kawamura, Mariko/Q-5517-2019; Naganawa, Shinji/I-1572-2012; Fujima, Noriyuki/AAF-7270-2020; Yamada, Akira/AAD-1536-2019; 倫太郎, 伊藤/AFH-9268-2022","Yanagawa, Masahiro/0000-0002-0911-6769; Ueda, Daiju/0000-0002-3878-3616",,,,,Fairness of artificial intelligence in healthcare: review and recommendations,,,,,,,,JAPANESE JOURNAL OF RADIOLOGY,,,,42,1,,,3,15,,,,10.1007/s11604-023-01474-3,,,,,AUG 2023,,Review,JAN 2024,2024,"In this review, we address the issue of fairness in the clinical integration of artificial intelligence (AI) in the medical field. As the clinical adoption of deep learning algorithms, a subfield of AI, progresses, concerns have arisen regarding the impact of AI biases and discrimination on patient health. This review aims to provide a comprehensive overview of concerns associated with AI fairness; discuss strategies to mitigate AI biases; and emphasize the need for cooperation among physicians, AI researchers, AI developers, policymakers, and patients to ensure equitable AI integration. First, we define and introduce the concept of fairness in AI applications in healthcare and radiology, emphasizing the benefits and challenges of incorporating AI into clinical practice. Next, we delve into concerns regarding fairness in healthcare, addressing the various causes of biases in AI and potential concerns such as misdiagnosis, unequal access to treatment, and ethical considerations. We then outline strategies for addressing fairness, such as the importance of diverse and representative data and algorithm audits. Additionally, we discuss ethical and legal considerations such as data privacy, responsibility, accountability, transparency, and explainability in AI. Finally, we present the Fairness of Artificial Intelligence Recommendations in healthcare (FAIR) statement to offer best practices. Through these efforts, we aim to provide a foundation for discussing the responsible and equitable implementation and deployment of AI in healthcare.",,,,,,,,,199,1,0,0,17,0,236,,,1867-1071,1867-108X,,,,,,,,,,"Osaka Metropolitan Univ, Grad Sch Med, Dept Diagnost & Intervent Radiol, 1-4-3 Asahi Machi,Abeno Ku, Osaka 5458585, JapanSTORIA Law Off, Chuo Ku, Kobe, Hyogo, JapanUniv Tokyo, Dept Radiol, Bunkyo Ku, Tokyo, JapanJuntendo Univ, Dept Radiol, Grad Sch Med, Bunkyo Ku, Tokyo, JapanKyoto Univ, Dept Diagnost Imaging & Nucl Med, Grad Sch Med, Sakyo Ku, Kyoto, JapanNagoya Univ, Dept Radiol, Grad Sch Med, Nagoya, Aichi, JapanOkayama Univ, Fac Med Dent & Pharmaceut Sci, Dept Radiol, Kita Ku, Okayama, JapanKeio Univ, Dept Radiol, Sch Med, Shinjuku Ku, Tokyo, JapanKumamoto Univ, Dept Diagnost Radiol, Grad Sch Med, Chuo Ku, Kumamoto, JapanHokkaido Univ Hosp, Dept Diagnost & Intervent Radiol, Sapporo, JapanHiroshima Univ, Dept Diagnost Radiol, Minami Ku, Hiroshima, JapanOsaka Univ, Dept Radiol, Grad Sch Med, Suita, Osaka, JapanHokkaido Univ, Grad Sch Med, Dept Diagnost Imaging, Kita Ku, Sapporo, Hokkaido, JapanShinshu Univ, Dept Radiol, Sch Med, Matsumoto, Nagano, JapanTokyo Med & Dent Univ, Dept Diagnost Radiol, Bunkyo Ku, Tokyo, Japan",Osaka Metropolitan UnivSTORIA Law Off,,,2023-10-18,WOS:001043049800002,37540463,
J,Kumar C,,,,,,,,,,,,,,,,,,Journal of Informatics Education and Research,,,,5,1,,,,,,,,10.52783/jier.v5i1.2166,,,,,Feb 2025,,Article,Feb 13 2025,2025,"Artificial intelligence (AI) has brought about change in the manner of traditional recruitment processes. It has improved the efficiency, accuracy, and experiences of candidates during the process significantly. The organizations are also fast adopting such AI-driven solutions in the form of resume-screening algorithms, chatbots, predictive analytics, and video interviews for assessments by streamlining the decision-making. However, concerns around the fairness of AI and reduction of bias in hiring successes are highly controversial. This study investigates the statistical effect of AI adoption on the effectiveness of recruitment using a quantitative research method. AI adoption acts as an independent variable, while recruitment efficiency, reduction of bias, and candidate experience are dependent variables. A total of 250 responses was collected from HR professionals across multinational corporations and startups with the help of structured questionnaires. Multiple regression analysis using SPSS was conducted to determine the relationships between AI adoption and the identified recruitment outcomes. The results show that AI adoption significantly influences efficiency in recruitment (β = 0.58, p < 0.001) and candidate experience (β = 0.61, p < 0.001); in other words, AI-driven tools speed up the process of hiring and enhance candidate engagement. Nevertheless, it was also concluded that AI has a positive, albeit weak, impact regarding bias reduction: β = 0.32, p = 0.041. This indicates that AI alone cannot completely remove the inherent biases of recruitment; hence, there is an ongoing need for human supervision and improvement of the models of AI. These results present both the opportunities and challenges associated with AI-driven recruitment. While AI saves a lot of time in the hiring process and improves the candidate experience, issues of fairness, algorithmic bias, and ethical concerns still prevail. Best practices for organizations include regular auditing of AI-driven tools, high-quality data input, and integration of human decision-making with AI solutions. Ethical considerations of data privacy, transparency, and responsible use of AI will help in gaining trust in the technology by both candidates and HR practitioners. This is an empirical study adding to the growing body of literature on AI in HR, as there has been little to no statistical relationship between AI adoption and key recruitment metrics. Hence, future research should focus on longitudinal studies that will examine the long-term impact of AI on recruitment and assess additional moderating variables such as industry type, company size, and AI training methodologies. The study provides clear evidence that a balanced approach is required, in which AI could complement but not replace human judgment in recruitment decisions.",,,,,,,,,0,0,0,0,0,0,0,,,,1526-4726,,,,,,,,,,,,,,2025-07-11,RC:152371597_S24,,
J,"Tahaei, Mohammad; Constantinides, Marios; Quercia, Daniele",,,,,,,,,,Toward Human-Centered Responsible Artificial Intelligence: A Review of CHI Research and Industry Toolkits,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2302.05284,,,,,,,preprint,Feb 26 2023,2023,"As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications. In this paper, we present a bottom-up mapping of the current state of research in Human-Centered Responsible AI (HCR-AI) by thematically reviewing and analyzing 27 CHI research papers and 19 toolkits from industry and academia. Our results show that the current research in HCR-AI places a heavy emphasis on explainability, fairness, privacy, and security. We also found that there is space to research accountability in AI and build usable tools for non-experts to audit AI. While the CHI community has started to champion the well-being of individuals directly or indirectly impacted by AI, more research and toolkits are still required to address the long-term effects of AI on individuals, societies, and natural resources (human flourishing and sustainability).",,,,,,,,,15,0,0,0,0,0,15,,,,,,,,,,,,,,"Nokia Bell Labs, Cambridge, England",,,,2023-05-10,PPRN:37346647,,
J,"Adanyin, Anthonette",,,,,,,,,,,,,,,,,,European Journal of Computer Science and Information Technology,,,,12,7,,,21,35,,,,10.37745/ejcsit.2013/vol12n72135,,,,,Jul 2024,,Article,Jul 26 2024,2024,"The adoption of artificial intelligence (AI) in retail has significantly transformed the industry, enabling more personalized services and efficient operations. However, the rapid implementation of AI technologies raises ethical concerns, particularly regarding consumer privacy and fairness. This study aims to analyze the ethical challenges of AI applications in retail, explore ways retailers can implement AI technologies ethically while remaining competitive, and provide recommendations on ethical AI practices. A descriptive survey design was used to collect data from 300 respondents across major e-commerce platforms. Data were analyzed using descriptive statistics, including percentages and mean scores. Findings shows a high level of concerns among consumers regarding the amount of personal data collected by AI-driven retail applications, with many expressing a lack of trust in how their data is managed. Also, fairness is another major issue, as a majority believe AI systems do not treat consumers equally, raising concerns about algorithmic bias. It was also found that AI can enhance business competitiveness and efficiency without compromising ethical principles, such as data privacy and fairness. Data privacy and transparency were highlighted as critical areas where retailers need to focus their efforts, indicating a strong demand for stricter data protection protocols and ongoing scrutiny of AI systems. The study concludes that retailers must prioritize transparency, fairness, and data protection when deploying AI systems. The study recommends ensuring transparency in AI processes, conducting regular audits to address biases, incorporating consumer feedback in AI development, and emphasizing consumer data privacy.",,,,,,,,,0,0,0,0,0,0,1,,,,2054-0965,,,,,,,,,,,,,,2025-07-10,RC:142813051_S24,,
J,"Adanyin, Anthonette",,,,,,,,,,Ethical AI in Retail: Consumer Privacy and Fairness,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2410.15369,,,,,,,preprint,Oct 20 2024,2024,"The adoption of artificial intelligence (AI) in retail has significantly transformed the industry, enabling more personalized services and efficient operations. However, the rapid implementation of AI technologies raises ethical concerns, particularly regarding consumer privacy and fairness. This study aims to analyze the ethical challenges of AI applications in retail, explore ways retailers can implement AI technologies ethically while remaining competitive, and provide recommendations on ethical AI practices. A descriptive survey design was used to collect data from 300 respondents across major e-commerce platforms. Data were analyzed using descriptive statistics, including percentages and mean scores. Findings shows a high level of concerns among consumers regarding the amount of personal data collected by AI-driven retail applications, with many expressing a lack of trust in how their data is managed. Also, fairness is another major issue, as a majority believe AI systems do not treat consumers equally, raising concerns about algorithmic bias. It was also found that AI can enhance business competitiveness and efficiency without compromising ethical principles, such as data privacy and fairness. Data privacy and transparency were highlighted as critical areas where retailers need to focus their efforts, indicating a strong demand for stricter data protection protocols and ongoing scrutiny of AI systems. The study concludes that retailers must prioritize transparency, fairness, and data protection when deploying AI systems. The study recommends ensuring transparency in AI processes, conducting regular audits to address biases, incorporating consumer feedback in AI development, and emphasizing consumer data privacy.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"15 Blackthorn Dr, Wolverhampton WV10 8AN, England",,,,2024-11-20,PPRN:118748092,,
J,"Laine, Joakim; Minkkinen, Matti; Mantymaki, Matti",,,,"Minkkinen, Matti/T-7067-2019","Minkkinen, Matti/0000-0002-6406-8093; Mantymaki, Matti/0000-0002-1981-566X",,,,,Ethics-based AI auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders,,,,,,,,INFORMATION & MANAGEMENT,,,,61,5,,,,,103969,,,10.1016/j.im.2024.103969,,,,,MAY 2024,,Review,JUL 2024,2024,"This systematic literature review synthesizes the conceptualizations of ethical principles in AI auditing literature and the knowledge contributions to the stakeholders of AI auditing. We explain how the literature discusses fairness, transparency, non-maleficence, responsibility, privacy, trust, beneficence, and freedom/autonomy. Conceptualizations vary along social/technical- and process/outcome-oriented dimensions. The main stakeholders of ethics-based AI auditing are system developers and deployers, the wider public, researchers, auditors, AI system users, and regulators. AI auditing provides three types of knowledge contributions to stakeholders: 1) guidance; 2) methods, tools, and frameworks; and 3) awareness and empowerment.",,,,,,,,,22,0,0,0,0,0,27,,,0378-7206,1872-7530,,,,,,,,,,"Univ Turku, Turku Sch Econ, Turku, FinlandUniv Turku, FI-20014 Turku, Finland",,,,2024-06-11,WOS:001239680100001,,
J,"-, Rachid Ejjami",,,,,,,,,,,,,,,,,,International Journal For Multidisciplinary Research,,,,6,4,,,,,,,,10.36948/ijfmr.2024.v06i04.26086,,,,,Aug 2024,,Article,Aug 14 2024,2024,"This integrative literature review examines the transformative impact of advanced technologies, particularly artificial intelligence (AI), Internet of Things (IoT), and blockchain, on public administration. It addresses the urgent need for enhanced operational efficiency and transparency through AI-driven decision-making. The study reviews current literature to highlight AI's potential to revolutionize public service delivery, improve smart city initiatives, and enable data-driven policy-making. Significant challenges such as data privacy, algorithmic transparency, and ethical considerations are also identified. The methodology involves a comprehensive review of scholarly articles, reports, and academic publications, focusing on AI applications in public administration and smart technologies. The analysis reveals notable improvements in operational efficiency and transparency due to AI, alongside concerns about biases, transparency, and implementation issues. The findings confirm AI's transformative potential in public administration but emphasize the necessity for ongoing supervision, regular audits, and the development of AI models capable of detecting and rectifying operational anomalies. The study proposes creating positions such as Public Administration AI Ethics Officers (PAAIEOs), Public Administration Data Transparency Managers (PADTMs), and Public Administration Predictive Analytics Officers (PAPAOs) to ensure responsible AI utilization, maintaining the integrity and efficiency of public services while addressing implementation challenges. The review concludes that AI is promising for transforming public administration; however, careful implementation is crucial to uphold operational integrity and resilience. Future research should prioritize longitudinal studies to evaluate AI's long-term impact, focus on addressing implementation concerns, and ensure fair and transparent integration of AI technologies. These findings have significant implications for practice and policy, underscoring the need for robust frameworks and regulatory measures to guide the effective use of AI in public administration.",,,,,,,,,1,0,0,0,0,0,2,,,,2582-2160,,,,,,,,,,,,,,2025-07-11,RC:151922802_S24,,
J,"Nur Asvia, Salwa; Miftahur Rohmah, Siti; Nabilah, Zahidah",,,,,,,,,,,,,,,,,,Interdisciplinary Explorations in Research Journal,,,,2,2,,,655,667,,,,10.62976/ierj.v2i2.525,,,,,Jun 2024,,Article,Jun 01 2024,2024,"Abstract Divorce in Indonesia is a complex social problem. Mediation is a more peaceful and constructive alternative to trial. The development of AI technology opens up new opportunities in mediation, including through the use of AI chatbots. This journal analyzes the potential and challenges of using AI chatbots as mediators in divorce cases in Indonesia within the framework of positive law. This research uses a normative juridical research method with a qualitative approach. Data was collected through literature study and legal analysis of relevant laws and regulations. The results showed that AI chatbots have the potential to help improve the accessibility and effectiveness of divorce mediation. AI chatbots can provide legal information and guidance on the mediation process, help disputing parties to identify and understand their issues, and facilitate communication and negotiation between the two parties. However, the use of AI chatbots in divorce mediation also presents some legal challenges. Among them are the limitations of AI in understanding the complexity of human emotions and situations, concerns about the privacy and security of disputants' data, the potential for bias and discrimination in AI algorithms, and the lack of specific legal regulations to govern the use of AI in mediation. The journal recommends several measures to address such legal challenges, such as the development of more sophisticated AI chatbots that are sensitive to the emotional and social context of mediation, the establishment of strict privacy and data security standards, testing and auditing of AI algorithms to ensure fairness and non-discrimination, and the establishment of specific legal regulations to govern the use of AI in mediation. In conclusion, AI chatbots have the potential to be a useful tool in divorce mediation in Indonesia. However, serious efforts need to be made to address the legal challenges associated with its use.",,,,,,,,,0,0,0,0,0,0,0,,,,3032-1069,,,,,,,,,,,,,,2025-07-22,RC:156229317_S24,,
J,"Diaz-Rodriguez, Natalia; Del Ser, Javier; Coeckelbergh, Mark; de Prado, Marcos Lopez; Herrera-Viedma, Enrique; Herrera, Francisco",,,,"Del Ser, Javier/J-2187-2014; Herrera, Francisco/AFT-2050-2022; HERRERA-VIEDMA, ENRIQUE/C-2704-2008; de Prado, Marcos/C-1518-2013; Díaz-Rodríguez, Natalia/B-1843-2019","Del Ser, Javier/0000-0002-1260-9775; Diaz-Rodriguez, Natalia/0000-0003-3362-9326;",,,,,"Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation",,,,,,,,INFORMATION FUSION,,,,99,,,,,,101896,,,10.1016/j.inffus.2023.101896,,,,,JUL 2023,,Article,NOV 2023,2023,"Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.",,,,,,,,,204,1,0,0,10,1,257,,,1566-2535,1872-6305,,,,,,,,,,"Univ Granada, DaSCI Andalusian Inst Data Sci & Computat Intellig, Dept Comp Sci & Artificial Intelligence, Granada 18071, SpainTECNALIA, Basque Res & Technol Alliance BRTA, Derio 48160, SpainUniv Basque Country UPV, EHU, Dept Commun Engn, Bilbao 48013, SpainUniv Vienna, Dept Philosophy, A-1010 Vienna, AustriaCornell Univ, Sch Engn, Ithaca, NY 14850 USAADIA Lab, Al Maryah Isl, Abu Dhabi, U Arab EmiratesKhalifa Univ Sci & Technol, Dept Math, Abu Dhabi, U Arab Emirates",TECNALIAADIA Lab,,,2023-08-15,WOS:001040646900001,,
J,"Hassan, Saad; Asad, Syeda Mah Noor; Eslami, Motahhare; Mattei, Nicholas; Culotta, Aron; Zimmerman, John",,,,,,,,,,,,,,,,,,Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,,,,12,,,,151,154,,,,10.1609/hcomp.v12i1.31610,,,,,Oct 2024,,Article,Oct 14 2024,2024,"Public sector leverages artificial intelligence (AI) to enhance the efficiency, transparency, and accountability of civic operations and public services. This includes initiatives such as predictive waste management, facial recognition for identification, and advanced tools in the criminal justice system. While public-sector AI can improve efficiency and accountability, it also has the potential to perpetuate biases, infringe on privacy, and marginalize vulnerable groups. Responsible AI (RAI) research aims to address these concerns by focusing on fairness and equity through participatory AI. We invite researchers, community members, and public sector workers to collaborate on designing, developing, and deploying RAI systems that enhance public sector accountability and transparency. Key topics include raising awareness of AI's impact on the public sector, improving access to AI auditing tools, building public engagement capacity, fostering early community involvement to align AI innovations with public needs, and promoting accessible and inclusive participation in AI development. The workshop will feature two keynotes, two short paper sessions, and three discussion-oriented activities. Our goal is to create a platform for exchanging ideas and developing strategies to design community-engaged RAI systems while mitigating the potential harms of AI and maximizing its benefits in the public sector.",,,,,,,,,0,0,0,0,0,0,0,,,,2769-1349,,,,,,,,,,,,,,2025-07-11,RC:153593031_S24,,
J,"Mastrogiovanni, Sergio",,,,,,,,,,,,,,,,,,South Florida Journal of Development,,,,6,9,,,e5810,e5810,,,,10.46932/sfjdv6n9-029,,,,,,,Article,Sep 01 2025,2025,"The integration of Artificial Intelligence (AI) in smart cities has transformed urban governance, enhancing efficiency in public services, infrastructure management, and decision-making. However, the widespread use of AI for data collection and analysis raises significant challenges related to privacy, algorithmic bias, transparency, and public trust. Without proper governance, AI systems risk exacerbating inequalities, infringing on citizen rights, and reducing accountability in automated decision-making. This paper explores how AI-driven frameworks can enhance data governance while ensuring privacy protection, algorithmic fairness, and citizen empowerment. Key strategies include federated learning to enable decentralized data processing, differential privacy to protect individual identities, and explainable AI to increase transparency in automated decisions. Additionally, bias detection mechanisms and algorithmic audits are essential to prevent discrimination in AI-driven urban systems. Public trust is crucial in smart city initiatives, requiring citizen engagement models, participatory AI councils, and transparent data-sharing policies. Case studies illustrate how open data initiatives, AI-driven services, and digital innovation can enhance public service delivery and civic engagement when guided by strong governance and ethical data management. The paper proposes a comprehensive governance framework integrating privacy-centric AI, fairness-aware algorithms, and public engagement strategies to ensure sustainable, transparent, and accountable AI-driven urban ecosystems. The findings suggest that aligning technological innovation with inclusive policies and capacity-building not only improves urban efficiency and resilience but also builds public trust and empowerment. By aligning technological advancements with ethical and legal safeguards, smart cities can optimize AI’s potential while maintaining public trust and regulatory compliance.",,,,,,,,,0,0,0,0,0,0,0,,,,2675-5459,,,,,,,,,,,,,,2025-09-25,RC:159920841_S24,,
J,"Kolo, Faith Hauwa Oluwapamilerin",,,,,,,,,,,,,,,,,,Journal of Engineering Research and Reports,,,,27,5,,,510,532,,,,10.9734/jerr/2025/v27i51520,,,,,,,Article,Apr 29 2025,2025,"Building on the findings of Obioha-Val (2024), which reveal both the transformative potential of AI and the risks associated with unregulated or opaque implementation, this study investigates the responsible deployment of AI-driven cybersecurity systems in e-commerce by examining structural, ethical, and governance challenges. Using four open-source datasets, including the IEEE-CIS Fraud Detection Dataset, OECD AI Readiness indicators, the Stanford AI Governance Index, and the Global AI Ethics Guidelines Dataset, the study applies Principal Component Analysis, Logistic Regression, Weighted Scoring Models, and K-means clustering to evaluate adoption barriers and framework adaptability. Results show that only 40% of e-commerce firms are AI integration-ready, with SMEs particularly hindered by outdated infrastructure and limited workforce capacity. Algorithmic fairness testing revealed zero transaction flags under the applied threshold, raising concerns of underfitting and potential hidden biases. Ethical risks such as privacy violations, consent ambiguity, and algorithmic discrimination, particularly in pricing and service delivery, are highlighted as critical threats. Governance analysis ranked the UK highest (8.0010), while 95% of firms lacked formal AI oversight structures. Cluster analysis indicated that only 30% of international AI frameworks sufficiently incorporate operational principles like security and human oversight. This study adapts the Obioha-Val framework originally applied in U.S. public school systems to the commercial e-commerce context, offering a recalibrated, sector-specific model of responsible AI governance. Recommendations include developing AI-specific cybersecurity protocols, integrating fairness auditing tools, harmonizing global standards, and investing in infrastructure and AI literacy for SMEs.",,,,,,,,,1,0,0,0,0,0,1,,,,2582-2926,,,,,,,,,,,,,,2025-08-05,RC:158146367_S24,,
J,Edith Ebele Agu; Angela Omozele Abhulimen; Anwuli Nkemchor Obiki-Osafiele; Olajide Soji Osundare; Ibrahim Adedeji Adeniran; Christianah Pelumi Efunniyi,,,,,,,,,,,,,,,,,,International Journal of Frontline Research in Multidisciplinary Studies,,,,3,2,,,1,9,,,,10.56355/ijfrms.2024.3.2.0024,,,,,Aug 2024,,Article,Aug 30 2024,2024,"This review paper examines the ethical considerations and proposes solutions for ensuring fairness in AI-driven financial services. Artificial intelligence (AI) technologies are increasingly integrated into financial systems, offering benefits such as enhanced efficiency and personalized services. However, the deployment of AI in financial services raises ethical concerns related to bias and discrimination, transparency and accountability, privacy rights, and algorithmic fairness. Biases inherent in training data can lead to discriminatory outcomes, while opaque decision-making processes challenge transparency and accountability. Privacy concerns arise from extensive data collection, necessitating robust data protection measures. Achieving algorithmic fairness presents complex challenges, requiring strategies to mitigate biases and ensure equitable outcomes. To address these challenges, this paper proposes several solutions. Algorithmic audits and transparency measures are essential to detect and rectify biases in AI systems. Inclusive data practices promote the use of representative datasets, mitigating biases and enhancing fairness. Regulatory frameworks play a crucial role in setting ethical standards and enforcing compliance. Ethical AI design principles guide the development of responsible AI systems that prioritize fairness and transparency. Stakeholder collaboration fosters industry-wide consensus and accountability. Future research should focus on advanced bias detection techniques, explainable AI (XAI) for transparency, comprehensive ethical frameworks tailored for AI governance, impact assessments, interdisciplinary collaboration, and consumer education. By advancing these areas, stakeholders can build a more equitable and trustworthy AI ecosystem in financial services, enhancing public trust and promoting responsible AI adoption.",,,,,,,,,1,0,0,0,0,0,6,,,,2945-4875,,,,,,,,,,,,,,2025-07-10,RC:148971476_S24,,
J,"Ivanov, Stanislav",,,,,,,,,,Responsible use of AI in social science research,,,,,,,,SERVICE INDUSTRIES JOURNAL,,,,,,,,,,,,,10.1080/02642069.2025.2537115,,,,,JUL 2025,,Article; Early Access,,2025,"The paper elaborates on the responsible use of AI in social science research. It outlines the principles of responsible AI use in research, namely: transparency, traceability/auditability, explainability, fairness, privacy and data protection, respect and awareness of the broader AI impacts, human oversight and quality control, and accountability. It delves into the responsible AI use in each step of the research process from ideation to manuscript evaluation. The paper formulates specific recommendations for authors, PhD students, early career researchers and PhD programmes, publishers, funders, and policymakers related to responsible AI use in research. Finally, the paper sketches the future prospects of AI use in research and future research directions.",,,,,,,,,0,0,0,0,0,0,0,,,0264-2069,1743-9507,,,,,,,,,,"Varna Univ Management, Dept Tourism, 13A Oborishte str, Varna 9000, BulgariaZangador Res Inst, Varna 9010, Bulgaria",Zangador Res Inst,,,2025-07-26,WOS:001532712300001,,
J,"Igbokwe, Innocent C.",,,,,,,,,,,,,,,,,,European Journal of Arts Humanities and Social Sciences,,,,1,6,,,3,10,,,,10.59324/ejahss.2024.1(6).01; 10.59324/EJAHSS.2024.1(6).01,,,,,Nov 2024,,Article,Nov 01 2024,2024,"Artificial intelligence (AI) is transforming various sectors globally, including education. In education, artificial intelligence has the potential to significantly enhance educational leadership by improving decision-making, streamlining administrative tasks, and personalizing student learning experiences. However, the integration of AI into educational systems also introduces risks related to bias, privacy, transparency, and accountability. Educational leaders bear the responsibility of managing these risks and ensuring that AI is used ethically and responsibly. This paper explores the risks and responsibilities associated with the implementation of AI in educational leadership. This paper examines ethical concerns, decision-making processes, privacy, accountability, and the need for responsible AI usage. It recommends among other things that educational leaders must ensure that AI systems are designed and operated in ways that promote fairness, equity, and inclusivity by developing and implementing comprehensive ethical guidelines to ensure the responsible use of AI; should implement bias-detection mechanisms so as to promote fairness in AI-driven decision-making processes and reduce discriminatory practices and must enforce strict data security protocols, such as encryption, secure access, and regular system audits, to safeguard sensitive information in educational institutions. To this end, by understanding these risks and responsibilities, educational leaders can better harness AI's potential to enhance educational outcomes while safeguarding the integrity of education systems.",,,,,,,,,2,0,0,0,0,0,4,,,,3041-1262,,,,,,,,,,,,,,2025-07-11,RC:154398216_S24,,
J,"Zahra, Yatama",,,,,,,,,,,,,,,,,,Journal of Computer Science Application and Engineering (JOSAPEN),,,,3,1,,,10,15,,,,10.70356/josapen.v3i1.47,,,,,Jan 2025,,Article,Jan 03 2025,2025,"The integration of Artificial Intelligence (AI) in legal practice is transforming the legal profession by enhancing efficiency and accessibility while presenting significant ethical and regulatory challenges. AI applications such as predictive analytics, automated document drafting, and AI-driven legal research hold immense potential to reduce administrative burdens, streamline case management, and improve access to justice. However, issues such as algorithmic bias, lack of transparency, and data privacy concerns raise critical questions about fairness and accountability in AI-driven decision-making. This study aims to analyze the dual landscape of challenges and opportunities associated with AI adoption in legal practice, emphasizing the need for balanced regulatory frameworks. A systematic review of existing literature was conducted to identify the obstacles and benefits of AI integration. Key challenges include algorithmic biases, inadequate legal frameworks, and the digital divide among legal professionals, while opportunities range from cost reduction to improved dispute resolution processes. The findings contribute to ongoing discussions on AI governance by proposing actionable strategies such as fairness audits, explainable AI practices, and targeted training programs for legal professionals.",,,,,,,,,1,0,0,0,0,0,1,,,,3031-2272,,,,,,,,,,,,,,2025-07-10,RC:143286406_S24,,
J,Amaka Justina Obinna; Azeez Jason Kess-Momoh,,,,,,,,,,,,,,,,,,GSC Advanced Research and Reviews,,,,19,1,,,146,160,,,,10.30574/gscarr.2024.19.1.0149,,,,,Apr 2024,,Article,Apr 30 2024,2024,"This study presents a conceptual technical framework aimed at promoting ethical AI deployment within the procurement domain, with a particular focus on legal oversight. As the integration of artificial intelligence (AI) technologies in procurement processes becomes increasingly prevalent, concerns surrounding ethical considerations and legal compliance have come to the forefront. The framework outlined in this study offers a structured approach to addressing these challenges, emphasizing the importance of legal oversight in ensuring ethical AI practices. Drawing on existing literature and best practices, the framework outlines key components and principles for guiding the development, implementation, and monitoring of AI systems in procurement contexts. Central to the framework is the recognition of legal requirements and regulatory frameworks governing AI deployment, including data protection laws, liability provisions, and procurement regulations. By incorporating these legal considerations into the design and operation of AI systems, organizations can mitigate risks and ensure compliance with applicable laws. Additionally, the framework emphasizes the need for transparency and accountability in AI procurement processes, advocating for clear documentation, audit trails, and stakeholder engagement mechanisms. Furthermore, the framework outlines strategies for ethical AI design, including the identification and mitigation of algorithmic bias, the promotion of fairness and equity, and the protection of privacy rights. By embedding ethical principles into the development lifecycle of AI systems, organizations can foster trust and confidence among stakeholders while minimizing the potential for harm or discrimination. Overall, the conceptual technical framework presented in this study provides a comprehensive approach to promoting ethical AI in procurement, with a specific emphasis on legal oversight. By integrating legal requirements, ethical principles, and technical considerations, organizations can ensure that AI deployment in procurement processes is conducted responsibly, transparently, and in accordance with legal and ethical standards.",,,,,,,,,1,0,0,0,0,0,1,,,,2582-4597,,,,,,,,,,,,,,2025-07-11,RC:151170985_S24,,
J,"Radanliev, Petar",,,,"Radanliev, Petar/L-7509-2015","Radanliev, Petar/0000-0001-5629-6857",,,,,"AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development",,,,,,,,APPLIED ARTIFICIAL INTELLIGENCE,,,,39,1,,,,,2463722,,,10.1080/08839514.2025.2463722,,,,,,,Review,DEC 31 2025,2025,"The expansion of Artificial Intelligence in sectors such as healthcare, finance, and communication has raised critical ethical concerns surrounding transparency, fairness, and privacy. Addressing these issues is essential for the responsible development and deployment of AI systems. This research establishes a comprehensive ethical framework that mitigates biases and promotes accountability in AI technologies. A comparative analysis of international AI policy frameworks from regions including the European Union, United States, and China is conducted using analytical tools such as Venn diagrams and Cartesian graphs. These tools allow for a visual and systematic evaluation of the ethical principles guiding AI development across different jurisdictions. The results reveal significant variations in how global regions prioritize transparency, fairness, and privacy, with challenges in creating a unified ethical standard. To address these challenges, we propose technical strategies, including fairness-aware algorithms, routine audits, and the establishment of diverse development teams to ensure ethical AI practices. This paper provides actionable recommendations for integrating ethical oversight into the AI lifecycle, advocating for the creation of AI systems that are both technically sophisticated and aligned with societal values. The findings underscore the necessity of global collaboration in fostering ethical AI development.",,,,,,,,,25,0,0,0,1,0,30,,,0883-9514,1087-6545,,,,,,,,,,"Univ Oxford, Dept Comp Sci, Oxford, England",,,,2025-02-14,WOS:001415497800001,,
J,"Clavell, Gemma Galdon; Gonzalez-Sendino, Ruben; Vazquez, Paola",,,,,,,,,,Demographic Benchmarking: Bridging Socio-Technical Gaps in Bias Detection,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2501.15985,,,,,,,preprint,Jan 27 2025,2025,"Artificial intelligence (AI) models are increasingly autonomous in decision-making, making pursuing responsible AI more critical than ever. Responsible AI (RAI) is defined by its commitment to transparency, privacy, safety, inclusiveness, and fairness. But while the principles of RAI are clear and shared, RAI practices and auditing mechanisms are still incipient. A key challenge is establishing metrics and benchmarks that define performance goals aligned with RAI principles. This paper describes how the ITACA AI auditing platform developed by Eticas.ai tackles demographic benchmarking when auditing AI recommender systems. To this end, we describe a Demographic Benchmarking Framework designed to measure the populations potentially impacted by specific AI models. The framework serves us as auditors as it allows us to not just measure but establish acceptability ranges for specific performance indicators, which we share with the developers of the systems we audit so they can build balanced training datasets and measure and monitor fairness throughout the AI lifecycle. It is also a valuable resource for policymakers in drafting effective and enforceable regulations. Our approach integrates socio-demographic insights directly into AI systems, reducing bias and improving overall performance. The main contributions of this study include:1. Defining control datasets tailored to specific demographics so they can be used in model training; 2. Comparing the overall population with those impacted by the deployed model to identify discrepancies and account for structural bias; and 3. Quantifying drift in different scenarios continuously and as a post-market monitoring mechanism.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"Eticas AI, New York City, NY 10013, USAEticas AI, Madrid, SpainEticas AI, Cuenca, Ecuador",Eticas AIEticas AIEticas AI,,,2025-03-22,PPRN:120964509,,
J,"Munz, Phil; Hennick, Max; Stewart, James",,,,,"Munz, Phil/0009-0008-1219-2480",,,,,Maximizing AI reliability through anticipatory thinking and model risk audits,,,,,,,,AI MAGAZINE,,,,44,2,,,173,184,,,,10.1002/aaai.12099,,,,,JUN 2023,,Article,JUN 2023,2023,"AI is transforming the way we live and work, with the potential to improve our lives in many ways. However, there are risks associated with AI deployments including failures of model robustness and security, explainability and interpretability, bias and fairness, and privacy and ethics. While there are international efforts to define governance standards for responsible AI, these are currently only principles-based, leaving organizations uncertain as to how they can prepare for emerging regulations or evaluate their effectiveness. We propose the use of anticipatory thinking and a flexible model risk audit (MRA) framework to bridge this gap and enable organizations to take an advantage of the benefits of responsible AI. This approach enables organizations to characterize risk at the model level and to apply the anticipatory thinking employed by high reliability organizations to achieve responsible AI deployments.",,,,,,,,,4,0,0,0,0,0,4,,,0738-4602,2371-9621,,,,,,,,,,"TrojAI Inc, St John, NB, Canada",TrojAI Inc,,,2023-07-05,WOS:001012977300001,,
C,"Balasubramaniam, Nagadivya; Kauppinen, Marjo; Hiekkanen, Kari; Kujala, Sari",,,,"Kujala, Sari/M-7004-2013","Balasubramaniam, Nagadivya/0009-0007-3869-6479","Gervasi, V; Vogelsang, A",,,,Transparency and Explainability of AI Systems: Ethical Guidelines in Practice,,,,,,,,"REQUIREMENTS ENGINEERING: FOUNDATION FOR SOFTWARE QUALITY, REFSQ 2022",,Lecture Notes in Computer Science,,13216,,,,3,18,,,,10.1007/978-3-030-98464-9_1,,,,,,,Proceedings Paper,2022,2022,"[Context and Motivation] Recent studies have highlighted transparency and explainability as important quality requirements of AI systems. However, there are still relatively few case studies that describe the current state of defining these quality requirements in practice. [Question] The goal of our study was to explore what ethical guidelines organizations have defined for the development of transparent and explainable AI systems. We analyzed the ethical guidelines in 16 organizations representing different industries and public sector. [Results] In the ethical guidelines, the importance of transparency was highlighted by almost all of the organizations, and explainability was considered as an integral part of transparency. Building trust in AI systems was one of the key reasons for developing transparency and explainability, and customers and users were raised as the main target groups of the explanations. The organizations also mentioned developers, partners, and stakeholders as important groups needing explanations. The ethical guidelines contained the following aspects of the AI system that should be explained: the purpose, role of AI, inputs, behavior, data utilized, outputs, and limitations. The guidelines also pointed out that transparency and explainability relate to several other quality requirements, such as trustworthiness, understandability, traceability, privacy, auditability, and fairness. [Contribution] For researchers, this paper provides insights into what organizations consider important in the transparency and, in particular, explainability of AI systems. For practitioners, this study suggests a structured way to define explainability requirements of AI systems.",,,,,28th International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ)28th International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ),"MAR 21-24, 2022MAR 21-24, 2022",Aston Univ; Univ Cologne; Durham Univ; Univ Pisa; Univ Politecnica Catalunya; Int Requirements Engn Board; BCS; EPSRC Res Project Twenty20Insight; SpringerAston Univ; Univ Cologne; Durham Univ; Univ Pisa; Univ Politecnica Catalunya; Int Requirements Engn Board; BCS; EPSRC Res Project Twenty20Insight; Springer,"Birmingham, ENGLANDBirmingham, ENGLAND",35,0,0,0,2,0,45,,,0302-9743,1611-3349,978-3-030-98464-9; 978-3-030-98463-2,,,,,,,,,"Aalto Univ, Dept Comp Sci, Espoo, Finland",,,,2022-05-11,WOS:000784606100001,,
J,"Kiselev, Aleksandr S.",,,,,,,,,,Theoretical and legal analysis of the risks of introducing technologies based on artificial intelligence in the socio-economic sphere,,,,,,,,VESTNIK TOMSKOGO GOSUDARSTVENNOGO UNIVERSITETA-PRAVO-TOMSK STATE UNIVERSITY JOURNAL OF LAW,,,,,55,,,,,,,,10.17223/22253513/55/4,,,,,,,Article,MAR 2025,2025,"In this paper the main risks of application of artificial intelligence in the socio-economic sphere and measures that can be utilized to minimize them have been considered. The following risks have been identified: disappearance of many professions, loss of jobs, vulnerability of AI-based systems to hacker attacks, formation of monopolies, discrimination of individual citizens and social groups, etc. Methodology: the work applied general scientific methods of research-analysis, synthesis, induction, deduction, systematic method and method of abstraction, as well as private-scientific methods: comparative-legal and formal-legal. Legal methods to minimize the risks of AI implementation were recommended, and prospects for further research in this area were identified. 1. Standards (GOSTs) should be created to which AI systems will be obligatorily compliant. These standards should regulate the transparency of operation and contain security protocols for AI-based systems, as well as protect human and civil rights and freedoms. The key principles, in our view, should be security, transparency and respect for human rights and freedoms. Defining ethical standards also involves making decisions about the collection and use of personal data. 2. Any AI-based systems must be understandable not only to operators and programmers, but also to the majority of citizens. Accordingly, it is necessary to develop a scale for assessing the degree of explainability of the functioning of individual AI-based technologies. This can help both the developers themselves and educators in developing appropriate educational programs, advanced training courses and professional retraining. 3. AI operation in experimental mode. It is important that before its direct implementation in any sphere of human activity, developers receive and take into account feedback from users. This approach allows identifying and correcting possible errors, errors in operation that may occur during the operation of the AI. 4. Systematic and continuous monitoring of AI-based systems by the authorities. That is why we should think about creating special interdepartmental expert commissions consisting of specialists in legal, technical, ethical and other issues of AI application. 5. When developing AI systems, the socio-cultural characteristics and needs of each particular community must be taken into account. We believe that the following laws are needed: on transparency of algorithms to ensure that they work in various sectors of the economy from commerce to social services; on the verification of the proper functioning, whereby AI should be available for systematic auditing and monitoring to ensure that it works properly and complies with government standards; on non-discrimination; and on fair distribution of AI-based high-tech to all members of society.",,,,,,,,,0,0,0,0,0,0,0,,,2225-3513,2311-3693,,,,,,,,,,"Financial Univ Govt Russian Federat, Fac Law, Ctr Res & Expertise, Moscow, RussiaFinancial Univ Govt Russian Federat, Fac Law, Dept Int & Publ Law, Moscow, Russia",,,,2025-06-01,WOS:001498090600003,,
J,"Biswas, Oishi; Sharma, Prerana; Das, Shreyashi; Bharti, Laxmi",,,,,,,,,,,,,,,,,,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,,,,9,4,,,1,9,,,,10.55041/ijsrem44266,,,,,,,Article,Apr 10 2025,2025,"The impact of artificial intelligence (AI) on human resources management (HRM) is profound, driving major shifts in how organizations approach workforce management. AI is fundamentally changing key HR processes such as recruitment, performance management, employee development, and HR administration. In the recruitment space, AI-powered tools can automate the initial stages of hiring, streamlining tasks like resume screening and interview scheduling. These technologies are able to analyze large pools of applicants, matching their qualifications and skills to the job requirements. This can lead to faster, more accurate hiring decisions and help organizations build diverse, high-performing teams. However, there is a potential risk that biases embedded in the data used by AI systems could reinforce inequalities, making it crucial for companies to ensure that their AI tools are regularly audited for fairness and inclusivity.In performance management, AI is transforming how employees are evaluated and developed. Traditional performance reviews can often be subjective and inconsistent, but AI can introduce a more objective and data-driven approach. By continuously tracking key performance indicators (KPIs) and employee behaviors, AI can provide real-time insights into performance trends, identify potential areas of improvement, and even predict future challenges. This enables HR professionals to tailor development plans for individual employees, fostering personal growth and enhancing overall productivity. However, the reliance on AI for performance evaluation raises concerns about employee privacy and the potential for over-monitoring, highlighting the need for clear ethical guidelines and transparency in how data is collected and used.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-3930,,,,,,,,,,,,,,2025-07-10,RC:142937789_S24,,
J,"Kang, El",,,,,,,,,,Artificial Intelligence Contracts and Realization of the Principle of Freedom of Contract,,,인공지능계약과 계약자유 원칙의 실현,,,,,Gachon Law Review,가천법학,,,17,4,,,3,46,,,,,,,,,,,research-article,2024,2024,"The development of artificial intelligence (AI) presents new challenges and opportunities in the realm of legal systems and contract law. In particular, concerns have been raised about the possibility of AI participating as a contracting party, which could threaten the traditional principle of freedom of contract. The principle of freedom of contract, which forms the foundation of private autonomy, guarantees the freedom of parties to autonomously decide on the conclusion, content, counterpart, and method of a contract. However, the introduction of AI into the contract formation process not only enhances efficiency in areas such as the presentation of contract terms, automated negotiations, and contract drafting but also raises issues such as the opacity of decision-making, data bias, and the infringement of personal information. The utilization of artificial intelligence in the contract formation process has the potential to limit human autonomy; however, it can also facilitate the formation of efficient contracts by complementing and expanding the principle of freedom of contract through the enhancement of private autonomy enabled by artificial intelligence.AI contracts offer efficiency and convenience but are accompanied by challenges related to ensuring trustworthiness, fairness, and accountability. To address these issues, AI governance and algorithm audits can play a crucial role. AI governance, based on limited regulation, the activation of self-regulation, and the participation of stakeholders, can promote the balanced development of innovation in AI contracts while upholding the principle of freedom of contract. Furthermore, algorithm audits can prevent distortions of the principle of freedom of contract by identifying and addressing system biases, errors, and unethical decision-making, while demonstrating the operation of fair and unbiased systems.To protect human autonomy and uphold the principle of freedom of contract in AI agreements, it is essential to establish fair and trustworthy AI systems. This will enhance trust among contracting parties and facilitate the formation of contracts freely. It is necessary to systematically analyze the limiting factors of the principle of freedom of contract that arise in the process of AI contracts. Based on this analysis, efforts should be made to explore ways to effectively implement the principle of freedom of contract in a manner that aligns with the AI environment.",,,,"인공지능(AI)의 발전은 법적 제도와 계약법의 영역에서 새로운 도전과 기회를 제공하고 있다. 특히, 인공지능이 계약의 당사자로 참여할 가능성이 제기되면서 전통적인 계약자유의 원칙이 위협받을 수 있다는 우려가 제기된다. 계약자유의 원칙은 사적자치의 근간으로 계약의 체결 여부, 내용, 상대방 선택, 방식 등을 당사자가 자율적으로 결정할 수 있는 자유를 보장한다. 하지만 인공지능이 계약체결 과정에 도입되면 계약조건의 제시, 협상의 자동화, 계약서 작성 등에서 효율성을 증진시키는 동시에 의사결정의 불투명성, 데이터 편향성, 개인정보 침해 등의 문제를 초래할 수 있다. 인공지능이 계약의 당사자로 인정될 경우, 인공지능의 권리능력과 행위능력 부여 및 그 범위에 대한 법적Â·사회적 합의가 필요하다. 특히, 계약위반 시 책임귀속 주체를 명확히 하는 것이 중요한 쟁점으로 대두된다. 이를 해결하기 위해 기존 계약법의 재검토와 함께 인공지능의 특성을 반영한 새로운 법리의 정립이 요구된다. 인공지능의 활용은 계약체결 과정에서 인간의 자율성을 제한할 가능성이 있지만, 동시에 인공지능의 활용으로 인한 사적자치의 확대를 통해 계약자유의 원칙을 보완하고 확장하여 효율적인 계약 형성을 지원할 수 있다.인공지능계약은 효율성과 편의성을 제공하지만 신뢰성, 공정성, 책임성 확보라는 과제를 동반한다. 이를 해결하기 위해 인공지능 거버넌스와 알고리즘 감사가 중요한 역할을 할 수 있다. 제한적 규제, 자율규제 활성화, 이해관계자 참여를 기반으로 한 인공지능 거버넌스는 인공지능계약의 혁신성과 계약자유 원칙 간의 균형적 발전을 도모할 수 있다. 또한, 인공지능 알고리즘 감사는 시스템의 편향, 오류, 비윤리적 의사결정을 방지하고, 공정하고 비편향적인 시스템 작동을 입증함으로써 계약자유 원칙의 훼손을 예방할 수 있다.인공지능계약에서 인간의 자율성을 보호하고 계약자유의 원칙을 실현하기 위해서는 공정하고 신뢰할 수 있는 인공지능시스템을 구축하는 것이 필수적이다. 이를 통해 계약 당사자 간 신뢰를 증진하고 자유로운 계약체결을 촉진할 수 있을 것이다. 인공지능의 발전과 활용은 계약자유의 원칙을 포함하는 계약법 영역에 새로운 도전과 기회를 제시하고 있으며, 공정성과 효율성을 동시에 담보할 수 있는 법적Â·사회적 합의 도출을 위한 지속적인 논의가 필요하다. 따라서 인공지능계약 과정에서 제기되는 계약자유 원칙의 제한 요소를 체계적으로 분석하고, 이를 토대로 인공지능 환경에 부합하는 계약자유의 원칙을 효과적으로 구현할 수 있는 방안을 모색할 필요가 있다.",,,,,0,0,0,0,0,0,0,,,2713-8151,,,,,,,,,,,,,,,2025-01-26,KJD:ART003166317,,
J,"Sharma, Prapti",,,,,,,,,,,,,,,,,,International Journal For Multidisciplinary Research,,,,7,4,,,,,,,,10.36948/ijfmr.2025.v07i04.50414,,,,,,,Article,Jan 01 2025,2025,"Artificial intelligence (AI) technologies are being used at a rapid pace, which has created previously unnoticed ethical difficulties that call for creative solutions to guarantee adherence to moral principles. This research paper presents the AI Ethics Compliance System (AECS), a cutting-edge system that combines smart contracts, blockchain technology, and AI governance mechanisms to address enduring ethical issues in AI, such as privacy assurance, transparency, and bias mitigation. AECS creates a scalable and adaptable ethical auditing layer for AI systems by combining real-time bias monitoring, immutable decision logging, and automated regulatory compliance enforcement in a way that is not possible with other solutions. With the goal to overcome current throughput and latency limits, the framework stands out for its hybrid blockchain architecture, support for smart contract-based legal adaptation, and modular design.When compared to well-known solutions like IBM's AI Fairness 360, AECS can perform better in terms of auditability and compliance accuracy. When tested using common benchmark datasets and metrics, prototype implementations in high-stakes applications—like facial recognition and medical diagnostics—show that AECS can increase compliance rates by up to 30%. Furthermore, with built-in capabilities for dynamic legal updates and jurisdictional variation, AECS is created in accordance with international regulatory frameworks such as the GDPR and the EU AI Act. These results highlight the potential of AECS as a strong compliance framework that can connect the progress of technical AI with changing legal and ethical standards.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-2160,,,,,,,,,,,,,,2025-08-05,RC:158384949_S24,,
J,Ruwaiza Sasmita; Tiara Azzahra Marpaung,,,,,,,,,,,,,,,,,,BLAZE Jurnal Bahasa dan Sastra dalam Pendidikan Linguistik dan Pengembangan,,,,3,1,,,108,119,,,,10.59841/blaze.v3i1.2260,,,,,Dec 2024,,Article,Dec 30 2024,2024,"The development of Artificial Intelligence (AI) has brought significant changes to the field of translation, offering ease and efficiency in the translation process. However, its use raises various ethical issues that need to be regulated through a clear code of ethics. This study discusses the Code of Ethics for the Use of AI in Translation, encompassing key principles such as accountability, transparency, privacy, fairness, and cultural respect.The code of ethics aims to optimize AI's potential while minimizing negative impacts, such as algorithmic bias, loss of cultural meaning in translations, and the declining role of human translators. Through literature reviews and case studies, this research recommends steps such as regular AI system audits, human-AI collaboration, and compliance with data privacy laws. By implementing this code of ethics, the use of AI in translation is expected to operate responsibly, support linguistic diversity, and uphold professional standards in the field of translation.",,,,,,,,,0,0,0,0,0,0,0,,,,3025-2423,,,,,,,,,,,,,,2025-07-10,RC:140494790_S24,,
J,"Kulal, Abhinandan; Dinesh, Sahana; Abhishek, N.",,,,"Dinesh, Sahana/NYT-2530-2025; Kulal, Abhinandan/GSN-5451-2022",,,,,,Organizational Impact of AI-Driven Recruitment Practices: A Mixed-Methods Study,,,,,,,,JOURNAL OF COMPUTER INFORMATION SYSTEMS,,,,,,,,,,,,,10.1080/08874417.2025.2508860,,,,,MAY 2025,,Article; Early Access,,2025,"The integration of Artificial Intelligence (AI) into recruitment processes has revolutionized talent acquisition, promising enhanced efficiency, fairness, and data-driven decision-making. However, its ethical implications remain a subject of debate, with concerns regarding algorithmic bias, transparency, and privacy. This study examines the impact of AI-driven recruitment on organizational outcomes, focusing on ethical compliance, workforce diversity, HR policy robustness, and employee engagement. Grounded in the Stimulus-Organism-Response (SOR) framework, the research employs a mixed-methods approach, combining survey data from HR professionals and qualitative insights from semi-structured interviews. The findings reveal that while AI enhances efficiency and standardizes hiring criteria, its opacity and potential biases pose challenges to fairness and inclusivity. Ethical AI governance, transparency mechanisms, and algorithmic audits emerge as critical factors in mitigating biases and fostering trust in AI-driven hiring. Furthermore, the study highlights the role of HR policy adaptability in ensuring legal compliance and maintaining employee confidence in AI recruitment systems.",,,,,,,,,0,0,0,0,0,0,0,,,0887-4417,2380-2057,,,,,,,,,,"Govt First Grade Coll, Siddakatte, Karnataka, IndiaGovt First Grade Coll, Napoklu, IndiaGovt First Grade Coll, Halebeedu, India",Govt First Grade CollGovt First Grade CollGovt First Grade Coll,,,2025-05-28,WOS:001494144200001,,
J,Aditi M Jain; Ayush Jain,,,,,,,,,,,,,,,,,,International Journal of Scientific Research in Computer Science Engineering and Information Technology,,,,11,1,,,3720,3728,,,,10.32628/cseit2410414,,,,,Feb 2025,,Article,Feb 25 2025,2025,"As e-commerce rapidly integrates artificial intelligence (AI) for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around data privacy, algorithmic bias, and consumer autonomy. Bias—whether cultural, gender based, or socioeconomic—can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes.This paper examines the ethical implications of AI driven content creation and product recommendations, emphasizing the need for frameworks to ensure fairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regular audits of algorithms, diversifying training data, and incorporating fairness metrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy, promoting transparency in decision making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.",,,,,,,,,0,0,0,0,0,0,0,,,,2456-3307,,,,,,,,,,,,,,2025-07-11,RC:151349742_S24,,
J,"Zisan, Tauhidul Islam; Pulok, Md Meherab Khandokar; Borman, Dipok; Barmon, Roni Chandra; Asif, Md. Rakibul Hasan",,,,,,,,,,,,,,,,,,European Journal of Theoretical and Applied Sciences,,,,2,6,,,324,332,,,,10.59324/ejtas.2024.2(6).26,,,,,Nov 2024,,Article,Nov 01 2024,2024,"AI and Big Data make the traditional landscape of auditing change by further improving auditing practices in terms of efficiency, accuracy, and enhancement of risk management capabilities. The paper summarizes significant discussions on the application areas, benefits, challenges, and relevant ethics related to AI in extensive data auditing. It also considers crucial applications of AI in the audit realm, such as anomaly detection, predictive analytics, process automation, and real-time monitoring. Finally, it discusses various challenges, viz., the use of AI within the auditing domain: data security risks, biases, regulatory compliance, and ever-evolving requirements towards auditors' competencies. Conclusions are drawn on the industry-specific adoption trends showing how different industries, from finance to healthcare, retail, and manufacturing, have adopted AI. Furthermore, some sections address the issue of ethics: transparency, privacy, and fairness are discussed as high-priority issues to create trust in AI-augmented audits. The findings support that while AI and big data hold much potential in transforming auditing, successful deliverables on these are dependent upon how these technical and ethical challenges will be addressed and nurtured in interdisciplinary talent. The paper concludes with future trend insight that real-time auditing, predictive analytics, and blockchains for security in data management will have increased adoption. The studies further highlight that to drive full benefits from AI in auditing; there needs to be further innovation, ethical solid frameworks, and adaptive skill development to ensure integrity and accountability. .",,,,,,,,,1,0,0,0,0,0,1,,,,2786-7447,,,,,,,,,,,,,,2025-07-10,RC:148658391_S24,,
J,"Kalluri, Neeraja",,,,,,,,,,,,,,,,,,International Journal For Multidisciplinary Research,,,,7,4,,,,,,,,10.36948/ijfmr.2025.v07i04.51880,,,,,,,Article,Jan 01 2025,2025,"This paper examines the growing role of artificial intelligence in workplace lawsuits, labour relations, and dispute resolution. It examines the ways AI-enabled tools are altering how managers make decisions, how employees are monitored, the risk of discrimination, and the management of conflicts. The authors conducted a literature review that draws on articles published in peer-reviewed journals indexed by Scopus and Web of Science. From that review, five recurring themes emerged: algorithmic decision-making, continual workplace monitoring, predictive analytics, AI-assisted mediation, and the broader legal fallout. The analysis finds that AI can help organisations reduce risk, boost compliance, and resolve disputes early; however, it also raises serious concerns about bias in algorithms, a lack of transparency in decision-making processes, and potential violations of worker privacy rights. The authors argue that hybrid approaches where human judgment complements AI tools are vital to preserving fairness and accountability under the law. They urge companies to implement responsible AI governance that prioritises transparency, explainability, regular bias audits, and explicit worker consent. At the same time, they contend that legal systems must adapt to protect employee rights in AI-driven workplaces. By linking AI ethics, regulatory compliance, and conflict management, this study seeks to enrich the emerging field of socio-technical systems and employment law.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-2160,,,,,,,,,,,,,,2025-08-05,RC:158347709_S24,,
J,"Musrifah, Fadlul; Hasanah, Ika Ariani",,,,,,,,,,,,,,,,,,Journal of Management and Informatics,,,,4,1,,,599,616,,,,10.51903/jmi.v4i1.140,,,,,,,Article,Jan 22 2025,2025,"Integrating Artificial Intelligence (AI) into digital recruitment platforms has introduced significant enhancements in efficiency and decision-making, alongside complex ethical challenges regarding fairness, transparency, and accountability in candidate evaluation. This study investigates how leading AI-driven recruitment platforms articulate and operationalize ethical principles and whether these commitments are effectively translated into practice. Employing a qualitative exploratory design, the research analyzes official white papers, privacy policies, and AI ethics statements from LinkedIn, HireVue, Pymetrics, and ModernHire. Data was examined using AI-assisted text mining and thematic content analysis to identify ethical discourse patterns and assess the depth of implementation. The findings indicate that moral terms such as “fairness” and “bias” are cited frequently, with LinkedIn referencing them 27 times and HireVue 19 times. A comparative transparency assessment yielded scores of 8.5 out of 10 for LinkedIn, 7.2 for HireVue, 6.8 for Pymetrics, and 4.3 for ModernHire, while formal mechanisms for candidate appeals were absent on most platforms. This study contributes to the field by revealing a persistent gap between stated ethical ideals and operational practices in AI recruitment and by recommending the adoption of explainable AI, transparent auditing frameworks, and international regulatory standards. Such measures are essential to foster more accountable, equitable, and humane AI-based hiring processes.",,,,,,,,,0,0,0,0,0,0,0,,,,2961-7472,,,,,,,,,,,,,,2025-08-04,RC:157243187_S24,,
J,"Kandoi, Raunak; Dixit, Deepali; Tyagi, Mihul; Yadav, Raghuraj Singh",,,,,,,,,,,,,,,,,,International Journal for Research in Applied Science and Engineering Technology,,,,12,3,,,769,775,,,,10.22214/ijraset.2024.58787,,,,,Mar 2024,,Article,Mar 31 2024,2024,"Abstract: Conversational AI systems are becoming increasingly popular across many industries and are transforming the way people interact with technology. For a more authentic, human-like connection and a smooth user experience, these systems should combine text-based interactions with multimodal capabilities. The authors of this work suggest a new approach to improving conversational AI systems' usability by combining speech and visual analysis. By combining visual and auditory processing capabilities, AI systems can better understand human inquiries and instructions. Both visual data and speech can be better understood with the use of computer vision algorithms and natural language processing techniques, respectively. Conversational AI systems can provide more accurate and tailored replies by integrating many modalities to better grasp human intent and context. The development of multimodal conversational AI presents a significant difficulty in ensuring the smooth integration of voice and visual processing units. A strong architectural design and advanced algorithms are necessary for the simultaneous synchronization and comprehension of data from several modalities in real-time. The system needs to keep track of the conversation's context even when it switches between different forms of communication so it can keep providing fair and relevant responses all through the engagement. Customization is key to making multimodal conversational AI better for users. Based on user data and preferences, the system may tailor interactions to offer more relevant ideas and support. Users are more invested in the AI system over time, and they have a better experience overall because to customization. Ensuring the privacy and security of important audiovisual data is of the utmost importance while building multimodal conversational AI. Strong encryption, anonymization technologies, and compliance with data protection regulations are vital for user privacy and system confidence. Continuous improvement is key to the success of multimodal conversational AI systems. The feedback from users can help the developers improve the system and add new features. Thanks to this iterative technique, the AI system stays flexible and can adjust to changing consumer preferences. By combining voice and picture processing, conversational AI systems have a great deal of promise for improving the user experience. Through the integration of visual and auditory signals, these systems have the ability to comprehend user intent more accurately, provide customized experiences, and completely transform the way humans engage with technology.",,,,,,,,,1,0,0,0,0,0,2,,,,2321-9653,,,,,,,,,,,,,,2025-07-10,RC:144085924_S24,,
J,"Oesterling, Alex; Bhalla, Usha; Venkatasubramanian, Suresh; Lakkaraju, Himabindu",,,,,,,,,,"Operationalizing the Blueprint for an AI Bill of Rights: Recommendations for Practitioners, Researchers, and Policy Makers",,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2407.08689,,,,,,,preprint,Jul 11 2024,2024,"As Artificial Intelligence (AI) tools become ubiquitous and are increasingly employed in diverse real-world applications, there has been significant interest in regulating these tools. To this end, several regulatory frameworks have been introduced by different countries worldwide. For example, the European Union recently passed the AI Act, and the White House issued an Executive Order on safe, secure, and trustworthy AI, associated guidance from the Office of Management and Budget, all of which built on the Blueprint for an AI Bill of Rights issued by the White House Office of Science and Technology Policy. Many of these frameworks emphasize the need for auditing and improving the trustworthiness of AI models and tools, underscoring the importance of safety, privacy, explainability, fairness, and human fallback options. Although these regulatory frameworks highlight the necessity of enforcing the aforementioned principles, practitioners often lack detailed guidance on implementing them. While there is extensive research on operationalizing each of these aspects, it is frequently buried in technical papers that are difficult for practitioners to parse. In this write-up, we address this shortcoming by providing an accessible overview of the various approaches available in existing literature to operationalize regulatory principles. For the sake of clarity and precision, we will specifically focus on the Blueprint for an AI Bill of Rights throughout this document. We provide easy-to-understand summaries of state-of-the-art literature and highlight various gaps that exist between the regulatory guidelines in the Blueprint for an AI Bill of Rights and existing AI research, including the trade-offs that emerge when operationalizing each of the highlighted principles. We hope that this work not only serves as a starting point for practitioners interested in learning more about operationalizing the regulatory guidelines outlined in the Blueprint for an AI Bill of Rights but also provides researchers with a list of critical open problems whose resolution can help bridge the gaps between regulations and state-of-the-art AI research. Finally, we note that this is a working paper and we invite feedback in line with the purpose of this document as described in the introduction.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"Harvard Univ, John A Paulson Sch Engn & Appl Sci, Boston, MA 02134, USAHarvard Univ, Harvard Business Sch, Boston, MA, USABrown Univ, Ctr Tech Responsibil, Providence, RI, USA",Brown Univ,,,2024-07-23,PPRN:90770445,,
J,"Kunz, Werner H.; Wirtz, Jochen",,,,"; Wirtz, Jochen/P-3235-2015; Kunz, Werner/KRQ-9193-2024","Kunz, Werner/0000-0001-6264-183X; Wirtz, Jochen/0000-0002-6297-4498;",,,,,Corporate digital responsibility (CDR) in the age of AI: implications for interactive marketing,,,,,,,,JOURNAL OF RESEARCH IN INTERACTIVE MARKETING,,,,18,1,SI,,31,37,,,,10.1108/JRIM-06-2023-0176,,,,,AUG 2023,,Article,JAN 30 2024,2024,"PurposeDespite all the recent achievements in the field of interactive marketing and artificial intelligence (AI), it is important to consider the ethical implications of these technologies. This paper explains the concept of corporate digital responsibility (CDR) and how it is affected by new advances in AI.Design/methodology/approachThe authors build on the work of Wirtz et al., (2023) and derive several managerial implications for the challenges that AI poses to CDR. CDR refers to a service company's ethical and fair use of data and technology within its digital service ecosystem. It involves establishing standards, protecting customer privacy, conducting external audits and striving for an equitable power dynamic between service firms and their partners.FindingsDespite the risks involved, many companies are not prioritizing good CDR practices. Financial benefits from the collection and use of consumer data, improved customer experience through AI-driven customization and personalization, cost reduction through service automation and the trade-offs between organizational goals and CDR practices can prevent companies from prioritizing good CDR practices.Originality/valueThis is one of the first articles in the service domain to take the concept of CDR and apply it to recent developments in generative AI.Research limitations/implicationsThe emergence of powerful AI tools presents opportunities and challenges. Research opportunities include responsible business restructuring, responsible service automation to ensure fairness and human oversight, addressing dehumanization of service delivery, responsible customer profiling to address privacy and discrimination concerns and preventing AI misuse.",,,,,,,,,58,0,0,0,0,0,65,,,2040-7122,2040-7130,,,,,,,,,,"Univ Massachusetts Boston, Dept Mkt, Boston, MA 02125 USANatl Univ Singapore, Dept Mkt, Singapore, Singapore",,,,2023-08-15,WOS:001041119600001,,
J,"Karras, Dimitrios A.",,,,,,,,,,On Modelling a Reliable Framework for Responsible and Ethical AI in Digitalization and Automation: Advancements and Challenges,,,,,,,,Financial Engineering,,,,3,,,,333,350,,,,10.37394/232032.2025.3.29,,,,,Jul 2025,,Article,Jul 23 2025,2025,"The rapid evolution of artificial intelligence (AI) has ushered in transformative advancements across industries, from healthcare to finance, while simultaneously raising critical ethical and societal challenges. This paper aims at comprehensively explore the latest developments and trends in creating a framework for responsible and ethical AI, emphasizing the need for Digitalization that prioritizes transparency, fairness, accountability, and humanenvironment safety. Key advancements, such as improved algorithmic auditing, bias mitigation techniques, and interdisciplinary collaboration, will be highlighted alongside persistent challenges, including data privacy concerns, cultural variability in ethical standards, and the risk of unintended consequences in using autonomous systems. By examining case studies and emerging best practices based on a comprehensive state-of-the-art literature review, this paper will underscore the importance of balancing innovation with oversight to ensure AI can provide improved services applied to Digitalization and Business Transformation while simultaneously minimizing all risks. Ultimately, it is herein attempted to define a suitable framework, named FRE-AIDT, for applying relevant policies towards a global, inclusive dialogue to shape a future in business transformation and digitalization where AI aligns with human values and priorities while addressing the complexities of its real-world deployment and in parallel minimizing all possible relevant risks for societies and the environment. To effectively confront these pressing challenges, worldwide initiatives have been admitted actively to establish robust frameworks that ensure AI systems are not only responsible and ethical but also fundamentally aligned with societal values and priorities. This paper delves into vital advancements in this field, based on case studies and state-of-the-art literature, defining models and emphasizing initiatives such as the EU AI Act, the USA's AI-related policies, and the IEEE's ethical AI framework, attempting to provide a comprehensive summary of these initiatives and models towards modelling the proposed FRE-AIDT framework.",,,,,,,,,0,0,0,0,0,0,0,,,,2945-1140,,,,,,,,,,"National and Kapodistrian University of Athens (NKUA), Department of Agricultural Development, Agri-Food and Natural Resources Management (AAADFP), School of Agricultural Development, Nutrition and Sustainability, Athens, GREECE",Natl Kapodistrian Univ Athens NKUA,,,2025-08-05,RC:158320246_S24,,
J,"Fahmy, Azza Moustafa",,,,,,,,,,Exploring the Role of AI in Predicting Chronic Disease Progression: Diabetes and Cardiovascular Diseases,,,,,,,,Premier Journal of Public Health,,,,,,,,,,,,,10.70389/pjph.100021,,,,,,,Article,Jul 25 2025,2025,"This review integrates multimodal data, which includes genetic information, wearable sensor outputs, and electronic health records (EHRs), providing an innovative analysis of artificial intelligence (AI) advancements for chronic disease prediction. AI is revolutionizing chronic disease management, particularly for diabetes and cardiovascular disease. Through the incorporation of evidence from AI-enabled models, the research projects predictive accuracies exceeding 80% in the onset and progression of illness and their role in ensuring early diagnosis, customized treatment, and operational efficiency. Inventions like neural networks improved by particle swarm optimization attain diagnostic accuracies of 99.67%, while edge computing helps real-time monitoring to reduce hospitalizations through early interventions. Although AI in health care has advanced significantly, algorithmic biases against underrepresented populations—e.g., older adults and non-White communities—fragmented data ecosystems preventing institutional interoperability, and ethical issues regarding privacy and transparency continue to impede scalability and fair implementation. To overcome these obstacles and ensure innovations are distributed equitably across global health care systems, future initiatives should prioritize multimodal data fusion, fairness audits during model development, federated learning frameworks that support safe cross-institutional collaboration, and large clinical trials that validate AI in multiple real-world settings. The advancement of AI has an opportunity to significantly enhance the quality of life for those struggling with chronic diseases and transform health care systems globally, provided that its advancement strategies are transparent, equitable, and focused on wide-ranging validation projects. By combining different multimodal data (EHRs, wearables, and genomes) with fairness audits and federated learning, this review provides a novel synthesis of AI advances for the prediction of chronic diseases. This assessment stands out from others because it addresses both technical performance and ethical deployment strategies. The aim is to support efforts to generate AI adoption equitably in the real world.",,,,,,,,,0,0,0,0,0,0,0,,,,3049-8910,,,,,,,,,,"Theodor Bilharz Research Institute,Giza",Theodor Bilharz Res Inst,,,2025-08-05,RC:158303220_S24,,
J,"Sankaran, Sridharan",,,,,,,,,,Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2504.16139,,,,,,,preprint,Apr 22 2025,2025,"As artificial intelligence (AI) reshapes industries and societies, ensuring its trustworthiness-through mitigating ethical risks like bias, opacity, and accountability deficits-remains a global challenge. International Organization for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to foster responsible development by embedding fairness, transparency, and risk management into AI systems. However, their effectiveness varies across diverse regulatory landscapes, from the EU's risk-based AI Act to China's stability-focused measures and the U.S.'s fragmented state-led initiatives. This paper introduces a novel Comparative Risk-Impact Assessment Framework to evaluate how well ISO standards address ethical risks within these contexts, proposing enhancements to strengthen their global applicability. By mapping ISO standards to the EU AI Act and surveying regulatory frameworks in ten regions-including the UK, Canada, India, Japan, Singapore, South Korea, and Brazil-we establish a baseline for ethical alignment. The framework, applied to case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO standards falter in enforcement (e.g., Colorado) and undervalue region-specific risks like privacy (China). We recommend mandatory risk audits, region-specific annexes, and a privacy-focused module to enhance ISO's adaptability. This approach not only synthesizes global trends but also offers a replicable tool for aligning standardization with ethical imperatives, fostering interoperability and trust in AI worldwide. Policymakers and standards bodies can leverage these insights to evolve AI governance, ensuring it meets diverse societal needs as the technology advances.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"Tata Consultancy Serv, Res & Innovat Grp, Chennai, India",,,,2025-08-07,PPRN:123066057,,
J,"Amoo, Tesleem Akinyemi",,,,,,,,,,,,,,,,,,FINGER Jurnal Ilmiah Teknologi Pendidikan,,,,4,1,,,34,41,,,,10.58723/finger.v4i1.365,,,,,Feb 2025,,Article,Feb 28 2025,2025,"This article explores the major technological advancements in education and the ethical issues they raise. In this research, we use the systematic literature mapping method. The study follows the research methodology outlined by Kabudi, Pappas, and Olsen, with guidance from Petersen, Vakkalanka, and Kuzniarz. The methodology applied in both studies is as follows: (i) search and selection, (ii) data extraction, (iii) classification and analysis, and (iv) evaluation of validity. The PRISMA approach, or Preferred Reporting Items for Systematic Reviews and Meta-Analyses, was used as a framework for the search and selection phase. Key findings reveal that AI can automate grading and provide personalized feedback to students, while also reducing cheating. It also helps in predictive analytics by predicting learning outcomes and identifying at-risk students. AI also evaluates non-cognitive traits like emotional states and collaborative skills. However, the study also highlights ethical issues in AI-based assessments, such as inclusivity, fairness, accountability, accuracy, explanation, auditability, security, privacy, autonomy, consent, and sustainability. The research identifies five key thematic areas: AI system design, AI-driven assessment rollout, data stewardship, assessment administration, and grading and evaluation. The study concludes that while AI presents transformative opportunities for educational assessments, it also introduces complex ethical challenges that must be carefully managed.",,,,,,,,,0,0,0,0,0,0,0,,,,2830-6813,,,,,,,,,,,,,,2025-07-10,RC:143207448_S24,,
J,"Jain, Aditi Madhusudan; Jain, Ayush",,,,,,,,,,AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2506.17370,,,,,,,preprint,Jun 20 2025,2025,"As e-commerce rapidly integrates artificial intelligence for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI-driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around data privacy, algorithmic bias, and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic -- can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes. This paper examines the ethical implications of AI-driven content creation and product recommendations, emphasizing the need for frameworks to ensure fairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regular audits of algorithms, diversifying training data, and incorporating fairness metrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy, promoting transparency in decision-making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,2025-09-19,PPRN:123901878,,
J,"Kumar, Anshu; Kumar Singh, Rana Saurav; Kumar, Sonu; Kumar, Gaurav; Sagar, Umang; Kumar, Avinash",,,,,,,,,,,,,,,,,,Journal of Neonatal Surgery,,,,14,32S,,,5173,5178,,,,10.63682/jns.v14i32s.8264,,,,,,,Article,Jun 12 2025,2025,"The convergence of Artificial Intelligence (AI) and healthcare has ushered in a transformative era in medical diagnostics, offering unprecedented precision, speed, and efficiency. From identifying early-stage cancers through radiological imaging to predicting genetic disorders and personalizing treatment plans, AI is fundamentally reshaping modern medicine. However, this technological leap comes with a parallel rise in legal and ethical complexities, particularly concerning patient data privacy, algorithmic transparency, and informed consent.This paper undertakes a multidimensional legal and ethical analysis of AI-driven medical diagnostics, with a special focus on data governance under the European Union’s General Data Protection Regulation (GDPR) and the enduring principles of medical ethics. It explores how core GDPR mandates including data minimization, purpose limitation, the right to explanation, and the right to erasure interact, and often conflict, with the operational realities of AI systems that function as data-intensive, opaque “black boxes.” The paper further investigates Article 22 of the GDPR, which limits fully automated decision-making, and examines the growing tension between legal mandates and algorithmic logic.Beyond regulatory scrutiny, the paper delves into ethical concerns such as loss of patient autonomy, the weakening of meaningful informed consent, and the risk of algorithmic bias leading to systemic discrimination particularly for underrepresented or vulnerable populations. Through case studies from jurisdictions like the UK (e.g., DeepMind-NHS controversy), the US (racial bias in AI triage systems), and India (challenges under the Digital Personal Data Protection Act, 2023), the study provides a comparative analysis of how various health systems are grappling with these issues.In bridging the legal and ethical dimensions, this research contributes original policy insights and practical recommendations aimed at strengthening accountability, ensuring fairness, and promoting transparency in AI-enabled healthcare. It advocates for regulatory modernization, mandatory algorithmic audits, explainable AI protocols, and patient-centric system design. The paper ultimately argues that while AI holds revolutionary promise in diagnostics, its deployment must be tempered by strong legal safeguards and ethical foresight to ensure that technological advancement does not come at the cost of patient rights, equity, or dignity",,,,,,,,,0,0,0,0,0,0,0,,,,2226-0439,,,,,,,,,,,,,,2025-08-04,RC:157922711_S24,,
J,"Cherep, Oleksandr; Kaliuzhna, Yuliia; Naumenko, Yevhen",,Zaporizhzhia National University,,,,,,,,,,,,,,,,Market Infrastructure,,,,,85,,,,,,,,10.32782/infrastruct85-14,,,,,,,Article,Jan 01 2025,2025,"This article explores the multifaceted ethical challenges arising from the application of artificial intelligence (AI) in the sphere of information warfare, with a strong focus on the Russian-Ukrainian war. As AI becomes increasingly prevalent in the creation, analysis, dissemination, and manipulation of information, its potential to influence public perception and strategic communication grows substantially. The authors critically examine risks associated with algorithmic decision-making, such as disinformation, deepfake technologies, data privacy violations, and the lack of transparency in automated systems. Emphasis is placed on the need for clear ethical standards and governance mechanisms to ensure responsible AI usage during wartime. The study outlines essential ethical principles – truthfulness, accountability, human rights protection, transparency, and fairness – as foundational values for trustworthy AI systems. It highlights how AI, when used irresponsibly, can exacerbate social fragmentation, fuel distrust in institutions, and distort democratic discourse. Particular attention is given to the importance of ethical education as a strategic tool for cultivating critical thinking, media literacy, and resilience against manipulative technologies. The integration of ethical reasoning into digital policies and academic curricula is deemed crucial for shaping a culture of ethical awareness. The paper advocates for the implementation of national ethical frameworks for AI in Ukraine, institutional oversight, transparent audit systems, and international cooperation to counteract digital threats. These frameworks must also address risks posed by autonomous decisionmaking in military contexts, where mistakes can have irreversible consequences. Additionally, fostering collaboration between governments, civil society, and the tech sector is vital for establishing globally accepted norms of ethical AI behavior. The authors emphasize the importance of balancing innovation with precaution, especially in domains involving national security and citizen well-being. Ethical design practices, including inclusive datasets and culturally sensitive model training, are proposed as long-term safeguards. Finally, the article concludes that sustainable AI development must be grounded in a human-centric vision, where technology serves democratic values rather than undermines them",,,,,,,,,0,0,0,0,0,0,0,,,,2519-2868,,,,,,,,,,,,,,2025-09-26,RC:159939550_S24,,
B,"Kim, Nicklaus Jun",,,,,,,,,,Privacy Auditing of Tabular Synthetic Data Generators Using Membership Inference Attacks,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2023,2023,,,,,,,,,,0,0,0,0,0,0,0,,,,,9.79838E+12,,,,,,,,,"University of California, Los Angeles, Statistics 0891, California, United States","University of California, Los Angeles",,,,PQDT:84485338,,
J,Goutham Sunkara,,,,,,,,,,,,,,,,,,World Journal of Advanced Research and Reviews,,,,24,2,,,2895,2905,,,,10.30574/wjarr.2024.24.2.3571,,,,,,,Article,Nov 30 2024,2024,"The high adoption rate of Artificial Intelligence (AI) in cybersecurity surveillance has played a major role in increasing detection of threats, behaviour, as well as incident response actions. Nevertheless, the technology has essential ethical and regulatory issues that concern privacy, consent, bias of algorithms, and accountability. The submitted paper researches the ethical issues and regulatory gaps, which arise due to the implementation of AI-based surveillance solutions in the area of both the private and the public sphere. Based on an interdisciplinary exploration of existing literature and regulatory approaches (including the EU GDPR and AI Act and the CCPA in the U.S.) as well as prominent examples of case studies, this paper will analyze the way in which these technologies threaten customary standards of transparency, fairness, and civil liberties. Analysis shows that the current regulations tend to be inconsistent, reactive, and poorly placed to handle the transparency and freedom of AI surveillance systems. It also points out the danger of increasing social disparities by means of unregulated algorithmic profiling. To alleviate such challenges, the paper proposes the multi-stakeholder methodology which implies to harmonize policies, incorporate the process of explainable AI, enforce the regular algorithmic audits, and introduce the privacy-preserving AI methodologies. Finally, the paper recommends proactive ethical governance and adaptive regulatory innovation so that cybersecurity surveillance technologies may work in the betterment of society without affecting the rights of individuals.",,,,,,,,,0,0,0,0,0,0,0,,,,2581-9615,,,,,,,,,,,,,,2025-08-04,RC:157890861_S24,,
J,"Tariq, Bilal; Ashraf, Muhammad Rehan; Rashid, Umar",,,,,,,,,,,,,,,,,,Ubiquitous Technology Journal,,,,1,2,,,61,73,,,,10.71346/utj.v1i2.23,,,,,,,Article,Jun 05 2025,2025,"As artificial intelligence (AI) becomes increasingly integral to critical sectors, the gap between abstract ethical principles and their concrete technical implementation presents a significant barrier to responsible innovation. This paper addresses this challenge by introducing a comprehensive framework designed to embed ethical considerations directly into the AI development lifecycle. The primary objective is to provide an operational methodology for proactive risk mitigation and the construction of verifiably trustworthy systems. Our proposed framework is structured around a core set of guiding principles, including fairness, transparency, accountability, and privacy. It advocates a multi-layered risk mitigation strategy that spans the design, development, deployment, and governance phases of AI systems. This approach integrates specific methodologies and tools, such as Ethical Impact Assessments, bias auditing techniques, Explainable AI (XAI) methods, and privacy-preserving technologies. The key contribution is a unified, actionable architecture that bridges the operationalization and fragmentation gaps currently plaguing the field. By systematically connecting high-level ethical goals to specific engineering practices and auditable checkpoints, this framework offers a practical pathway for developers and organizations to foster responsible AI and mitigate potential societal harms, ensuring technology remains aligned with human values.",,,,,,,,,0,0,0,0,0,0,0,,,,3079-5273,,,,,,,,,,,,,,2025-08-05,RC:158224160_S24,,
J,"Raza, Sayeed Arshad",,,,,,,,,,,,,,,,,,INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,,,,9,6,,,1,9,,,,10.55041/ijsrem49671,,,,,,,Article,Jun 09 2025,2025,"Abstract: This report critically examines the burgeoning integration of Artificial Intelligence (AI) within recruitment processes, dissecting the inherent tension between aspirations for heightened efficiency and the crucial imperative of maintaining ethical standards. By examining AI tools used in pre-screening, candidate engagement, and evaluation through the diverse frameworks of Human Capital Theory, Organizational Justice Theory, the Technology Acceptance Model (TAM), and Critical Theory, this study highlights underlying concerns such as algorithmic bias, the risk of impersonal or dehumanized candidate experiences, and the diminishing protection of applicant privacy. It contributes a novel and comprehensive framework meticulously designed for evaluating AI recruitment strategies, integrating disparate theoretical perspectives to furnish practical guidance for both Human Resource (HR) professionals and AI vendors. Emphasizing the necessity of rigorous ethical audits, algorithmic transparency, the indispensable role of human oversight, and a steadfast commitment to responsible AI development and deployment, the report advocates for proactive measures to ensure fairness, inclusivity, and a positive candidate experience. Furthermore, it identifies promising avenues for future research, including longitudinal studies to assess long-term impacts on diversity and the development of robust and reliable fairness metrics",,,,,,,,,0,0,0,0,0,0,0,,,,2582-3930,,,,,,,,,,,,,,2025-08-04,RC:157609751_S24,,
J,Goutham Yenuganti,,,,,,,,,,,,,,,,,,World Journal of Advanced Engineering Technology and Sciences,,,,15,3,,,776,782,,,,10.30574/wjaets.2025.15.3.0963,,,,,,,Article,Jun 30 2025,2025,"The deployment of Artificial Intelligence systems within regulated enterprise environments presents significant challenges in maintaining ethical standards while achieving operational objectives. This article addresses the critical need for transparent and accountable AI architectures that satisfy regulatory requirements across sectors including financial services, healthcare, and telecommunications. The framework presented encompasses four foundational principles: transparency and explainability, fairness and non-discrimination, accountability and auditability, and privacy protection. Implementation strategies include the development of auditable AI pipelines with comprehensive data governance, policy-driven constraint systems that automate compliance enforcement, human-in-the-loop validation mechanisms for critical oversight, and transparent decision communication interfaces for end-user understanding. The architectural solutions demonstrate how organizations can successfully balance innovation with regulatory compliance through systematic integration of ethical considerations into AI system design. These implementations provide measurable improvements in compliance rates, stakeholder trust, and operational reliability while maintaining competitive advantages in AI-driven business processes.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-8266,,,,,,,,,,,,,,2025-08-04,RC:157622237_S24,,
J,"Ismail, Osama; Ahmad, Naim",,,,,,,,,,,,,,,,,,International Journal of Interactive Mobile Technologies (iJIM),,,,19,14,,,121,136,,,,10.3991/ijim.v19i14.56981,,,,,,,Article,Jul 29 2025,2025,"Rapid proliferation of artificial intelligence (AI) in key domains such as healthcare, education, public services, and digital economies has heightened global commitment to developing strong ethical and governance standards. This systematic review, based on the PRISMA approach, synthesizes evidence from 22 peer-reviewed journal articles, white papers, and policy documents between 2020 and 2025. It examines governance frameworks, ethical concepts, regulatory approaches, sectoral application, and common challenges in operationalizing AI ethics internationally. The review sees wide confluence on core ethical values of fairness, transparency, accountability, explainability, and sustainability. There remains, however, wide variability in the application and institutionalization of these values across jurisdictions. The European Union’s AI Act provides a binding three-tiered risk system with centralized monitoring, whereas global institutions such as the World Health Organization (WHO) and the United Nations advocate high-level ethical frameworks but without statutory implementation. Regional efforts by ASEAN, Hong Kong SAR, and China reflect different states of maturity in the application of lifecycle audit and sector-level policy tools. Five recurring challenges are identified throughout literature: algorithmic bias, privacy risks of the data, patchwork of regulatory environments, corporate and state incumbents, and risks of labor market dislocation. Institutional models by the Alan Turing Institute and corporate players such as NTT DATA stress the value of building ethics from the outset of the life of AI. Educational and civil society voices add further calls for inclusive and collaborative governance frameworks. This paper concludes by advocating for the creation of an internationally coordinated AI regulation agency, cross-border certification schemes, ethics-by-design design methodologies, and adaptive governance mechanisms for the safe, equitable, and transparent application of AI technology.",,,,,,,,,0,0,0,0,0,0,0,,,,1865-7923,,,,,,,,,,,,,,2025-08-05,RC:158418676_S24,,
J,"Ade-Ibijola, Abejide; Nakatumba-Nabende, Joyce",,,,,,,,,,,,,,,,,,Ubiquitous Technology Journal,,,,1,2,,,33,45,,,,10.71346/utj.v1i2.20,,,,,,,Article,Jun 05 2025,2025,"Artificial Intelligence has become integral to modern systems across critical sectors, yet its widespread deployment raises serious ethical and risk-related concerns that remain inadequately addressed in current design practices. This paper identifies the urgent need for a structured approach that bridges the gap between normative ethics and technical implementation in AI systems. In response, it introduces a comprehensive framework that supports ethical AI design and proactive risk mitigation. The framework is built upon guiding principles including fairness, transparency, accountability, privacy, robustness, and human oversight. It incorporates a multi-layered strategy that embeds these principles into the design, development, deployment, and governance phases of AI. Methodologies such as bias auditing, explain ability techniques, privacy-preserving computation, and ethical impact assessments are detailed as core tools within this structure. The framework’s applicability is demonstrated through domain-specific case studies, showcasing how it addresses real-world challenges in autonomous vehicles, healthcare diagnostics, financial services, recommendation systems, and large language models. Key contributions of this research include a replicable structure for ethical compliance, the operationalization of abstract ethical goals, and actionable guidance for integrating social values into AI pipelines. This work concludes by affirming the critical role of ethical frameworks in guiding responsible AI innovation and calls for sustained interdisciplinary efforts to align future technological progress with human and societal well-being.",,,,,,,,,0,0,0,0,0,0,0,,,,3079-5273,,,,,,,,,,,,,,2025-08-04,RC:157592545_S24,,
J,"Sutherns, J.; Fanta, G. B.",,,,"Fanta, Getnet/V-9976-2019",,,,,,THE IMPLICATIONS OF INTEGRATING ARTIFICIAL INTELLIGENCE INTO DATA-DRIVEN DECISION-MAKING,,,,,,,,SOUTH AFRICAN JOURNAL OF INDUSTRIAL ENGINEERING,,,,35,3,,,195,207,,,,10.7166/35-3-3096,,,,,,,Article,NOV 2024,2024,"Integrating artificial intelligence (AI) into data-driven decision-making offers advantages like increased performance, reduced costs and improved organisational efficiency; however, there are associated risks. The study employs a PRISMA protocol to systematically review academic articles from Scopus, ScienceDirect, and Web of Science databases to determine whether the risks AI pose are worth the rewards they offer. Literature trends reveal a growing interest in AI-driven decision-making, with significant research gaps in African contexts. The study indicates that AI is highly utilized for decision-making to foster competitiveness in manufacturing, finance, healthcare, education, and transport. Identified risks include bias, discrimination, privacy issues, and cybersecurity threats. It is highlighted that businesses need to address concerns about privacy, fairness, and transparency. Policymakers must develop ethical and legal standards besides regular monitoring and auditing of AI uses to mitigate risks.",,,,,,,,,0,0,0,0,0,0,1,,,1012-277X,2224-7890,,,,,,,,,,"Univ Pretoria, Dept Engn & Technol Management, Pretoria, South Africa",,,,2025-02-13,WOS:001417174600015,,
J,"Tubbs, Abigail; Vazquez, Enrique Alvarez",,,,,,,,,,Digital twins in increasing diversity in clinical trials: A systematic review,,,,,,,,JOURNAL OF BIOMEDICAL INFORMATICS,,,,169,,,,,,104879,,,10.1016/j.jbi.2025.104879,,,,,,,Review,SEP 2025,2025,"The integration of digital twin (DT) technology and artificial intelligence (AI) into clinical trials holds transformative potential for addressing persistent inequities in participant representation. This systematic review evaluates the role of these technologies in improving diversity, particularly in racial, ethnic, gender, age, and socioeconomic dimensions, minimizing bias, and allowing personalized medicine in clinical research settings. Evidence from 90 studies reveals that digital twins offer dynamic simulation capabilities for trial design, while AI facilitates predictive analytics and recruitment optimization. However, implementation remains hindered by fragmented regulatory frameworks, biased datasets, and infrastructural disparities. Ethical concerns,including privacy, consent, and algorithmic opacity, further complicate the deployment. Inclusive data practices identified in the literature include the use of demographically representative training data, participatory data collection frameworks, and equity audits to detect and correct systemic bias. Fairness in AI and DT models is primarily operationalized through group fairness metrics such as demographic parity and equalized odds, along with fairness, aware model training and validation. Key gaps include the lack of global standards, underrepresentation in model training, and challenges in real-world adoption. To overcome these barriers, the review proposes actionable directions: developing inclusive data practices, harmonizing regulatory oversight, and embedding fairness into computational model design. By focusing on diversity as a design principle, AI and DT technologies can support a more equitable and generalizable future for clinical research.",,,,,,,,,0,0,0,0,0,0,0,,,1532-0464,1532-0480,,,,,,,,,,"Univ North Dakota, Biomed Engn, 501 N Columbia Rd, Grand Forks, ND 58202 USA",,,,2025-09-30,WOS:001578211900001,40784603,
J,"Daneshjou, Roxana; Smith, Mary P.; Sun, Mary D.; Rotemberg, Veronica; Zou, James",,,,"Daneshjou, Roxana/ABE-7764-2021","Daneshjou, Roxana/0000-0001-7988-9356",,,,,Lack of Transparency and Potential Bias in Artificial Intelligence Data Sets and Algorithms A Scoping Review,,,,,,,,JAMA DERMATOLOGY,,,,157,11,,,1362,1369,,,,10.1001/jamadermatol.2021.3129,,,,,SEP 2021,,Review,NOV 2021,2021,"IMPORTANCE Clinical artificial intelligence (AI) algorithms have the potential to improve clinical care, but fair, generalizable algorithms depend on the clinical data on which they are trained and tested.OBJECTIVE To assess whether data sets used for training diagnostic AI algorithms addressing skin disease are adequately described and to identify potential sources of bias in these data sets.DATA SOURCES In this scoping review, PubMed was used to search for peer-reviewed research articles published between January 1, 2015, and November 1, 2020, with the following paired search terms: deep learning and dermatology, artificial intelligence and dermatology, deep learning and dermatologist, and artificial intelligence and dermatologist.STUDY SELECTION Studies that developed or tested an existing deep learning algorithm for triage, diagnosis, or monitoring using clinical or dermoscopic images of skin disease were selected, and the articles were independently reviewed by 2 investigators to verify that they met selection criteria.CONSENSUS PROCESS Data set audit criteria were determined by consensus of all authors after reviewing existing literature to highlight data set transparency and sources of bias.RESULTS A total of 70 unique studies were included. Among these studies, 1 065 291 images were used to develop or test AI algorithms, of which only 257 372 (24.2%) were publicly available. Only 14 studies (20.0%) included descriptions of patient ethnicity or race in at least 1 data set used. Only 7 studies (10.0%) included any information about skin tone in at least 1 data set used. Thirty-six of the 56 studies developing new AI algorithms for cutaneous malignant neoplasms (64.3%) met the gold standard criteria for disease labeling. Public data sets were cited more often than private data sets, suggesting that public data sets contribute more to new development and benchmarks.CONCLUSIONS AND RELEVANCE This scoping review identified 3 issues in data sets that are used to develop and test clinical AI algorithms for skin disease that should be addressed before clinical translation: (1) sparsity of data set characterization and lack of transparency, (2) nonstandard and unverified disease labels, and (3) inability to fully assess patient diversity used for algorithm development and testing.",,,,,,,,,201,1,0,0,31,1,242,,,2168-6068,2168-6084,,,,,,,,,,"Stanford Sch Med, Stanford Dept Dermatol, 450 Broadway, Redwood City, CA 94061 USAStanford Sch Med, Stanford Dept Biomed Data Sci, Stanford, CA 94305 USAMem Sloan Kettering Canc Ctr, Dept Med, 1275 York Ave, New York, NY 10021 USAIcahn Sch Med Mt Sinai, New York, NY 10029 USAMem Sloan Kettering Canc Ctr, Dermatol Serv, 1275 York Ave, New York, NY 10021 USAStanford Univ, Dept Elect Engn, Stanford, CA 94305 USAStanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USAChan Zuckerberg Biohub, San Francisco, CA USA",,,,2021-09-22,WOS:000698780200001,34550305,
J,"Humadi, Mustafa; Abbas, Haider Hadi; Hilou, Hassan Waryoush; Najm, Nahlah. M. A. D.; Ali, Ammar Abdulkhaleq; Batumalay, M.",,,,,,,,,,,,,,,,,,International Journal of Engineering Science and Information Technology,,,,5,1,,,480,489,,,,10.52088/ijesty.v5i1.1288,,,,,,,Article,Dec 01 2024,2024,"The integration of Artificial Intelligence (AI) in Energy Management Systems changed completely how sustainable infrastructure operates?and is guarded. But the growing independence of AI decision-making presents some serious ethical questions about?fairness, transparency, and accountability. The article introduces a new framework with Ethical AI for Sustainable and Adaptive Energy Management Systems (EAI-SEM) that is designed to combine functional (re)configuration for operational control and ethical governance in centralized: smart buildings and?decentralized: nano-grid settings. The approach incorporates deep reinforcement learning for adaptive control, federated learning for privacy-preserving model updates, and an?integrated Ethics Verification Module for a dynamic assessment of privacy-conformance levels. In experimental simulations over 30-day operation of the smart building and 10-rounds of federated training of the nano-grid, unjust fairness deviation and explainability of the system experienced enhancements, which also indicated?the reduction of carbon dioxide emissions. The?study demonstrated that ethical protocols can be included without impacting on computational efficiency and system responsiveness. Additionally, the federated structure facilitated decentralized ethical responsibility across different actors and thus allowed for the scalable?implementation. The authors verify the possibility of integrating ethics into the computational core of?intelligent energy systems, near from auditing static policies, towards dynamic ethical choices. In the future the process innovation work could be applied to deployments in other infrastructure systems like water?systems and mobility systems, and it provides a reproducible model for the embedding of normative reasoning into AI for infrastructure.",,,,,,,,,0,0,0,0,0,0,0,,,,2775-2674,,,,,,,,,,,,,,2025-08-22,RC:158902743_S24,,
J,Karthik Ravva,,,,,,,,,,,,,,,,,,World Journal of Advanced Research and Reviews,,,,26,3,,,1035,1042,,,,10.30574/wjarr.2025.26.3.2167,,,,,,,Article,Jun 30 2025,2025,"Ensuring that AI models used in business intelligence systems comply with regulations represents a critical governance challenge as rapid development cycles enabled by MLOps on cloud platforms accelerate model deployment. Manual verification processes prove slow, error-prone, and unscalable in this environment. This article explores techniques and frameworks for automating compliance verification directly within cloud-based MLOps pipelines, investigating the integration of automated checks for fairness, explainability, privacy protection, and robustness testing. The integration of these verification capabilities as mandatory gates in the CICD pipeline transforms compliance from a periodic manual activity to an integral part of the development workflow. A reference architecture is proposed that leverages cloud-native services to enforce compliance checks, addressing the challenges of defining quantifiable metrics for complex regulations while enhancing the speed, reliability, and auditability of AI model governance in enterprise cloud environments. The proposed implementation demonstrates how organizations can balance regulatory adherence with innovation velocity, enabling responsible AI deployment at scale.",,,,,,,,,0,0,0,0,0,0,0,,,,2581-9615,,,,,,,,,,,,,,2025-08-05,RC:158196604_S24,,
J,"KIM, Tae Hyun; SON, Seok Jin",,Legal Research Institute of Chonnam National University,,,,,,,,,,,,,,,,Institute for Legal Studies Chonnam National University,,,,43,2,,,215,243,,,,10.38133/cnulawreview.2023.43.2.215,,,,,May 2023,,Article,May 31 2023,2023,"Recruitment is a process whereby a job seeker (employer) seeks to establish an employment relationship with a job applicant (worker) while attempting to minimize future risks by thoroughly evaluating the job seeker. For a long time, recruitment has been considered an area of free management behavior for employers. However, according to the Constitution, job seekers are entitled to dignity and the pursuit of happiness (Article 10), equality and protection against discrimination (Article 11(1)), freedom to choose their occupation (Article 15), public service (Article 25), the right to work (Article 32), and the right to live as a human being (Article 34). The right to equality is a fundamental norm that has the nature of a natural right and is widely applied today not only in the relationship between the state and individuals but also in the relationship between individuals and individuals. Therefore, it should be applied not only in the recruitment of employees by the state or public organizations but also in the recruitment of employees by private companies. A 2019 survey by the Korea Institute of Labor Research found that blind recruitment promotes fairness and that job competency through blind recruitment is relatively high. It is essential for our society to ensure fair recruitment opportunities, as work enables workers to make a living and realize their potential. It is our challenge to secure fairness in recruitment while selecting talented individuals with excellent job skills, and society will develop in a positive direction. Consequently, the Act on Fairness in Recruitment Procedures has been amended to expand the use of blind recruitment. While the regulations under the ｢Fair Hiring Procedure Act｣ may raise concerns about infringement on the freedom of business or enterprise, the public purpose of the law justifies it, even if there is some restriction on business or enterprise freedom. As a society, we must continue to discuss improvements and complementary points regarding the introduction and activation of blind recruitment. In the meantime, fairness has become a hot topic, and companies have been incorporating artificial intelligence (AI) into their recruitment processes in recent years. For human resources teams, AI interviews are the best way to promote “fairness” and “objective evaluation” in hiring, in addition to saving time and money. However, AI is prone to bias and discrimination. This has resulted in various discussions and legislation in the United States and Europe to prevent bias in AI recruitment and promote fairness, transparency, and accountability. Employers who want to use AI recruitment must obtain the consent of job seekers in advance, explain how AI recruitment works to job seekers, and undergo an external audit of its fairness. As legal liability, fairness, and privacy will likely become issues in Korea due to AI recruitment in the future, it is necessary to revise the ｢Fair Hiring Procedure Act｣ by referring to legislative examples and actual cases in the United States and Europe to prepare for this.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,2025-07-10,RC:142344169_S24,,
J,"Fan, Sizheng; Zhang, Hongbo; Zeng, Yuchen; Cai, Wei",,,,"Cai, Wei/AAC-4630-2022; Zeng, Yuchen/W-4920-2017","Cai, Wei/0000-0002-4658-0034;",,,,,Hybrid Blockchain-Based Resource Trading System for Federated Learning in Edge Computing,,,,,,,,IEEE INTERNET OF THINGS JOURNAL,,,,8,4,,,2252,2264,,,,10.1109/JIOT.2020.3028101,,,,,,,Article,FEB 15 2021,2021,"By training a machine learning algorithm across multiple decentralized edge nodes, federated learning (FL) ensures the privacy of the data generated by the massive Internet-of-Things (IoT) devices. To economically encourage the participation of heterogeneous edge nodes, a transparent and decentralized trading platform is needed to establish a fair market among distinct edge companies. In this article, we propose a hybrid blockchain-based resource trading system that combines the advantages of both public and consortium blockchains. We design and implement a smart contract to facilitate an automatic, autonomous, and auditable rational reverse auction mechanism among edge nodes. Moreover, we leverage the payment channel technique to enable credible, fast, low-cost, and high-frequency payment transactions between requesters and edge nodes. Simulation results show that the proposed reverse auction mechanism can achieve the properties, including budget feasibility, truthfulness, and computational efficiency.",,,,,,,,,87,6,0,0,0,0,125,,,2327-4662,,,,,,,,,,,"Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R ChinaShenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen, Peoples R China",,,,2021-03-14,WOS:000616317000014,,
J,"Son,, Seok-Jin; KIM, TAE-HYUN",,,,,,,,,,A Study on Fair Hiring Procedures,,,공정채용 절차에 대한 소고,,,,,,법학논총,,,43,2,,,215,243,,,,,,,,,,,research-article,2023,2023,"Recruitment is a process whereby a job seeker (employer) seeks to establish an employment relationship with a job applicant (worker) while attempting to minimize future risks by thoroughly evaluating the job seeker. For a long time, recruitment has been considered an area of free management behavior for employers. However, according to the Constitution, job seekers are entitled to dignity and the pursuit of happiness (Article 10), equality and protection against discrimination (Article 11(1)), freedom to choose their occupation (Article 15), public service (Article 25), the right to work (Article 32), and the right to live as a human being (Article 34). The right to equality is a fundamental norm that has the nature of a natural right and is widely applied today not only in the relationship between the state and individuals but also in the relationship between individuals and individuals. Therefore, it should be applied not only in the recruitment of employees by the state or public organizations but also in the recruitment of employees by private companies.A 2019 survey by the Korea Institute of Labor Research found that blind recruitment promotes fairness and that job competency through blind recruitment is relatively high. It is essential for our society to ensure fair recruitment opportunities, as work enables workers to make a living and realize their potential. It is our challenge to secure fairness in recruitment while selecting talented individuals with excellent job skills, and society will develop in a positive direction. Consequently, the Act on Fairness in Recruitment Procedures has been amended to expand the use of blind recruitment.While the regulations under the 「Fair Hiring Procedure Act」 may raise concerns about infringement on the freedom of business or enterprise, the public purpose of the law justifies it, even if there is some restriction on business or enterprise freedom. As a society, we must continue to discuss improvements and complementary points regarding the introduction and activation of blind recruitment.In the meantime, fairness has become a hot topic, and companies have been incorporating artificial intelligence (AI) into their recruitment processes in recent years. For human resources teams, AI interviews are the best way to promote “fairness” and “objective evaluation” in hiring, in addition to saving time and money. However, AI is prone to bias and discrimination. This has resulted in various discussions and legislation in the United States and Europe to prevent bias in AI recruitment and promote fairness, transparency, and accountability. Employers who want to use AI recruitment must obtain the consent of job seekers in advance, explain how AI recruitment works to job seekers, and undergo an external audit of its fairness. As legal liability, fairness, and privacy will likely become issues in Korea due to AI recruitment in the future, it is necessary to revise the 「Fair Hiring Procedure Act」 by referring to legislative examples and actual cases in the United States and Europe to prepare for this.",,,,"채용은 구인자(사용자)가 구직자(근로자)와 근로계약 관계를 형성하기 위해 진행하는 절차이다. 이때, 구인자는 구직자를 최대한 파악함으로써 장래 있을 수 있는 위험을 최소화하고자 노력한다. 오랜 기간 채용은 사용자의 자유로운 경영행위의 영역으로 보장되어 왔었다. 구직자는 헌법에 의하여 제10조 인간의 존엄성 및 행복 추구권, 제11조 제1항 평등권 및 차별받지 않을 권리, 제15조 직업선택의 자유, 제25조 공무담임권, 제32조 근로권, 제34조 인간답게 살 권리를 가진다. 평등권은 자연권으로서의 성격을 가지는 근본규범으로 오늘날 국가와 개인 사이의 관계뿐만 아니라 개인과 개인 사이의 관계에서도 널리 적용되므로 국가나 공공단체의 직원채용뿐만 아니라 사기업의 직원채용에서도 적용되어야 한다. 2019년 한국노동연구원에 의한 조사 결과 블라인드 채용이 공정성을 높이는 데 기여하고 있고, 블라인드 채용을 통한 직무 역량도 비교적 높은 것으로 나타났다. 근로는 근로자가 생활의 영위뿐만 아니라 자아실현을 가능하게 하므로 공정한 채용의 기회를 보장받는 것은 우리 사회에 있어 매우 중대한 일이 아닐 수 없다. 채용에 있어서 직무능력이 뛰어난 인재를 선발하면서도 공정성을 확보하는 것이 우리 시대의 과제이고 이를 통해 사회는 유익한 방향으로 발전할 수 있을 것이다. 그리고 그 함의를 담아 블라인드 채용의 사회적 확대를 위해 「채용절차의 공정화에 관한 법률」이 개정되었다.채용절차법상의 규제가 사용자의 영업의 자유 내지 기업의 자유를 침해하는 것은 아닌지 문제될 수 있으나, 법률이 달성하려는 공익적 목적이 큰 만큼 사용자의 영업의 자유 내지 기업의 자유가 다소 제한되는 면이 있더라도 정당하다고 보아야 할 것이다. 우리 사회는 이러한 블라인드 채용의 도입 및 활성화와 관련하여 개선･보완점에 대한 논의를 지속해 나아갈 필요가 있다.한편, 공정성이 시대적 화두가 됨에 따라 최근 들어서는 기업에서 인공지능(AI)을 채용프로세스 상에 앞다투어 적용하고 있다. 기업의 인사팀에게 AI 면접은 비용과 시간을 절약할 뿐만 아니라 채용의 ‘공정성’과 ‘객관적 평가’를 홍보할 수 있는 가장 좋은 수단이기 때문이다. 다만 AI는 편향성과 차별이 나타날 가능성이 높다. 이와 같은 문제로 최근 미국, 유럽에서는 AI 채용에 따른 편향성을 방지하고 공정성․투명성․책임성을 제고하기 위한 다양한 논의가 진행되고 입법화되고 있다. AI 채용을 활용하려는 구인자는 사전에 구직자의 동의를 받고 AI 채용의 작동방식을 구직자에게 설명하며, 그 공정성에 대해 외부 감사를 받아야 한다는 것이 주된 내용이다. 우리나라에서도 향후 AI 채용으로 인한 법적 책임, 공정성, 개인정보 보호 등이 문제가 될 것이 명약관화한바, 미국이나 유럽의 입법례 및 실제 사례를 참고하여 채용절차법의 개정을 통해 이에 대비하여야 할 것이다.",,,,,0,0,0,0,0,0,0,,,1738-6233,,,,,,,,,,,,,,,2023-01-01,KJD:ART002960794,,
J,Oyegoke Oyebode,,,,,,,,,,,,,,,,,,International Journal of Science and Research Archive,,,,15,2,,,1876,1896,,,,10.30574/ijsra.2025.15.2.1613,,,,,,,Article,May 30 2025,2025,"The rapid growth of artificial intelligence (AI) in decentralized systems such as healthcare, financial networks, and autonomous transportation has underscored the critical need for interpretability, fairness, and verifiable trust in decision-making. Traditional federated learning frameworks, while addressing data privacy and scalability, often suffer from bias propagation, opaque model behaviors, and limited mechanisms for ensuring accountability. This article introduces Chain-of-Trust AI, a novel paradigm that integrates zero-knowledge proofs (ZKPs), federated reinforcement learning (FRL), and generative learning models to create an interpretable, bias-free, and verifiable decision-making framework for complex distributed environments. The proposed framework leverages FRL to enable adaptive coordination across heterogeneous agents while maintaining local data sovereignty. Generative learning models, such as variational autoencoders, provide transparent causal representations that support bias detection and enhance interpretability of reinforcement-driven policies. ZKPs are embedded as cryptographic guarantees to verify model updates and decision outcomes without exposing sensitive information, thus ensuring compliance, trust, and transparency across decentralized networks. Methodologically, the framework is evaluated through MATLAB-based multi-agent simulations, benchmarking performance in terms of interpretability, fairness indices, convergence stability, and verification overhead. Theoretical analyses confirm convergence under heterogeneous reward structures, cryptographic soundness of proofs, and bias reduction capabilities through generative regularization. Case studies in decentralized healthcare diagnostics, financial fraud detection, and autonomous vehicular coordination highlight the practical scalability and robustness of Chain-of-Trust AI. By uniting reinforcement learning, generative interpretability, and zero-knowledge verification, this work pioneers a secure, auditable, and ethically aligned AI architecture for decentralized complex systems, advancing both technical rigor and governance in distributed intelligence.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-8185,,,,,,,,,,,,,,2025-09-13,RC:159443911_S24,,
J,Jeffrey Chidera Ogeawuchi,,,,,,,,,,,,,,,,,,International Journal of Scientific Research in Computer Science Engineering and Information Technology,,,,10,2,,,905,917,,,,10.32628/cseit24102141,,,,,,,Article,Apr 30 2024,2024,"The integration of artificial intelligence into financial decision-making has introduced transformative efficiencies and competitive advantages across domains such as algorithmic trading, credit risk assessment, fraud detection, and customer profiling. However, these advancements raise profound ethical concerns, particularly in high-stakes environments where opaque models and biased algorithms can perpetuate discrimination, reduce accountability, and compromise consumer trust. This paper critically investigates the ethical implications of AI-driven financial systems, emphasizing the risks of algorithmic bias, the challenge of model interpretability, and the urgency of safeguarding data privacy. By drawing on normative ethical principles—fairness, accountability, transparency, and human oversight—the study proposes a comprehensive governance framework to guide the ethical lifecycle of AI deployment in finance. It evaluates the role of financial institutions, regulatory bodies, and central banks in setting enforceable standards, while offering a practical model for integrating ethics from design to audit. The paper concludes by reflecting on the responsibilities of key stakeholders and outlining future research and policy directions to ensure that AI innovations support not only profitability but also inclusive and socially responsible financial ecosystems.",,,,,,,,,0,0,0,0,0,0,0,,,,2456-3307,,,,,,,,,,,,,,2025-08-04,RC:157482317_S24,,
J,"Murikah, Wilberforce; Nthenge, Jeff Kimanga; Musyoka, Faith Mueni",,,,"Murikah, Wilberforce/NXB-8952-2025; musyoka, faith/LZH-4756-2025","Nthenge, Jeff/0009-0009-3009-9770; Murikah, Wilberforce/0009-0005-2688-2069",,,,,Bias and ethics of AI systems applied in auditing - A systematic review,,,,,,,,SCIENTIFIC AFRICAN,,,,25,,,,,,e02281,,,10.1016/j.sciaf.2024.e02281,,,,,JUN 2024,,Review,SEP 2024,2024,"The integration of artificial intelligence into auditing shows great potential in enhancing automation and gaining insights from complex data. However, it also presents significant ethical challenges, including algorithmic biases, transparency, accountability, and fairness. This study aimed to investigate the sources of bias and risks posed by AI systems applied in auditing and the complex downstream interactions and effects they have. The study also explored the technical and ethical guardrails proposed and recommendations for translating principles into auditing practice. A systematic methodology was employed to acquire relevant studies across scientific databases. This involved a three-step process, including a targeted search query using Boolean operators and snowballing to yield 310 preliminary publications. A systematic review process was then conducted to identify 123 relevant articles focused on AI's implications for auditing, accounting, finance, or assurance contexts. Finally, screening and filtering on research quality distilled 83 high-quality publications from the year 2018 to 2023 spanning computer science, accounting, management science, and ethics disciplines. The analysis revealed five primary sources driving technical and human biases: data deficiencies, demographic homogeneity, spurious correlations, improper comparators, and cognitive biases. It also highlighted wider issues, such as trade-offs between efficiency and diligence, erosion of human skills and judgement, data dependence risks, and privacy violations from uncontrolled personal data exploitation. The study found promising remedies, including causal modeling to enable auditors to uncover subtle biases, representative algorithmic testing to evaluate fairness, periodic auditing of AI systems, human oversight alongside automation, and embedding ethical values like fairness and accountability into system design. The study concludes that auditors play a crucial role in assessing and ensuring AI's reliable and socially beneficial integration. It recommends governance, risk assessment before deployment, ongoing performance monitoring, and policies fostering trust and collaboration to responsibly translate principles into auditing practice.",,,,,,,,,17,0,0,0,2,0,35,,,2468-2276,,,,,,,,,,,"US Int Univ Africa, Dept Comp & Informat, Nairobi, KenyaUniv Embu, Dept Comp & Informat Technol, Embu, Kenya",Univ Embu,,,2024-11-11,WOS:001345332900001,,
J,"Neuman, Erica L.; Sheu, Robert J.",,,,,"Sheu, Robert/0000-0003-4000-8173; Neuman, Erica/0000-0002-0774-4243",,,,,Big Data Analytics in IRS Audit Procedures and Its Effects on Tax Compliance: A Moderated Mediation Analysis,,,,,,,,JOURNAL OF THE AMERICAN TAXATION ASSOCIATION,,,,44,2,,,97,113,,,,10.2308/JATA-2020-038,,,,,,,Article,FAL 2022,2022,"Big Data analytics could be a panacea for the IRS by enabling creation of taxpayer profiles to better capture noncompliance using artificial intelligence and machine learning, requiring fewer costly manpower hours. Privacy, fair information practices, and embedded biases are critiques of such practices, and it is unknown how taxpayers will respond. Deterrence theory suggests improved audit effectiveness will increase compliance but excludes elements of tax morale, including perceived fairness. We find evidence supporting a moderated mediation model where procedural fairness mediates the relationship between audit procedures and tax compliance, moderated by participatory monitoring, which captures how effects vary when taxpayers willingly increase traceability of their income by advertising online. When taxpayers advertise business online, use of advanced technologies in audit selection significantly increases compliance with no significant effect on perceived fairness; when they do not, use of advanced technologies has no effect on compliance, but significantly decreases perceived fairness.",,,,,,,,,5,0,0,0,0,0,5,,,0198-9073,1558-8017,,,,,,,,,,"Univ Dayton, Sch Business Adm, Dept Accounting, Dayton, OH 45469 USACase Western Reserve Univ, Weatherhead Sch Management, Dept Accountancy, Cleveland, OH 44106 USA",,,,2023-04-28,WOS:000965882900005,,
J,Nishant Nisan Jha,,,,,,,,,,,,,,,,,,Journal of Computer Science and Technology Studies,,,,7,4,,,573,579,,,,10.32996/jcsts.2025.7.4.67,,,,,,,Article,May 09 2025,2025,"The integration of artificial intelligence into cloud observability systems has revolutionized infrastructure monitoring while simultaneously introducing equity challenges that disproportionately affect underserved populations. These AI-driven systems, predominantly trained on data from high-density urban environments, frequently exhibit biased performance that manifests as prolonged resolution times and decreased detection accuracy in rural and developing regions. As cloud infrastructure increasingly underpins critical services such as healthcare, education, and financial systems, these disparities represent significant barriers to digital inclusion for billions of users worldwide. This article presents ethical AI auditing as a comprehensive framework to identify, quantify, and mitigate these biases through three key components: synthetic data generation to represent underserved scenarios, fairness metrics implementation to establish quantitative benchmarks, and bias mitigation techniques to correct algorithmic disparities. Case studies across European cloud providers, global content delivery networks, and emergency response systems demonstrate substantial improvements in service equity following audit implementation. Despite challenges related to resource requirements, performance trade-offs, privacy considerations, and evolving regulatory landscapes, ethical AI audits offer a viable path toward equitable cloud resilience that benefits both marginalized users and service providers through expanded market reach, enhanced reputation, and improved regulatory compliance.",,,,,,,,,0,0,0,0,0,0,0,,,,2709-104X,,,,,,,,,,,,,,2025-08-04,RC:157480888_S24,,
J,"SUNYONG, BYUN",,,,,,,,,,A study on data ethics required in the age of artificial intelligence,,,데이터윤리의 의미와 원칙에 대한 연구,,,,,KOREAN ELEMENTARY MORAL EDUCATION SOCIETY,초등도덕교육,,,83,,,,189,213,,,,,,,,,,,research-article,2023,2023,"The emphasis of data ethics is shifting from big data to the importance of AI learning data, and certification and verification of what learning data AI has learned from will become an important criterion for AI's social acceptability.In this regard, this study aims to deal with the following topics in order to emphasize the importance of data ethics required in the age of artificial intelligence. First, it examines why data ethics guidelines are necessary in the age of artificial intelligence. Second, in the discussion of data ethics, how data ethics is defined is explored. Third, cases in which data ethics principles are being prepared are identified. Fourth, a theoretical framework of data ethics is constructed from related cases. Based on this, I would like to ultimately present the meaning and principles of data ethics.Data ethics should be defined as faithfully fulfilling responsibility for the entire process of data in order to protect the rights of each subject regarding data, and consists of data rights and data responsibilities. As the principles of data ethics, privacy protection, publicity, fairness, damage minimization, reliability, solidarity, transparency, sustainability of audit, and safety should be presented, and it should be emphasized that the contents of rights and responsibilities are detailed for each relevant subject.",,,,"데이터윤리의 강조점이 빅데이터에서 인공지능 학습용 데이터의 중요성으로 이동하고 있으며, 앞으로 우리가 이용하게 될 인공지능이 어떤 학습데이터를 가지고 학습했는지에 대한 인증과 검증이 인공지능의 사회적 수용성의 중요한 기준으로 작용할 것으로 예측된다. 이에 본 연구는 인공지능 시대에 요구되는 데이터윤리에 대한 심층적인 연구를 수행하기 위해 다음과 같은 주제를 다루고자 한다. 첫째, 인공지능 시대에서 데이터윤리 가이드라인이 필요한 이유를 살펴본다. 둘째, 데이터윤리에 대한 논의에서 데이터윤리가 어떻게 정의되고 있는지를 탐색한다. 셋째, 데이터윤리 원칙이 마련되고 있는 사례들을 파악한다. 넷째, 관련된 사례들로부터 데이터윤리의 이론적 프레임워크를 구성한다. 이를 바탕으로 궁극적으로는 데이터윤리의 의미와 원칙들을 제시하고자 한다. 데이터윤리는 데이터에 대한 각 주체의 권리를 보호하기 위하여 데이터의 전 과정에 대한 책임을 성실히 이행하는 것으로 정의되어야 하고, 데이터 권리와 데이터 책임으로 구성된다. 데이터윤리의 원칙으로는 프라이버시 보호, 공공성, 공정성, 피해최소화, 신뢰성, 연대성, 투명성, 검토의 지속가능성, 그리고 안전성이 제시되어야 하고, 이에 대한 권리와 책임의 내용이 관련주체별로 상세화되는 것이 강조되어야 한다.",,,,,0,0,0,0,0,0,0,,,1598-3110,,,,,,,,,,,,,,,2024-06-25,KJD:ART002976179,,
J,"Pulina, Tetiana; Kuts, Andrii; Yudytskyi, Vladyslav",,,,,,,,,,,,,,,,,,Economic scope,,,,,203,,,216,222,,,,10.30838/ep.203.216-222,,,,,,,Article,Aug 13 2025,2025,"The article explores the ethical challenges associated with the implementation of artificial intelligence (AI) in recruitment processes, using the case of «SoftServe» LLC as an illustrative example. Special attention is given to the risks of algorithmic bias and the potential discrimination of candidates based on gender, age, ethnicity, and social background, as well as to possible violations of the rights to privacy, data protection, and access to fair and transparent assessment procedures. The study investigates the issue of limited transparency of algorithmic systems, emphasizing their «black box» nature, which complicates the identification of the sources of errors, biases, and unjustified exclusions.The key factors contributing to ethical risks are identified, including the use of historically biased datasets, insufficient representativeness of training samples, low interpretability of models, and the absence of mechanisms for independent audit of automated decision-making processes. The author highlights the need for an interdisciplinary approach to AI development in recruitment, involving close cooperation between technical experts, HR professionals, ethicists, and legal advisors.It is argued that companies should develop internal policies and guidelines for the ethical use of automated hiring tools, based on the principles of transparency, accountability, inclusiveness, and continuous model evaluation. The study proposes mandatory human oversight at final stages of decision-making and underscores the importance of involving HR specialists in the design, testing, and ongoing monitoring of AI systems. This human-centered approach not only mitigates risks of discrimination but also fosters greater candidate trust and improves hiring quality.The practical significance of the research lies in the development of a set of comprehensive, applicable recommendations for IT companies. These aim to ensure the ethical and responsible integration of AI into recruitment workflows while maintaining a balance between technological efficiency and respect for fundamental human rights and freedoms. The proposed approach contributes to building a sustainable and trust-based digital culture within human resource management.",,,,,,,,,0,0,0,0,0,0,0,,,,2224-6290,,,,,,,,,,,,,,2025-08-22,RC:158999687_S24,,
J,"Pentyala, Sikha; Melanson, David; De Cock, Martine; Farnadi, Golnoosh",,,,"Pentyala, Sikha/IQU-1384-2023; De Cock, Martine/B-8567-2009",,,,,,PrivFair: a Library for Privacy-Preserving Fairness Auditing,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2202.04058,,,,,,,preprint,May 23 2022,2022,"Machine learning (ML) has become prominent in applications that directly affect people's quality of life, including in healthcare, justice, and finance. ML models have been found to exhibit discrimination based on sensitive attributes such as gender, race, or disability. Assessing if an ML model is free of bias remains challenging to date, and by definition has to be done with sensitive user characteristics that are subject of anti-discrimination and data protection law. Existing libraries for fairness auditing of ML models offer no mechanism to protect the privacy of the audit data. We present PrivFair, a library for privacy-preserving fairness audits of ML models. Through the use of Secure Multiparty Computation (MPC), PrivFair protects the confidentiality of the model under audit and the sensitive data used for the audit, hence it supports scenarios in which a proprietary classifier owned by a company is audited using sensitive audit data from an external investigator. We demonstrate the use of PrivFair for group fairness auditing with tabular data or image data, without requiring the investigator to disclose their data to anyone in an unencrypted manner, or the model owner to reveal their model parameters to anyone in plaintext.",,,,,,,,,2,0,0,0,1,0,2,,,,,,,,,,,,,,"Mila Quebec Inst, Montreal, QC, CanadaUniv Washington Tacoma, Sch Engn & Technol, Tacoma, WA 98402, USAGhent Univ, Dept Appl Math, Comp Sci & Stat, Ghent, BelgiumHEC Montreal, Dept Decis Sci, Montreal, QC, Canada",Mila Quebec InstUniv Washington TacomaHEC Montreal,,,2022-11-23,PPRN:12841288,,
J,"Ray, Partha Pratim",,,,"Ray, Partha Pratim/HRC-0757-2023","Ray, Partha Pratim/0000-0003-2306-2792",,,,,Toward Transparent AI-Enabled Patient Selection in Cosmetic Surgery by Integrating Reasoning and Medical LLMs,,,,,,,,AESTHETIC PLASTIC SURGERY,,,,,,,,,,,,,10.1007/s00266-025-05038-w,,,,,JUN 2025,,Letter; Early Access,,2025,"Existing AI solutions-like the XGBoost tool by Li et al.-show potential for preoperative screening but rely on fixed questionnaires and opaque feature weighting. We introduce a hybrid framework that combines reasoning LLMs (OpenAI o3, DeepSeek R1, Google Gemini 2.5, Anthropic Claude 3.7 Sonnet) with specialty medical models (Baichuan-M1, Zhipu AI GLM-4-9B-Chat, OpenBioLLM-Llama-70B, MedLLaMA3-v20, Med-PaLM 2, SurgeryLLM). Patient inputs-structured and free-text-are ingested via a secure mobile app and processed through a retrieval-augmented pipeline. Reasoning LLMs expose chain-of-thought steps for full transparency, while medical LLMs validate each risk factor against clinical guidelines. An ensemble then delivers a composite suitability score, complete with an audit trail of data points and citations. We address key hurdles-model recency, hallucination control, data privacy, and fairness-and recommend a medical-device regulatory approach with independent validation, ongoing bias monitoring, and co-design with multidisciplinary stakeholders. Level of Evidence V This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266.",,,,,,,,,0,0,0,0,0,0,0,,,0364-216X,1432-5241,,,,,,,,,,"Sikkim Univ, Dept Comp Applicat, Gangtok, Sikkim, India",,,,2025-07-01,WOS:001516989800001,40563006,
J,"Birahim, Shaikh Afnan",,,,,,,,,,Contesting the algorithm: advancing a right to challenge AI decisions under the GDPR for algorithmic fairness,,,,,,,,TRANSFORMING GOVERNMENT- PEOPLE PROCESS AND POLICY,,,,,,,,,,,,,10.1108/TG-05-2025-0148,,,,,SEP 2025,,Article; Early Access,,2025,"Purpose - This study aims to challenge the adequacy of Article 22 of the General Data Protection Regulation (GDPR) in safeguarding individuals against harmful automated decisions. It argues that explainability alone is insufficient for algorithmic accountability and proposes a legally enforceable right to contest such decisions. Through comparative legal analysis, it reveals the shortcomings of current GDPR protections and advocates for an amended framework that empowers individuals with substantive rights and remedies. The goal is to enable individuals not only to understand but also to challenge, correct or overturn artificial intelligence (AI)-driven decisions that significantly affect their lives.Design/methodology/approach - This study adopts a qualitative comparative case study approach, analyzing enforcement and contestability in eight jurisdictions: four under the GDPR (The Netherlands, UK, France and Germany) and four outside it (California, New York City and Canada - public and private sectors). Data sources include legal texts, academic literature, court rulings and policy documents. A structured analytical matrix was applied to assess algorithm type, sector, availability of contestability mechanisms and enforcement effectiveness. This desk-based comparative legal analysis triangulates secondary sources to identify regulatory gaps and formulate reform proposals for strengthening contestability rights in AI governance.Findings - The analysis reveals that GDPR Article 22 is functionally weak due to vague language, broad exceptions and limited enforcement. In practice, individuals rarely access meaningful mechanisms to contest consequential AI-driven decisions. By contrast, non-GDPR jurisdictions such as California and Canada show more proactive governance through opt-out rights, bias audits and algorithmic impact assessments. This study finds that effective contestability requires not only individual rights but also institutional safeguards, including human-in-the-loop review, independent oversight and public accountability mechanisms. Transparency alone is insufficient - robust, enforceable procedural rights are essential to ensure fairness and protect affected individuals.Originality/value - This paper offers a novel ethical and legal case for rethinking algorithmic fairness beyond explainability, introducing a structured proposal to amend GDPR Article 22. It moves the discourse from transparency to contestability, grounded in comparative case analysis across EU and non-EU jurisdictions. The work bridges theoretical critique and practical reform, offering actionable policy recommendations, including an explicit right to contest, standards for human review and regulatory oversight models. It contributes original insights into how algorithmic harms can be addressed through due process-based contestation rights, reinforcing autonomy, fairness and justice in AI governance.",,,,,,,,,0,0,0,0,0,0,0,,,1750-6166,1750-6174,,,,,,,,,,"Univ Glasgow, Sch Comp Sci, Glasgow, Scotland",,,,2025-10-01,WOS:001577295500001,,
J,"Bourree, Jade Garcia; Lautraite, Hadrien; Gambs, Sebastien; Tredan, Gilles; Le Merrer, Erwan; Rottembourg, Benoit",,,,,,,,,,P2NIA: Privacy-Preserving Non-Iterative Auditing,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2504.00874,,,,,,,preprint,Apr 01 2025,2025,"The emergence of AI legislation has increased the need to assess the ethical compliance of high-risk AI systems. Traditional auditing methods rely on platforms' application programming interfaces (APIs), where responses to queries are examined through the lens of fairness requirements. However, such approaches put a significant burden on platforms, as they are forced to maintain APIs while ensuring privacy, facing the possibility of data leaks. This lack of proper collaboration between the two parties, in turn, causes a significant challenge to the auditor, who is subject to estimation bias as they are unaware of the data distribution of the platform. To address these two issues, we present P2NIA, a novel auditing scheme that proposes a mutually beneficial collaboration for both the auditor and the platform. Extensive experiments demonstrate P2NIA's effectiveness in addressing both issues. In summary, our work introduces a privacy-preserving and non-iterative audit scheme that enhances fairness assessments using synthetic or local data, avoiding the challenges associated with traditional API-based audits.",,,,,,,,,0,0,0,0,0,0,0,,,,,,,,,,,,,,"Univ Rennes, Inria, CNRS, IRISA, Rennes, FranceUniv Quebec Montreal, Montreal, PQ, CanadaCNRS, LAAS, Toulouse, FranceInria, Paris, France",Univ RennesUniv Quebec MontrealCNRS,,,2025-08-07,PPRN:122761937,,
J,Jayaprakash Ramsaran,,,,,,,,,,,,,,,,,,Journal of Computer Science and Technology Studies,,,,7,5,,,314,327,,,,10.32996/jcsts.2025.7.5.39,,,,,,,Article,May 29 2025,2025,"This article presents a comprehensive framework for implementing responsible artificial intelligence in revenue lifecycle automation systems. As organizations increasingly deploy AI to enhance revenue operations through contract analysis, pricing optimization, and approval workflows, they face complex ethical considerations and compliance challenges. The framework addresses these challenges through five interconnected domains: fairness in algorithmic decision-making, explainability and transparency, data governance and privacy, human-in-the-loop controls, and compliance and auditability. Drawing from real-world implementations across financial services, technology, and regulated industries, the article outlines practical design patterns that balance innovation with ethical considerations. Case studies demonstrate how organizations have successfully applied these principles to contract intelligence and dynamic pricing systems, achieving both business value and ethical implementation. The article provides a phased implementation roadmap and explores current challenges and future research directions. By embedding responsible AI principles into revenue operations, organizations can mitigate risks while maximizing business value, ensuring systems operate equitably, transparently, and in alignment with organizational values and regulatory requirements.",,,,,,,,,0,0,0,0,0,0,0,,,,2709-104X,,,,,,,,,,,,,,2025-08-04,RC:157754660_S24,,
J,Nayab Farooq,,,,,,,,,,,,,,,,,,Annual Methodological Archive Research Review,,,,3,6,,,1,18,,,,10.63075/nb02bq32,,,,,,,Article,May 29 2025,2025,"By allowing highly targeted ads, tailored campaigns, and automated judgments, artificial intelligence (AI) is revolutionizing the advertising sector. These tools create major ethical questions even if efficiency increases output. They endanger trust, fairness, and consumer rights. The ethical problems in AI-driven advertising—including biased algorithms, manipulative strategies, privacy violations, lack of transparency, and hazards to consumer confidence—are examined in this study article under ethical perspective. AI systems can, for example, use personal data to affect consumer behavior in ways that blur the boundaries between persuasion and manipulation. Trained on faulty data, biased algorithms could ethically reject people or propagate prejudices. The article explores practical solutions to the ethical challenges raised by the use of artificial intelligence into advertising by combining academic study findings and regulatory agency guidelines. One of them is the development of ethical guidelines for the development of artificial intelligence (such as fairness and privacy concerns). By using thorough audit methods, biases can be discovered and transparency increased. The findings emphasize the significance of maintaining a balance between innovative thinking and accountability. Companies can build confidence and prevent harm by emphasizing ethical procedures. These principles include clarifying artificial intelligence-driven decisions, respecting user consent, and limiting data collection. According to the study's conclusions, long-term development in advertising driven by artificial intelligence depends on properly linking ethical duty with technological advancement. Unless these concerns are addressed, businesses run the risk of facing backlash from the public, legal challenges, and the loss of long-term consumer confidence. The ultimate objective is to ensure that the use of artificial intelligence in marketing does not violate the rights of consumers, in addition to maintaining fairness and respect for human dignity.",,,,,,,,,0,0,0,0,0,0,0,,,,3007-3197,,,,,,,,,,,,,,2025-08-04,RC:157638192_S24,,
J,"Yazdinejad, Abbas; Dehghantanha, Ali; Srivastava, Gautam",,,,"; Yazdinejad, Abbas/X-5602-2019; Srivastava, Gautam/N-5668-2019","Srivastava, Gautam/0000-0001-9851-4103;",,,,,AP2FL: Auditable Privacy-Preserving Federated Learning Framework for Electronics in Healthcare,,,,,,,,IEEE TRANSACTIONS ON CONSUMER ELECTRONICS,,,,70,1,,,2527,2535,,,,10.1109/TCE.2023.3318509,,,,,,,Article,FEB 2024,2024,"The growing application of machine learning (ML) techniques in healthcare has led to increased interest in federated learning (FL), which enables the secure and private training of robust ML models. However, conventional FL methods often fall short of providing adequate privacy protection and face challenges in handling non-independent and identically distributed (Non-IID) training data. These shortcomings are of significant concern when employing FL in electronic devices in healthcare. To address these issues, we propose an Auditable Privacy-Preserving Federated Learning (AP2FL) model tailored for electronics in healthcare settings. By leveraging Trusted Execution Environments (TEE), AP2FL ensures secure training and aggregation processes on both client and server sides, effectively mitigating data leakage risks. To manage Non-IID data within the proposed framework, we incorporate the Active Personalized Federated Learning (ActPerFL) model and Batch Normalization (BN) techniques to consolidate user updates and identify data similarities. Additionally, we introduce an auditing mechanism in AP2FL that reveals the contribution of each client to the FL process, facilitating the updating of the global model following diverse data types and distributions. In other words, it ensures the FL process's integrity, transparency, fairness, and robustness. Our results demonstrate that the proposed AP2FL model outperforms existing methods in accuracy and effectively eliminates privacy leakage.",,,,,,,,,54,0,0,0,1,0,59,,,0098-3063,1558-4127,,,,,,,,,,"Univ Guelph, Cyber Sci Lab, Canada Cyber Foundry, Guelph, ON N1G 4S7, CanadaBrandon Univ, Dept Math & Comp Sci, Brandon, MB R7A 6A9, CanadaChina Med Univ, Res Ctr Interneural Comp, Taichung 404, TaiwanLebanese Amer Univ, Dept Comp Sci & Math, Beirut 11022801, Lebanon",,,,2024-06-17,WOS:001244813000353,,
J,Nisheedh Raveendran,,,,,,,,,,,,,,,,,,Global Journal of Engineering and Technology Advances,,,,24,1,,,16,27,,,,10.30574/gjeta.2025.24.1.0212,,,,,,,Article,Jul 30 2025,2025,"Information retrieval systems face unprecedented regulatory complexity as healthcare privacy requirements, European data protection mandates, and digital market oversight converge to create multifaceted compliance challenges. The Health Insurance Portability and Accountability Act extends beyond traditional medical records to encompass search queries, user interactions, and behavioral analytics within healthcare environments, requiring sophisticated access controls and audit mechanisms. The General Data Protection Regulation introduces comprehensive data subject rights, including erasure, portability, and consent management, that demand fundamental architectural modifications across distributed processing systems. Digital Markets Act obligations for gatekeeper platforms mandate algorithmic transparency, interoperability requirements, and fairness monitoring that conflict with traditional optimization objectives. Technical implementation challenges encompass data minimization principles in large-scale indexing, cross-border data transfer mechanisms, machine learning model explainability, and bias detection across diverse user populations. Privacy-preserving technologies, including differential privacy, federated learning, and homomorphic encryption, offer pathways for maintaining compliance while preserving analytical capabilities, though practical deployment requires substantial expertise and computational overhead. Compliance-focused architecture patterns emphasizing modular audit systems, comprehensive data governance, and flexible design principles enable adaptation to evolving regulatory requirements. The regulatory landscape continues evolving rapidly with emerging artificial intelligence governance frameworks, cross-border enforcement coordination, and industry standardization efforts that will reshape information retrieval system development.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-5003,,,,,,,,,,,,,,2025-08-04,RC:157943943_S24,,
J,"Muchtar, Jarot; Soeleman, Moch Arief",,,,,,,,,,,,,,,,,,Jurnal Ilmu Kepolisian,,,,19,2,,,15,24,,,,10.35879/jik.v19i2.646,,,,,,,Article,Aug 26 2025,2025,"We develop a predictive model to enhance digital leadership among Indonesian National Police (Polri) officers, addressing the pressing need for technological proficiency in modern law enforcement. Digital leadership, vital for combating cyber threats and improving operational efficiency, remains underdeveloped in Polri due to limited technological skills and a lack of systematic leadership identification. We train machine learning models on 564 anonymized officer records, incorporating attributes like rank, position, and education, guided by Transformational, Adaptive, and Contingency Leadership theories. The Light GBM model excels, achieving an F1 Score of 0.9674, a Log Loss of 0.2244, a Cohen's Kappa of 0.9616, and a Matthews Correlation Coefficient of 0.9620, demonstrating high predictive accuracy. This model empowers Polri to identify officers with strong digital leadership potential, enabling targeted training programs and strategic personnel selection to drive digital transformation. We prioritize ethical deployment by excluding sensitive attributes, such as religion and gender, to mitigate bias and employ k-anonymity to safeguard data privacy. Fairness audits and interpretable outputs ensure equitable and transparent decision-making. Our approach aligns with global policing trends, offering a scalable solution to enhance leadership in tech-driven environments. By integrating robust technical performance with ethical safeguards, this study contributes to Polri’s strategic goals and sets a foundation for future research in diverse policing contexts. We advocate for continuous model monitoring to sustain fairness and effectiveness in real-world applications.",,,,,,,,,0,0,0,0,0,0,0,,,,2621-8410,,,,,,,,,,,,,,2025-08-29,RC:159180804_S24,,
J,"Akhundov, Ayaz",,,,,,,,,,,,,,,,,,Porta Universorum,,,,1,4,,,169,177,,,,10.69760/portuni.0104017,,,,,,,Article,May 24 2025,2025,"Ethical reflection has become central to the development of contemporary technologies, influencing design, implementation, and oversight. This article critically examines how ethical principles shape modern technology, focusing on areas such as artificial intelligence, biotechnology, surveillance, and digital platforms. The discussion begins by defining technology ethics and outlining its historical evolution, noting how public controversies often spur ethical discourse. Key normative theories (utilitarianism, deontology, virtue ethics) are reviewed and applied to technology contexts, drawing on Kant’s emphasis on human dignity and Rawls’s justice-as-fairness. We then survey current ethical challenges – including algorithmic bias, privacy infringement, autonomous systems dilemmas, and environmental impacts – with examples from both developed and developing regions. Several case studies illustrate these challenges in practice: the Cambridge Analytica data scandal (privacy and democracy), OpenAI’s ChatGPT (AI biases and misinformation), and facial-recognition misclassification (discrimination). We review global regulatory responses, from the EU’s GDPR and AI Act to UNESCO guidelines and OECD principles, highlighting how they enshrine values like transparency, accountability, and human rights. Finally, the paper advocates fostering an ethical culture through education, professional codes of conduct, and algorithmic audits, drawing on examples like ACMIEEE ethics codes and emerging audit frameworks. This comprehensive analysis emphasizes that only through international cooperation and multidisciplinary engagement can technology advance in ways that respect fundamental ethical values and social well-being.",,,,,,,,,0,0,0,0,0,0,0,,,,3030-2234,,,,,,,,,,,,,,2025-08-04,RC:157641984_S24,,
J,"Zhou, Junhao; Chen, Yufei; Shen, Chao; Zhang, Yang",,,,"CHEN, YUFEI/KRO-7149-2024; chao, shen/M-1619-2019",,,,,,Property Inference Attacks Against GANs,,,,,,,,Arxiv,,,,,,,,,,,1,,arXiv:2111.07608,,,,,,,preprint,Nov 15 2021,2021,"While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN's training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.",,,,,,,,,9,0,0,0,1,0,10,,,,,,,,,,,,,,"Xian Jiaotong Univ, Fac Elect & Informat Engn, Xian, Peoples R ChinaCISPA Helmholtz Ctr Informat Secur, Saarbrucken, Germany",CISPA Helmholtz Ctr Informat Secur,,,2022-11-15,PPRN:11953501,,
J,"Jongsoo, Yoon",,,,,,,,,,"Realization of the Right to Information Self-determination and Utilization of Data in the Interoperability System of IoT, Blockchain and AI",,,"사물인터넷, 블록체인, 인공지능의 상호운용에 있어서 개인정보자기결정권의 실현 및 데이터 이용 활성화",,,,,Journal of Korea Infomation Law,정보법학,,,24,3,,,107,148,,,,,,,,,,,research-article,2020,2020,"The features of IoT(Internet of Things), Blockchain and AI(Artificial Intelligence), such as continuous processing of vast amounts of data, flexibility and variability in the scope and purpose of use, distributed processing by multiple actors, automated processing without human intervention, and complexity of data analysis, -inevitably lead to conflict with personal information protection laws. In particular, it is not easy to obtain prior consent, as informed consent, of data subjects in such a technology environment. Some solutions for this problem have been proposed, but there is a further fundamental issue that the practice of personal information protection laws have heavily relied on the prior consent mechanism.It causes that other mechanism for personal information protection is treated lightly and all the responsibility for personal information protection is shifted to the data subject. The data subject who can’t understand or is unconcerned with the effect of consent is making reckless consent or reject categorically without any consideration of implication of sharing information. The processor of personal information who use a consent of the data subject as a means of exemption is trying to obtain the consent by the irregular and evasional way.On the other hand, the processor who really tries to obtain the informed consent of the data subject is confronting the unreasonable cost of regulation and increasing legal risks in the IoT and AI environment. In this situation, the new introduced pseudonymized information system replacing consent with de-identification processing is criticized by both the data subject and the processor due to legal uncertainty and insufficient protection of data subject.While the consent of the data subject is just a passive expression of permission and one of the various legal basis for lawful processing of personal information, the right to informational self-determination as a constitutional fundamental right and moral right is an active right which can control autonomously the flow of personal information in the modern informational society. Therefore, it is wrong understanding to recognize the right of the consent of the data subject as the right to information self-determination, or an essential content of the right to information self-determination. The right to information self-determination can be realized by active protection of data subject interest and expansion of the use of data, not by passive consent. The data subject and the processor should be cooperative through a fair agreement to obtain a mutual benefit when utilizing personal information. We should focus on guaranteeing the rights of data subject for active involvement in the entire process of personal information and the post-control through the enhancement of transparency and auditability. It is very important in the interoperability model of IoT, blockchain and AI which can create social and economic values by using data flow. Also, the data subject should not bear all the burden of the protection of the right to information self-determination, but the public and the private sectors as well as the data subject need to share the burden. Therefore, the key is how to build a governance system based on ethical legitimacy and mutual trust. We can realize the fair agreement model upgraded from the consent model by using data platforms often mentioned in the era of artificial intelligence and big data. The data subject can keep control of his/her personal information and make use of it effectively with a MyData model that has technical support such as data management and storage systems such as PIMS and PDS and legal support of right to data portability. But It is not sufficient for the fair agreement model. So, a data trust model in which a trustee with great expertise and resources negotiates with processor and participates in the entire process of personal information and the post-control instead of data subject need to be considered. And the interoperability model of IoT, blockchain and AI can support this data trust model effectively.",,,,"데이터 경제의 핵심기술인 사물인터넷, 블록체인, 인공지능의 방대한 데이터의 상시 처리, 활용 범위 및 목적의 유연성과 변동성, 분산처리 및 자동처리, 데이터 분석의복잡성은 필연적으로 개인정보 보호법제와 충돌한다. 특히 고지에 의한 동의(informed consent)를 충족해야 하는 정보주체의 사전동의를 받는 것이 쉽지 않다. 이문제를 해결하기 위한 다양한 방안이 시도되지만, 보다 근본적인 문제는 우리의 개인정보 보호법제와 그 운용이 정보주체의 동의에 지나치게 의존하고 있다는 점에 있다.동의 제도에 대한 지나친 의존은 나머지 정보보호 메커니즘을 소홀하게 만들고, 정보주체에게 모든 책임을 전가한다. 동의에 따른 결과를 정확하게 이해하지 못하고 그럴의사도 없는 정보주체는 형식적인 동의를 남발하거나 정보이용의 함의에 대한 이해없이 무조건 동의를 거절하고, 정보주체의 동의를 면책수단으로만 활용하려는 정보처리자는 변칙적이고 탈법적인 방법으로 동의를 확보한다. 한편, 제대로 정보주체로부터 동의를 받고자 하는 정보처리자에게 엄격한 사전동의 제도는 비합리적인 규제이며, 특히 사물인터넷, 블록체인, 인공지능 기술환경에서 법적 리스크를 증가시키는원인이 된다. 동의의 비중이 과도한 이런 상황에서 비식별처리로 동의를 대체하는 가명정보 제도는 그 적정성을 엄격하게 판단할 수 밖에 없어 여전히 법적 불확실성이해소되지 않는다는 비판과 함께, 동의를 대체하는 것에만 집중한 나머지 정보주체가정보처리에 관여할 수 있는 여지를 오히려 축소시켜 정보주체의 보호에도 미흡하다는 비판을 동시에 받고 있다. 정보주체의 동의는 침해의 위법성을 조각시키는 소극적, 수동적인 의사표시로서 개인정보처리의 여러 법적 근거 중 하나일 뿐이다. 반면 정보주체의 헌법적 기본권이자 인격권인 개인정보자기결정권은 고도로 정보화된 현대사회에서 자신에 대한 정보를 자율적으로 통제하는 적극적, 능동적 권리이다. 따라서 정보주체의 동의권을 개인정보자기결정권과 같은 권리, 또는 개인정보자기결정권의 본질적인 내용으로 받아들여 정보주체의 동의를 얻어야만 개인정보자기결정권이 실현되는 것으로 받아들이는 것은 잘못이다. 개인정보자기결정권의 실현은 동의에 의한소극적, 수동적 자율성보호에 의존할 게 아니라 적극적, 능동적인 이익 보호로서 정보주체의 이익을 보호하고 동시에 데이터 활용을 확대시키는 것이 되어야 한다. 즉 정보주체와 정보처리자가 함께 개인정보 할용으로 상호이익을 얻기 위해 협력하는 공정한 합의 모델이 되어야 한다. 이를 위해서는 개인정보 전 처리과정에서 정보주체의참여를 적극 보장하고, 투명성과 감사가능성 등 사후관여, 사후통제 실현에 초점을 맞추어야 한다. 이는 데이터의 흐름과 활용을 통해 사회, 경제적 가치를 창출하는 사물인터넷, 블록체인, 인공지능 기술의 상호운용에서 매우 중요한 의미를 갖는다. 나아가고도 정보사회에서 개인의 결정의 자유와 자유민주체제의 근간을 보존하기 위한 필요 최소한의 헌법적 보장장치로 개인정보자기결정권을 이해한다면, 개인정보자기결정권의 보호는 어는 한 주체에 모든 부담이 주어져서는 안되고, 정보주체와 공공, 민간이 함께 그 부담을 나누어야 한다. 따라서 개인정보의 적극적, 능동적 이익 보호 모델은 거버넌스 구조를 띄게 된다. 개인정보자기결정권의 실현을 위한 적극적, 능동적모델은 윤리적 정당성과 상호신뢰에 기반을 둔 거버넌스 시스템을 어떻게 구축할 것인지가 핵심이다. 동의 모델에서 진화한 공정한 합의 모델은 합의 내용과 과정에서의투명성과 공정성을 보장하면서 이해당사자들의 참여에 의한 의사결정을 주도적으로실현할 수 있는 장치가 필요한 바, 최근 등장하는 데이터 플랫폼들을 활용할 수 있다.PIMS, PDS와 같은 데이터 관리, 저장시스템 등의 기술적 지원과 데이터 이동권에 의해 뒷받침 되는 마이데이터 모델은 정보주체가 통제권을 잃지 않으면서 효율적으로자신의 정보를 활용할 수 있다. 다만 그것만으로는 동의 제도의 한계를 넘는 공정한합의 모델이 되기에는 부족하다. 전문성과 역량을 가진 수탁자가 정보주체의 정보부족과 미흡한 협상력을 보완하여 위임의 범위 내에서 정보주체 대신 구체적인 합의를도출하고 사후 모니터링과 정기적인 리포트를 통해 정보처리에 지속적으로 관여하는데이터 신탁 플랫폼이 중요한 의미를 갖는다. 사물인터넷, 블록체인, 인공지능의 상호운용은 이를 효과적으로 뒷받침 할 수 있을 것이다.",,,,,0,0,0,0,0,0,0,,,1598-5911,,,,,,,,,,,,,,,2021-06-22,KJD:ART002671185,,
J,"Alshareet, Osama; Awasthi, Anjali",,,,"Alshareet, Osama/KAL-9892-2024; Awasthi, Anjali/MVV-7555-2025",,,,,,A Novel Framework for Integrating Blockchain-Driven Federated Learning with Neural Networks in E-Commerce,,,,,,,,JOURNAL OF NETWORK AND SYSTEMS MANAGEMENT,,,,33,3,,,,,56,,,10.1007/s10922-025-09928-x,,,,,,,Article,JUL 2025,2025,"Federated learning (FL) enables collaborative model training without raw data sharing; however, its decentralized architecture remains vulnerable to inference attacks, malicious updates, and opaque governance. To address these challenges, we introduce an end-to-end framework integrating FL with permissioned blockchain technology, systematically guided by TRIZ innovation principles, ensuring verifiable, privacy-preserving, and ethically accountable machine learning collaborations. Our framework integrates multi-layered security measures, including encrypted local model updates, blockchain-based smart-contract consensus mechanisms for secure global aggregation, and an immutable complaint-redress system that transparently records grievances, initiates forensic audits, and documents remedial actions. Employing iterative ARIZ cycles, we effectively resolve the contradiction between strict data locality and collective model intelligence. The proposed pipeline comprises eight structured phases: cryptographic initialization, heterogeneous data preparation, dual-model instantiation, client-side optimization, weighted model aggregation, ledger anchoring, proactive dispute resolution, and comprehensive performance evaluation. Experimental evaluations on textual datasets with deliberately designed non-IID distributions demonstrate stable convergence, reduced communication overhead, and robust predictive accuracy across diverse client scenarios. Validation using recurrent neural networks and linear models for e-commerce sentiment analysis, clinical note triage, and vehicular telemetry illustrates the framework's domain-agnostic versatility. Privacy analyses using gradient-inversion attacks and efficiency benchmarks under varied bandwidth and participation levels confirm robustness. Comparative analysis reveals our approach offers enhanced adaptability, richer analytical capabilities, and explicit ethical integration compared to existing blockchain-enhanced FL solutions. Ultimately, this research proposes a pathway towards transparent, human-centered artificial intelligence systems, harmonizing regulatory compliance, organizational objectives, and societal trust without compromising technical performance. Future studies will explore alternative blockchain consensus mechanisms, formal fairness assessments, and pilot deployments in healthcare and financial sectors.",,,,,,,,,0,0,0,0,0,0,0,,,1064-7570,1573-7705,,,,,,,,,,"Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada",,,,2025-06-08,WOS:001501159300001,,
B,"Rastegarpanah, Bashir",,,,,,,,,,Tools for Responsible Decision-Making in Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2021,2021,,,,,,,,,,0,0,0,0,0,0,0,,,,,979-8-209-87677-9,,,,,,,,,"Boston University, Computer Science GRS, Massachusetts, United States",Boston University,,,,PQDT:51243091,,
J,"Huynh, Trung Dong; Tsakalakis, Niko; Helal, Ayah; Stalla-Bourdillon, Sophie; Moreau, Luc",,,,,,,,,,Addressing Regulatory Requirements on Explanations for Automated Decisions with Provenance—A Case Study,,,,,,,,Digital Government Research and Practice,,,,2,2,,,1,14,,,,10.1145/3436897,,,,,Jan 2021,,Article,Apr 30 2021,2021,"AI-based automated decisions are increasingly used as part of new services being deployed to the general public. This approach to building services presents significant potential benefits, such as the reduced speed of execution, increased accuracy, lower cost, and ability to adapt to a wide variety of situations. However, equally significant concerns have been raised and are now well documented such as concerns about privacy, fairness, bias, and ethics. On the consumer side, more often than not, the users of those services are provided with no or inadequate explanations for decisions that may impact their lives. In this article, we report the experience of developing a socio-technical approach to constructing explanations for such decisions from their audit trails, or provenance, in an automated manner. The work has been carried out in collaboration with the UK Information Commissioner’s Office. In particular, we have implemented an automated Loan Decision scenario, instrumented its decision pipeline to record provenance, categorized relevant explanations according to their audience and their regulatory purposes, built an explanation-generation prototype, and deployed the whole system in an online demonstrator.",,,,,,,,,6,0,0,0,0,0,10,,,,2639-0175,,,,,,,,,,"Kings Coll, London, UKUniversity of Southampton, Southampton, United Kingdom",Univ Southampton,,,2025-07-11,RC:155403220_S24,,
J,"Zeng, Honghong; Lou, Jiong; Li, Kailai; Wu, Chentao; Xue, Guangtao; Luo, Yuan; Cheng, Fan; Zhao, Wei; Li, Jie",,,,"; Li, Jie/NTR-4993-2025","Li, Jie/0000-0002-4974-6116;",,,,,ESFL: Accelerating Poisonous Model Detection in Privacy-Preserving Federated Learning,,,,,,,,IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING,,,,22,4,,,3780,3794,,,,10.1109/TDSC.2025.3541633,,,,,,,Article,JUL-AUG 2025,2025,"Privacy-preserving federated learning (PPFL) is a promising secure distributed learning paradigm, which enables collaborative training of a global machine learning model through sharing encrypted local models instead of sensitive raw data. PPFL, however, is vulnerable to model poisoning attacks. Most existing Byzantine-robust PPFL solutions typically employ two non-colluding servers to achieve secure model detection and aggregation by executing interactive security protocols, which incur considerable computation and communication overheads. To tackle this issue, we propose an efficient and secure federated learning (ESFL) technique to accelerate the detection of poisonous models in PPFL. First, to improve computational efficiency, we construct a lightweight non-interactive efficient decryption functional encryption (NED-FE) scheme to protect the data privacy of local models. Then, to ensure high communication performance, we elaborately design a non-interactive privacy-preserving robust aggregation strategy, which efficiently detects the blind poisonous models and aggregates benign models. Finally, we implement ESFL and conduct extensive theoretical analysis and experiments. The numerical results demonstrate that ESFL not only achieves the confidentiality and robustness design goals but also maintains high efficiency. Compared with the baseline, ESFL effectively reduces the aggregation latency by up to 88%.",,,,,,,,,0,0,0,0,0,0,0,,,1545-5971,1941-0018,,,,,,,,,,"Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200000, Peoples R ChinaYancheng Blockchain Res Inst, Hengyang 421000, Hunan, Peoples R ChinaShanghai Key Lab Trusted Data Circulat & Governanc, Shanghai 200000, Peoples R ChinaWeb3, Shanghai 200000, Peoples R ChinaChinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518000, Peoples R China",Yancheng Blockchain Res InstShanghai Key Lab Trusted Data Circulat & GovernancWeb3,,,2025-07-24,WOS:001531376500009,,
J,"Mokale, Mahesh",,,,,,,,,,,,,,,,,,International Journal For Multidisciplinary Research,,,,3,4,,,,,,,,10.36948/ijfmr.2021.v03i04.39051,,,,,Jul 2021,,Article,Jul 07 2021,2021,"As global data privacy regulations continue to evolve, telecom operators face significant challenges in ensuring compliance while maintaining operational efficiency. With regulations such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), Brazil’s General Data Protection Law (LGPD), China’s Personal Information Protection Law (PIPL), and various other regional data protection laws, telecom operators must navigate complex legal landscapes that differ across jurisdictions. The penalties for non-compliance can be severe, including hefty fines, reputational damage, loss of business, and potential operational shutdowns due to non-adherence to regulatory mandates. Telecom operators collect and process vast amounts of personally identifiable information (PII) and sensitive data, such as call records, location data, and billing details. The need to comply with various regulations requires a robust data governance framework that ensures lawful data collection, secure storage, minimal processing, and adherence to data retention policies. Additionally, telecom operators must implement mechanisms that facilitate user rights, such as data access, correction, deletion, and portability, while ensuring compliance with consent management and lawful processing obligations. Beyond regulatory requirements, consumer expectations for privacy and transparency are growing. The increasing number of cyber threats and data breaches has led to heightened scrutiny, demanding that telecom companies adopt state-of-the-art security measures, including end-to-end encryption, zero-trust security models, and continuous monitoring of data flows. Compliance with regulations also necessitates significant investments in privacy-enhancing technologies, automation tools, and dedicated compliance teams to handle regulatory audits and data protection impact assessments (DPIAs). The complexity of cross-border data transfers further amplifies the challenges for telecom operators. Regulations such as GDPR impose restrictions on data transfers outside the European Economic Area (EEA) unless appropriate safeguards, such as Standard Contractual Clauses (SCCs) or Binding Corporate Rules (BCRs), are in place. Similarly, China’s PIPL requires explicit regulatory approval for international data transfers, and India’s DPDPA mandates data localization for certain types of sensitive data. Telecom companies must implement region-specific data transfer mechanisms while ensuring business continuity and operational efficiency across global markets. Furthermore, emerging technologies such as artificial intelligence (AI), big data analytics, and 5G networks introduce additional regulatory challenges. AI-driven data processing must comply with fairness, transparency, and accountability principles, while big data applications must ensure anonymization and pseudonymization techniques to mitigate risks. The deployment of 5G networks, which enables massive data transmission and real-time processing, requires enhanced security protocols to prevent unauthorized data access and cyber threats. This white paper provides an in-depth guide on global data privacy regulations, their implications for telecom operators, and best practices for compliance. It explores how telecom companies can align their data handling processes with regulatory mandates, integrate privacy-first approaches, and leverage advanced technology for efficient compliance management. By understanding regulatory requirements and implementing robust data protection strategies, telecom companies can mitigate risks, build customer trust, and ensure seamless service delivery while maintaining a competitive advantage in an increasingly data-driven and regulated world.",,,,,,,,,0,0,0,0,0,0,0,,,,2582-2160,,,,,,,,,,,,,,2025-07-08,RC:124483437_S24,,
C,"Young, Meg; Katell, Michael A.; Krafft, P. M.",,,ACM,,,,,,,Confronting Power and Corporate Capture at the FAccT Conference,,,,,,,,"PROCEEDINGS OF 2022 5TH ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2022",,,,,,,,1375,1386,,,,10.1145/3531146.3533194,,,,,,,Proceedings Paper,2022,2022,"Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference's neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., Building and auditing fair algorithms: A case study in candidate screening as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus-un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.",,,,,2022 Conference on Fairness Accountability and Transparency-FACCT2022 Conference on Fairness Accountability and Transparency-FACCT,"JUN 21-24, 2022JUN 21-24, 2022",,"Seoul, SOUTH KOREASeoul, SOUTH KOREA",43,0,0,0,0,0,45,,,,,978-1-4503-9352-2,,,,,,,,,"Cornell Tech, New York, NY 10044 USAAlan Turing Inst, London, EnglandUniv Arts London, London, England",Cornell Tech,,,2022-01-01,WOS:001511029000108,,
J,"Kroll, Joshua A.; Huey, Joanna; Barocas, Solon; Felten, Edward W.; Reidenberg, Joel R.; Robinson, David G.; Yu, Harlan",,,,,"Barocas, Solon/0000-0003-4577-466X; Huey, Joanna Nanami/0009-0000-5934-9184",,,,,ACCOUNTABLE ALGORITHMS,,,,,,,,UNIVERSITY OF PENNSYLVANIA LAW REVIEW,,,,165,3,,,633,705,,,,,,,,,,,Article,FEB 2017,2017,"Many important decisions historically made by people are now made by computers. Algorithms count votes, approve loan and credit card applications, target citizens or neighborhoods for police scrutiny, select taxpayers for IRS audit, grant or deny immigration visas, and more.The accountability mechanisms and legal standards that govern such decision processes have not kept pace with technology. The tools currently available to policymakers, legislators, and courts were developed to oversee human decisionmakers and often fail when applied to computers instead. For example, how do you judge the intent of a piece of software? Because automated decision systems can return potentially incorrect, unjustified, or unfair results, additional approaches are needed to make such systems accountable and governable. This Article reveals a new technological toolkit to verify that automated decisions comply with key standards of legal fairness.We challenge the dominant position in the legal literature that transparency will solve these problems. Disclosure of source code is often neither necessary (because of alternative techniques from computer science) nor sufficient (because of the issues analyzing code) to demonstrate the fairness of a process. Furthermore, transparency may be undesirable, such as when it discloses private information or permits tax cheats or terrorists to game the systems determining audits or security screening.The central issue is how to assure the interests of citizens, and society as a whole, in making these processes more accountable. This Article argues that technology is creating new opportunities-subtler and more flexible than total transparency-to design decisionmaking algorithms so that they better align with legal and policy objectives. Doing so will improve not only the current governance of automated decisions, but also-in certain cases-the governance of decisionmaking in general. The implicit (or explicit) biases of human decisionmakers can be difficult to find and root out, but we can peer into the brain of an algorithm: computational processes and purpose specifications can be declared prior to use and verified afterward.The technological tools introduced in this Article apply widely. They can be used in designing decisionmaking processes from both the private and public sectors, and they can be tailored to verify different characteristics as desired by decisionmakers, regulators, or the public. By forcing a more careful consideration of the effects of decision rules, they also engender policy discussions and closer looks at legal standards. As such, these tools have far-reaching implications throughout law and society.Part I of this Article provides an accessible and concise introduction to foundational computer science techniques that can be used to verify and demonstrate compliance with key standards of legal fairness for automated decisions without revealing key attributes of the decisions or the processes by which the decisions were reached. Part II then describes how these techniques can assure that decisions are made with the key governance attribute of procedural regularity, meaning that decisions are made under an announced set of rules consistently applied in each case. We demonstrate how this approach could be used to redesign and resolve issues with the State Department's diversity visa lottery. In Part III, we go further and explore how other computational techniques can assure that automated decisions preserve fidelity to substantive legal and policy choices. We show how these tools may be used to assure that certain kinds of unjust discrimination are avoided and that automated decision processes behave in ways that comport with the social or legal standards that govern the decision. We also show how automated decisionmaking may even complicate existing doctrines of disparate treatment and disparate impact, and we discuss some recent computer science work on detecting and removing discrimination in algorithms, especially in the context of big data and machine learning. And lastly, in Part IV, we propose an agenda to further synergistic collaboration between computer science, law, and policy to advance the design of automated decision processes for accountability.",,,,,,,,,454,4,1,0,1,4,549,,,0041-9907,,,,,,,,,,,"Ctr Informat Technol Policy, Princeton, NJ 08544 USAFordham Law Sch, Law, New York, NY USAYale Law Sch, Informat Soc Project, New Haven, CT USAStanford Ctr Internet & Soc, Stanford, CA USA",Ctr Informat Technol PolicyStanford Ctr Internet & Soc,,,2017-04-04,WOS:000397048900003,,
B,"Asvadishirehjini, Aref",,,,,,,,,,"Ensuring Integrity, Privacy, and Fairness for Machine Learning Using Trusted Execution Environments",,,,,,,,,,,,,,,,,,,,,,,,,,,,Dissertation/Thesis,Jan 01 2021,2021,,,,,,,,,,0,0,0,0,0,0,0,,,,,9.79836E+12,,,,,,,,,"The University of Texas at Dallas, Computer Science, Texas, United States",The University of Texas at Dallas,,,,PQDT:68549721,,
