
@misc{schweighofer_safe_2025,
	title = {Safe and {Certifiable} {AI} {Systems}: {Concepts}, {Challenges}, and {Lessons} {Learned}},
	shorttitle = {Safe and {Certifiable} {AI} {Systems}},
	url = {http://arxiv.org/abs/2509.08852},
	doi = {10.48550/arXiv.2509.08852},
	abstract = {There is an increasing adoption of artificial intelligence in safety-critical applications, yet practical schemes for certifying that AI systems are safe, lawful and socially acceptable remain scarce. This white paper presents the T{\textbackslash}"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology for assessing and certifying machine learning systems. The audit catalog has been in continuous development since 2019 in an ongoing collaboration with scientific partners. Building on three pillars - Secure Software Development, Functional Requirements, and Ethics \& Data Privacy - the catalog translates the high-level obligations of the EU AI Act into specific, testable criteria. Its core concept of functional trustworthiness couples a statistically defined application domain with risk-based minimum performance requirements and statistical testing on independently sampled data, providing transparent and reproducible evidence of model quality in real-world settings. We provide an overview of the functional requirements that we assess, which are oriented on the lifecycle of an AI system. In addition, we share some lessons learned from the practical application of the audit catalog, highlighting common pitfalls we encountered, such as data leakage scenarios, inadequate domain definitions, neglect of biases, or a lack of distribution drift controls. We further discuss key aspects of certifying AI systems, such as robustness, algorithmic fairness, or post-certification requirements, outlining both our current conclusions and a roadmap for future research. In general, by aligning technical best practices with emerging European standards, the approach offers regulators, providers, and users a practical roadmap for legally compliant, functionally trustworthy, and certifiable AI systems.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Schweighofer, Kajetan and Brune, Barbara and Gruber, Lukas and Schmid, Simon and Aufreiter, Alexander and Gruber, Andreas and Doms, Thomas and Eder, Sebastian and Mayer, Florian and Stadlbauer, Xaver-Paul and Schwald, Christoph and Zellinger, Werner and Nessler, Bernhard and Hochreiter, Sepp},
	month = sep,
	year = {2025},
	note = {arXiv:2509.08852 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	annote = {Comment: 63 pages, 27 figures},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\JUW8L2Z8\\Schweighofer et al. - 2025 - Safe and Certifiable AI Systems Concepts, Challenges, and Lessons Learned.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\XUTRWNJT\\2509.html:text/html},
}

@misc{wang_towards_2025,
	title = {Towards {Enhancing} {Data} {Equity} in {Public} {Health} {Data} {Science}},
	url = {http://arxiv.org/abs/2508.20301},
	doi = {10.48550/arXiv.2508.20301},
	abstract = {Data-driven decisions shape public health policies and practice, yet persistent disparities in data representation skew insights and undermine interventions. To address this, we advance a structured roadmap that integrates public health data science with computer science and is grounded in reflexivity. We adopt data equity as a guiding concept: ensuring the fair and inclusive representation, collection, and use of data to prevent the introduction or exacerbation of systemic biases that could lead to invalid downstream inference and decisions. To underscore urgency, we present three public health cases where non-representative datasets and skewed knowledge impede decisions across diverse subgroups. These challenges echo themes in two literatures: public health highlights gaps in high-quality data for specific populations, while computer science and statistics contribute criteria and metrics for diagnosing bias in data and models. Building on these foundations, we propose a working definition of public health data equity and a structured self-audit framework. Our framework integrates core computational principles (fairness, accountability, transparency, ethics, privacy, confidentiality) with key public health considerations (selection bias, representativeness, generalizability, causality, information bias) to guide equitable practice across the data life cycle, from study design and data collection to measurement, analysis, interpretation, and translation. Embedding data equity in routine practice offers a practical path for ensuring that data-driven policies, artificial intelligence, and emerging technologies improve health outcomes for all. Finally, we emphasize the critical understanding that, although data equity is an essential first step, it does not inherently guarantee information, learning, or decision equity.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Wang, Yiran and Boyd, Alicia E. and Rountree, Lillian and Ren, Yi and Nyhan, Kate and Nagar, Ruchit and Higginbottom, Jackson and Ranney, Megan L. and Parikh, Harsh and Mukherjee, Bhramar},
	month = aug,
	year = {2025},
	note = {arXiv:2508.20301 [stat]},
	keywords = {Statistics - Applications},
	annote = {Comment: 111 pages, 4 figures, 1 table},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\3623CHK2\\Wang et al. - 2025 - Towards Enhancing Data Equity in Public Health Data Science.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\A8QQ2SCN\\2508.html:text/html},
}

@misc{noauthor_250815830_nodate,
	title = {[2508.15830] {DAIQ}: {Auditing} {Demographic} {Attribute} {Inference} from {Question} in {LLMs}},
	url = {https://arxiv.org/abs/2508.15830},
	urldate = {2025-10-31},
	file = {[2508.15830] DAIQ\: Auditing Demographic Attribute Inference from Question in LLMs:C\:\\Users\\mattia\\Zotero\\storage\\DQZ9R92H\\2508.html:text/html},
}

@misc{ramineni_beyond_2025,
	title = {Beyond {Internal} {Data}: {Bounding} and {Estimating} {Fairness} from {Incomplete} {Data}},
	shorttitle = {Beyond {Internal} {Data}},
	url = {http://arxiv.org/abs/2508.13040},
	doi = {10.48550/arXiv.2508.13040},
	abstract = {Ensuring fairness in AI systems is critical, especially in high-stakes domains such as lending, hiring, and healthcare. This urgency is reflected in emerging global regulations that mandate fairness assessments and independent bias audits. However, procuring the necessary complete data for fairness testing remains a significant challenge. In industry settings, legal and privacy concerns restrict the collection of demographic data required to assess group disparities, and auditors face practical and cultural challenges in gaining access to data. In practice, data relevant for fairness testing is often split across separate sources: internal datasets held by institutions with predictive attributes, and external public datasets such as census data containing protected attributes, each providing only partial, marginal information. Our work seeks to leverage such available separate data to estimate model fairness when complete data is inaccessible. We propose utilising the available separate data to estimate a set of feasible joint distributions and then compute the set plausible fairness metrics. Through simulation and real experiments, we demonstrate that we can derive meaningful bounds on fairness metrics and obtain reliable estimates of the true metric. Our results demonstrate that this approach can serve as a practical and effective solution for fairness testing in real-world settings where access to complete data is restricted.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Ramineni, Varsha and Rahmani, Hossein A. and Yilmaz, Emine and Barber, David},
	month = aug,
	year = {2025},
	note = {arXiv:2508.13040 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 3 figures},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\GJ6ZFBPJ\\Ramineni et al. - 2025 - Beyond Internal Data Bounding and Estimating Fairness from Incomplete Data.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\VK3DGS57\\2508.html:text/html},
}

@misc{noauthor_save_nodate,
	title = {save to zotero connector shortcut - {Google} {Search}},
	url = {https://www.google.com/search?q=save+to+zotero+connector+shortcut&sca_esv=fbce5c1644a5d5c8&sxsrf=AE3TifOesxMPKScdJdc15lMCluYvWiKfRQ%3A1761920757712&ei=9cYEaYacK4-ji-gP59jBkAg&ved=0ahUKEwjG6oqn0s6QAxWP0QIHHWdsEIIQ4dUDCBM&uact=5&oq=save+to+zotero+connector+shortcut&gs_lp=Egxnd3Mtd2l6LXNlcnAiIXNhdmUgdG8gem90ZXJvIGNvbm5lY3RvciBzaG9ydGN1dDIKECEYoAEYwwQYCjIKECEYoAEYwwQYCjIKECEYoAEYwwQYCkifFFCkAlj5E3ACeACQAQCYAWigAZIGqgEEMTAuMbgBA8gBAPgBAZgCCqAChgXCAgoQABiwAxjWBBhHwgIGEAAYFhgewgILEAAYgAQYhgMYigXCAgUQABjvBcICCBAAGIAEGKIEwgIHECMYsAIYJ8ICCBAhGKABGMMEmAMAiAYBkAYIkgcDOC4yoAfaQrIHAzYuMrgH_QTCBwUwLjYuNMgHHQ&sclient=gws-wiz-serp},
	urldate = {2025-10-31},
	file = {save to zotero connector shortcut - Google Search:C\:\\Users\\mattia\\Zotero\\storage\\2989MFWF\\search.html:text/html},
}

@misc{pan_beyond_2025,
	title = {Beyond {Benchmarks}: {Dynamic}, {Automatic} {And} {Systematic} {Red}-{Teaming} {Agents} {For} {Trustworthy} {Medical} {Language} {Models}},
	shorttitle = {Beyond {Benchmarks}},
	url = {http://arxiv.org/abs/2508.00923},
	doi = {10.48550/arXiv.2508.00923},
	abstract = {Ensuring the safety and reliability of large language models (LLMs) in clinical practice is critical to prevent patient harm and promote trustworthy healthcare applications of AI. However, LLMs are advancing so rapidly that static safety benchmarks often become obsolete upon publication, yielding only an incomplete and sometimes misleading picture of model trustworthiness. We demonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming framework that continuously stress-tests LLMs can reveal significant weaknesses of current LLMs across four safety-critical domains: robustness, privacy, bias/fairness, and hallucination. A suite of adversarial agents is applied to autonomously mutate test cases, identify/evolve unsafe-triggering strategies, and evaluate responses, uncovering vulnerabilities in real time without human intervention. Applying DAS to 15 proprietary and open-source LLMs revealed a stark contrast between static benchmark performance and vulnerability under adversarial pressure. Despite a median MedQA accuracy exceeding 80{\textbackslash}\%, 94{\textbackslash}\% of previously correct answers failed our dynamic robustness tests. We observed similarly high failure rates across other domains: privacy leaks were elicited in 86{\textbackslash}\% of scenarios, cognitive-bias priming altered clinical recommendations in 81{\textbackslash}\% of fairness tests, and we identified hallucination rates exceeding 66{\textbackslash}\% in widely used models. Such profound residual risks are incompatible with routine clinical practice. By converting red-teaming from a static checklist into a dynamic stress-test audit, DAS red-teaming offers the surveillance that hospitals/regulators/technology vendors require as LLMs become embedded in patient chatbots, decision-support dashboards, and broader healthcare workflows. Our framework delivers an evolvable, scalable, and reliable safeguard for the next generation of medical AI.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Pan, Jiazhen and Jian, Bailiang and Hager, Paul and Zhang, Yundi and Liu, Che and Jungmann, Friedrike and Li, Hongwei Bran and You, Chenyu and Wu, Junde and Zhu, Jiayuan and Liu, Fenglin and Liu, Yuyuan and Bubeck, Niklas and Wachinger, Christian and Chen and Chen and Gong, Zhenyu and Ouyang, Cheng and Kaissis, Georgios and Wiestler, Benedikt and Rueckert, Daniel},
	month = jul,
	year = {2025},
	note = {arXiv:2508.00923 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\LQSS7UAS\\Pan et al. - 2025 - Beyond Benchmarks Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Lang.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\AUNZJAEW\\2508.html:text/html},
}

@misc{sharma_ethical_2025,
	title = {Ethical {AI}: {Towards} {Defining} a {Collective} {Evaluation} {Framework}},
	shorttitle = {Ethical {AI}},
	url = {http://arxiv.org/abs/2506.00233},
	doi = {10.48550/arXiv.2506.00233},
	abstract = {Artificial Intelligence (AI) is transforming sectors such as healthcare, finance, and autonomous systems, offering powerful tools for innovation. Yet its rapid integration raises urgent ethical concerns related to data ownership, privacy, and systemic bias. Issues like opaque decision-making, misleading outputs, and unfair treatment in high-stakes domains underscore the need for transparent and accountable AI systems. This article addresses these challenges by proposing a modular ethical assessment framework built on ontological blocks of meaning-discrete, interpretable units that encode ethical principles such as fairness, accountability, and ownership. By integrating these blocks with FAIR (Findable, Accessible, Interoperable, Reusable) principles, the framework supports scalable, transparent, and legally aligned ethical evaluations, including compliance with the EU AI Act. Using a real-world use case in AI-powered investor profiling, the paper demonstrates how the framework enables dynamic, behavior-informed risk classification. The findings suggest that ontological blocks offer a promising path toward explainable and auditable AI ethics, though challenges remain in automation and probabilistic reasoning.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Sharma, Aasish Kumar and Kyosev, Dimitar and Kunkel, Julian},
	month = may,
	year = {2025},
	note = {arXiv:2506.00233 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 6 pages, 3 figures, accepted at 8th IEEE International Workshop on Advances in Artificial Intelligence and Machine Learning (AIML 2025): Futuristic AI and ML models \& Intelligent Systems},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\D77SKUPP\\Sharma et al. - 2025 - Ethical AI Towards Defining a Collective Evaluation Framework.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\UFUIG9KY\\2506.html:text/html},
}

@misc{yuan_quantitative_2025,
	title = {Quantitative {Auditing} of {AI} {Fairness} with {Differentially} {Private} {Synthetic} {Data}},
	url = {http://arxiv.org/abs/2504.21634},
	doi = {10.48550/arXiv.2504.21634},
	abstract = {Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Yuan, Chih-Cheng Rex and Wang, Bow-Yaw},
	month = apr,
	year = {2025},
	note = {arXiv:2504.21634 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\MJ62B2T3\\Yuan and Wang - 2025 - Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\9778T9AS\\2504.html:text/html},
}

@misc{bourree_p2nia_2025,
	title = {{P2NIA}: {Privacy}-{Preserving} {Non}-{Iterative} {Auditing}},
	shorttitle = {{P2NIA}},
	url = {http://arxiv.org/abs/2504.00874},
	doi = {10.48550/arXiv.2504.00874},
	abstract = {The emergence of AI legislation has increased the need to assess the ethical compliance of high-risk AI systems. Traditional auditing methods rely on platforms' application programming interfaces (APIs), where responses to queries are examined through the lens of fairness requirements. However, such approaches put a significant burden on platforms, as they are forced to maintain APIs while ensuring privacy, facing the possibility of data leaks. This lack of proper collaboration between the two parties, in turn, causes a significant challenge to the auditor, who is subject to estimation bias as they are unaware of the data distribution of the platform. To address these two issues, we present P2NIA, a novel auditing scheme that proposes a mutually beneficial collaboration for both the auditor and the platform. Extensive experiments demonstrate P2NIA's effectiveness in addressing both issues. In summary, our work introduces a privacy-preserving and non-iterative audit scheme that enhances fairness assessments using synthetic or local data, avoiding the challenges associated with traditional API-based audits.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Bourrée, Jade Garcia and Lautraite, Hadrien and Gambs, Sébastien and Tredan, Gilles and Merrer, Erwan Le and Rottembourg, Benoît},
	month = apr,
	year = {2025},
	note = {arXiv:2504.00874 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 8 figures},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\BI5DJVD8\\Bourrée et al. - 2025 - P2NIA Privacy-Preserving Non-Iterative Auditing.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\BJ8FTTPD\\2504.html:text/html},
}

@misc{basu_sublinear_2025,
	title = {Sublinear {Algorithms} for {Wasserstein} and {Total} {Variation} {Distances}: {Applications} to {Fairness} and {Privacy} {Auditing}},
	shorttitle = {Sublinear {Algorithms} for {Wasserstein} and {Total} {Variation} {Distances}},
	url = {http://arxiv.org/abs/2503.07775},
	doi = {10.48550/arXiv.2503.07775},
	abstract = {Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic framework to learn the probability and cumulative distribution functions (PDFs and CDFs) of a sub-Weibull, i.e. almost any light- or heavy-tailed, distribution while the samples from it arrive in a stream. The idea is to reduce these problems into estimating the frequency of an {\textbackslash}textit\{appropriately chosen subset\} of the support of a {\textbackslash}textit\{properly discretised distribution\}. We leverage this reduction to compute mergeable summaries of distributions from the stream of samples while requiring only sublinear space relative to the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two distributions while samples arrive in streams and from multiple sources. Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities, and further extend the mergeable summaries framework to continuous distributions with possibly infinite support. Our results are tight with respect to the existing lower bounds for bounded discrete distributions. In addition, we leverage our proposed estimators of Wasserstein and TV distances to tightly audit the fairness and privacy of algorithms. We empirically demonstrate the efficiency of proposed algorithms across synthetic and real-world datasets.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Basu, Debabrota and Chanda, Debarshi},
	month = jun,
	year = {2025},
	note = {arXiv:2503.07775 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computers and Society, Statistics - Computation},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\SALZYBCR\\Basu and Chanda - 2025 - Sublinear Algorithms for Wasserstein and Total Variation Distances Applications to Fairness and Pri.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\2EQAVLYU\\2503.html:text/html},
}

@misc{lin_beyond_2025,
	title = {Beyond {Diagnostic} {Performance}: {Revealing} and {Quantifying} {Ethical} {Risks} in {Pathology} {Foundation} {Models}},
	shorttitle = {Beyond {Diagnostic} {Performance}},
	url = {http://arxiv.org/abs/2502.16889},
	doi = {10.48550/arXiv.2502.16889},
	abstract = {Pathology foundation models (PFMs), as large-scale pre-trained models tailored for computational pathology, have significantly advanced a wide range of applications. Their ability to leverage prior knowledge from massive datasets has streamlined the development of intelligent pathology models. However, we identify several critical and interrelated ethical risks that remain underexplored, yet must be addressed to enable the safe translation of PFMs from lab to clinic. These include the potential leakage of patient-sensitive attributes, disparities in model performance across demographic and institutional subgroups, and the reliance on diagnosis-irrelevant features that undermine clinical reliability. In this study, we pioneer the quantitative analysis for ethical risks in PFMs, including privacy leakage, clinical reliability, and group fairness. Specifically, we propose an evaluation framework that systematically measures key dimensions of ethical concern: the degree to which patient-sensitive attributes can be inferred from model representations, the extent of performance disparities across demographic and institutional subgroups, and the influence of diagnostically irrelevant features on model decisions. We further investigate the underlying causes of these ethical risks in PFMs and empirically validate our findings. Then we offer insights into potential directions for mitigating such risks, aiming to inform the development of more ethically robust PFMs. This work provides the first quantitative and systematic evaluation of ethical risks in PFMs. Our findings highlight the urgent need for ethical safeguards in PFMs and offer actionable insights for building more trustworthy and clinically robust PFMs. To facilitate future research and deployment, we will release the assessment framework as an online toolkit to support the development, auditing, and deployment of ethically robust PFMs.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Lin, Weiping and Liu, Shen and Zhu, Runchen and Lin, Yixuan and Wang, Baoshun and Wang, Liansheng},
	month = jul,
	year = {2025},
	note = {arXiv:2502.16889 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 33 pages,5 figure,23 tables},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\7KQESCYI\\Lin et al. - 2025 - Beyond Diagnostic Performance Revealing and Quantifying Ethical Risks in Pathology Foundation Model.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\LI7GPR29\\2502.html:text/html},
}

@misc{noauthor_250115985_nodate,
	title = {[2501.15985] {Demographic} {Benchmarking}: {Bridging} {Socio}-{Technical} {Gaps} in {Bias} {Detection}},
	url = {https://arxiv.org/abs/2501.15985},
	urldate = {2025-10-31},
	file = {[2501.15985] Demographic Benchmarking\: Bridging Socio-Technical Gaps in Bias Detection:C\:\\Users\\mattia\\Zotero\\storage\\NV9AZI2G\\2501.html:text/html},
}

@misc{ganescu_trust_2024,
	title = {Trust the {Process}: {Zero}-{Knowledge} {Machine} {Learning} to {Enhance} {Trust} in {Generative} {AI} {Interactions}},
	shorttitle = {Trust the {Process}},
	url = {http://arxiv.org/abs/2402.06414},
	doi = {10.48550/arXiv.2402.06414},
	abstract = {Generative AI, exemplified by models like transformers, has opened up new possibilities in various domains but also raised concerns about fairness, transparency and reliability, especially in fields like medicine and law. This paper emphasizes the urgency of ensuring fairness and quality in these domains through generative AI. It explores using cryptographic techniques, particularly Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance fairness and accuracy while protecting model privacy. Applying ZKPs to Machine Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables independent validation of AI-generated content without revealing sensitive model information, promoting transparency and trust. ZKML enhances AI fairness by providing cryptographic audit trails for model predictions and ensuring uniform performance across users. We introduce snarkGPT, a practical ZKML implementation for transformers, to empower users to verify output accuracy and quality while preserving model privacy. We present a series of empirical results studying snarkGPT's scalability and performance to assess the feasibility and challenges of adopting a ZKML-powered approach to capture quality and performance fairness problems in generative AI models.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Ganescu, Bianca-Mihaela and Passerat-Palmbach, Jonathan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06414 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted at PPAI-24: The 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence 2024},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\32XLPPGU\\Ganescu and Passerat-Palmbach - 2024 - Trust the Process Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\PTHKC8WA\\2402.html:text/html},
}

@misc{toreini_verifiable_2023,
	title = {Verifiable {Fairness}: {Privacy}-preserving {Computation} of {Fairness} for {Machine} {Learning} {Systems}},
	shorttitle = {Verifiable {Fairness}},
	url = {http://arxiv.org/abs/2309.06061},
	doi = {10.48550/arXiv.2309.06061},
	abstract = {Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Toreini, Ehsan and Mehrnezhad, Maryam and Moorsel, Aad van},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06061 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	annote = {Comment: accepted in International Workshop on Private, Secure, and Trustworthy AI (PriST-AI), ESORICS'23 workshop},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\UWHYV9QH\\Toreini et al. - 2023 - Verifiable Fairness Privacy-preserving Computation of Fairness for Machine Learning Systems.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\KTKASY2X\\2309.html:text/html},
}

@misc{stewart_datasheets_2025,
	title = {Datasheets for {Machine} {Learning} {Sensors}},
	url = {http://arxiv.org/abs/2306.08848},
	doi = {10.48550/arXiv.2306.08848},
	abstract = {Machine learning (ML) is becoming prevalent in embedded AI sensing systems. These "ML sensors" enable context-sensitive, real-time data collection and decision-making across diverse applications ranging from anomaly detection in industrial settings to wildlife tracking for conservation efforts. As such, there is a need to provide transparency in the operation of such ML-enabled sensing systems through comprehensive documentation. This is needed to enable their reproducibility, to address new compliance and auditing regimes mandated in regulation and industry-specific policy, and to verify and validate the responsible nature of their operation. To address this gap, we introduce the datasheet for ML sensors framework. We provide a comprehensive template, collaboratively developed in academia-industry partnerships, that captures the distinct attributes of ML sensors, including hardware specifications, ML model and dataset characteristics, end-to-end performance metrics, and environmental impacts. Our framework addresses the continuous streaming nature of sensor data, real-time processing requirements, and embeds benchmarking methodologies that reflect real-world deployment conditions, ensuring practical viability. Aligned with the FAIR principles (Findability, Accessibility, Interoperability, and Reusability), our approach enhances the transparency and reusability of ML sensor documentation across academic, industrial, and regulatory domains. To show the application of our approach, we present two datasheets: the first for an open-source ML sensor designed in-house and the second for a commercial ML sensor developed by industry collaborators, both performing computer vision-based person detection.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Stewart, Matthew and Zhang, Yuke and Warden, Pete and Omri, Yasmine and Prakash, Shvetank and Huckelberry, Jacob and Santos, Joao Henrique and Hymel, Shawn and Brown, Benjamin Yeager and MacArthur, Jim and Jeffries, Nat and Moss, Emanuel and Sloane, Mona and Plancher, Brian and Reddi, Vijay Janapa},
	month = oct,
	year = {2025},
	note = {arXiv:2306.08848 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\HHQRLA3R\\Stewart et al. - 2025 - Datasheets for Machine Learning Sensors.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\D4RRPG8A\\2306.html:text/html},
}

@article{imana_having_2023,
	title = {Having your {Privacy} {Cake} and {Eating} it {Too}: {Platform}-supported {Auditing} of {Social} {Media} {Algorithms} for {Public} {Interest}},
	volume = {7},
	issn = {2573-0142},
	shorttitle = {Having your {Privacy} {Cake} and {Eating} it {Too}},
	url = {http://arxiv.org/abs/2207.08773},
	doi = {10.1145/3579610},
	abstract = {Social media platforms curate access to information and opportunities, and so play a critical role in shaping public discourse today. The opaque nature of the algorithms these platforms use to curate content raises societal questions. Prior studies have used black-box methods to show that these algorithms can lead to biased or discriminatory outcomes. However, existing auditing methods face fundamental limitations because they function independent of the platforms. Concerns of potential harm have prompted proposal of legislation in both the U.S. and the E.U. to mandate a new form of auditing where vetted external researchers get privileged access to social media platforms. Unfortunately, to date there have been no concrete technical proposals to provide such auditing, because auditing at scale risks disclosure of users' private data and platforms' proprietary algorithms. We propose a new method for platform-supported auditing that can meet the goals of the proposed legislation. Our first contribution is to enumerate the challenges of existing auditing methods to implement these policies at scale. Second, we suggest that limited, privileged access to relevance estimators is the key to enabling generalizable platform-supported auditing by external researchers. Third, we show platform-supported auditing need not risk user privacy nor disclosure of platforms' business interests by proposing an auditing framework that protects against these risks. For a particular fairness metric, we show that ensuring privacy imposes only a small constant factor increase (6.34x as an upper bound, and 4x for typical parameters) in the number of samples required for accurate auditing. Our technical contributions, combined with ongoing legal and policy efforts, can enable public oversight into how social media platforms affect individuals and society by moving past the privacy-vs-transparency hurdle.},
	number = {CSCW1},
	urldate = {2025-10-31},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Imana, Basileal and Korolova, Aleksandra and Heidemann, John},
	month = apr,
	year = {2023},
	note = {arXiv:2207.08773 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
	pages = {1--33},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\4R8C5VTC\\Imana et al. - 2023 - Having your Privacy Cake and Eating it Too Platform-supported Auditing of Social Media Algorithms f.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\ZKHY7HP9\\2207.html:text/html},
}

@misc{pentyala_privfair_2022,
	title = {{PrivFair}: a {Library} for {Privacy}-{Preserving} {Fairness} {Auditing}},
	shorttitle = {{PrivFair}},
	url = {http://arxiv.org/abs/2202.04058},
	doi = {10.48550/arXiv.2202.04058},
	abstract = {Machine learning (ML) has become prominent in applications that directly affect people's quality of life, including in healthcare, justice, and finance. ML models have been found to exhibit discrimination based on sensitive attributes such as gender, race, or disability. Assessing if an ML model is free of bias remains challenging to date, and by definition has to be done with sensitive user characteristics that are subject of anti-discrimination and data protection law. Existing libraries for fairness auditing of ML models offer no mechanism to protect the privacy of the audit data. We present PrivFair, a library for privacy-preserving fairness audits of ML models. Through the use of Secure Multiparty Computation (MPC), PrivFair protects the confidentiality of the model under audit and the sensitive data used for the audit, hence it supports scenarios in which a proprietary classifier owned by a company is audited using sensitive audit data from an external investigator. We demonstrate the use of PrivFair for group fairness auditing with tabular data or image data, without requiring the investigator to disclose their data to anyone in an unencrypted manner, or the model owner to reveal their model parameters to anyone in plaintext.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Pentyala, Sikha and Melanson, David and Cock, Martine De and Farnadi, Golnoosh},
	month = may,
	year = {2022},
	note = {arXiv:2202.04058 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\PADFDFEY\\Pentyala et al. - 2022 - PrivFair a Library for Privacy-Preserving Fairness Auditing.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\ITL6H69J\\2202.html:text/html},
}

@misc{aalmoes_alignment_2024,
	title = {On the {Alignment} of {Group} {Fairness} with {Attribute} {Privacy}},
	url = {http://arxiv.org/abs/2211.10209},
	doi = {10.48550/arXiv.2211.10209},
	abstract = {Group fairness and privacy are fundamental aspects in designing trustworthy machine learning models. Previous research has highlighted conflicts between group fairness and different privacy notions. We are the first to demonstrate the alignment of group fairness with the specific privacy notion of attribute privacy in a blackbox setting. Attribute privacy, quantified by the resistance to attribute inference attacks (AIAs), requires indistinguishability in the target model's output predictions. Group fairness guarantees this thereby mitigating AIAs and achieving attribute privacy. To demonstrate this, we first introduce AdaptAIA, an enhancement of existing AIAs, tailored for real-world datasets with class imbalances in sensitive attributes. Through theoretical and extensive empirical analyses, we demonstrate the efficacy of two standard group fairness algorithms (i.e., adversarial debiasing and exponentiated gradient descent) against AdaptAIA. Additionally, since using group fairness results in attribute privacy, it acts as a defense against AIAs, which is currently lacking. Overall, we show that group fairness aligns with attribute privacy at no additional cost other than the already existing trade-off with model utility.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Aalmoes, Jan and Duddu, Vasisht and Boutet, Antoine},
	month = mar,
	year = {2024},
	note = {arXiv:2211.10209 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: arXiv admin note: text overlap with arXiv:2202.02242},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\H9LTGA2L\\Aalmoes et al. - 2024 - On the Alignment of Group Fairness with Attribute Privacy.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\SF249S2U\\2211.html:text/html},
}

@misc{dehart_proposing_2021,
	title = {Proposing an {Interactive} {Audit} {Pipeline} for {Visual} {Privacy} {Research}},
	url = {http://arxiv.org/abs/2111.03984},
	doi = {10.48550/arXiv.2111.03984},
	abstract = {In an ideal world, deployed machine learning models will enhance our society. We hope that those models will provide unbiased and ethical decisions that will benefit everyone. However, this is not always the case; issues arise during the data preparation process throughout the steps leading to the models' deployment. The continued use of biased datasets and processes will adversely damage communities and increase the cost of fixing the problem later. In this work, we walk through the decision-making process that a researcher should consider before, during, and after a system deployment to understand the broader impacts of their research in the community. Throughout this paper, we discuss fairness, privacy, and ownership issues in the machine learning pipeline; we assert the need for a responsible human-over-the-loop methodology to bring accountability into the machine learning pipeline, and finally, reflect on the need to explore research agendas that have harmful societal impacts. We examine visual privacy research and draw lessons that can apply broadly to artificial intelligence. Our goal is to systematically analyze the machine learning pipeline for visual privacy and bias issues. We hope to raise stakeholder (e.g., researchers, modelers, corporations) awareness as these issues propagate in this pipeline's various machine learning phases.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {DeHart, Jasmine and Xu, Chenguang and Egede, Lisa and Grant, Christan},
	month = nov,
	year = {2021},
	note = {arXiv:2111.03984 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
	annote = {Comment: Extended version of IEEE BigData 2021 Short Paper, 14 pages, grammar edits},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\H24D9M7J\\DeHart et al. - 2021 - Proposing an Interactive Audit Pipeline for Visual Privacy Research.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\WSKF3YUL\\2111.html:text/html},
}

@article{yazdinejad_ap2fl_2024,
	title = {{AP2FL}: {Auditable} {Privacy}-{Preserving} {Federated} {Learning} {Framework} for {Electronics} in {Healthcare}},
	volume = {70},
	issn = {1558-4127},
	shorttitle = {{AP2FL}},
	url = {https://ieeexplore.ieee.org/document/10261257},
	doi = {10.1109/TCE.2023.3318509},
	abstract = {The growing application of machine learning (ML) techniques in healthcare has led to increased interest in federated learning (FL), which enables the secure and private training of robust ML models. However, conventional FL methods often fall short of providing adequate privacy protection and face challenges in handling non-independent and identically distributed (Non-IID) training data. These shortcomings are of significant concern when employing FL in electronic devices in healthcare. To address these issues, we propose an Auditable Privacy-Preserving Federated Learning (AP2FL) model tailored for electronics in healthcare settings. By leveraging Trusted Execution Environments (TEE), AP2FL ensures secure training and aggregation processes on both client and server sides, effectively mitigating data leakage risks. To manage Non-IID data within the proposed framework, we incorporate the Active Personalized Federated Learning (ActPerFL) model and Batch Normalization (BN) techniques to consolidate user updates and identify data similarities. Additionally, we introduce an auditing mechanism in AP2FL that reveals the contribution of each client to the FL process, facilitating the updating of the global model following diverse data types and distributions. In other words, it ensures the FL process’s integrity, transparency, fairness, and robustness. Our results demonstrate that the proposed AP2FL model outperforms existing methods in accuracy and effectively eliminates privacy leakage.},
	number = {1},
	urldate = {2025-10-31},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Yazdinejad, Abbas and Dehghantanha, Ali and Srivastava, Gautam},
	month = feb,
	year = {2024},
	keywords = {Data models, auditing, Data privacy, FL, healthcare, Load modeling, Medical services, non-IID, Privacy, Servers, Training},
	pages = {2527--2535},
	file = {Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\HNLZKN3H\\10261257.html:text/html},
}

@article{yazdinejad_ap2fl_2024-1,
	title = {{AP2FL}: {Auditable} {Privacy}-{Preserving} {Federated} {Learning} {Framework} for {Electronics} in {Healthcare}},
	volume = {70},
	issn = {0098-3063},
	shorttitle = {{AP2FL}},
	url = {https://doi.org/10.1109/TCE.2023.3318509},
	doi = {10.1109/TCE.2023.3318509},
	abstract = {The growing application of machine learning (ML) techniques in healthcare has led to increased interest in federated learning (FL), which enables the secure and private training of robust ML models. However, conventional FL methods often fall short of providing adequate privacy protection and face challenges in handling non-independent and identically distributed (Non-IID) training data. These shortcomings are of significant concern when employing FL in electronic devices in healthcare. To address these issues, we propose an Auditable Privacy-Preserving Federated Learning (AP2FL) model tailored for electronics in healthcare settings. By leveraging Trusted Execution Environments (TEE), AP2FL ensures secure training and aggregation processes on both client and server sides, effectively mitigating data leakage risks. To manage Non-IID data within the proposed framework, we incorporate the Active Personalized Federated Learning (ActPerFL) model and Batch Normalization (BN) techniques to consolidate user updates and identify data similarities. Additionally, we introduce an auditing mechanism in AP2FL that reveals the contribution of each client to the FL process, facilitating the updating of the global model following diverse data types and distributions. In other words, it ensures the FL process\&amp;\#x2019;s integrity, transparency, fairness, and robustness. Our results demonstrate that the proposed AP2FL model outperforms existing methods in accuracy and effectively eliminates privacy leakage.},
	number = {1},
	urldate = {2025-10-31},
	journal = {IEEE Trans. on Consum. Electron.},
	author = {Yazdinejad, Abbas and Dehghantanha, Ali and Srivastava, Gautam},
	month = feb,
	year = {2024},
	pages = {2527--2535},
}

@article{tang_privacy-preserving_2023,
	title = {Privacy-{Preserving} and {Trustless} {Verifiable} {Fairness} {Audit} of {Machine} {Learning} {Models}},
	volume = {14},
	issn = {2156-5570},
	url = {https://thesai.org/Publications/ViewPaper?Volume=14&Issue=2&Code=IJACSA&SerialNo=94},
	doi = {10.14569/IJACSA.2023.0140294},
	abstract = {In the big data era, machine learning has devel-oped prominently and is widely used in real-world systems. Yet, machine learning raises fairness concerns, which incurs discrimination against groups determined by sensitive attributes such as gender and race. Many researchers have focused on developing fairness audit technique of machine learning model that enable users to protect themselves from discrimination. Existing solutions, however, rely on additional external trust as-sumptions, either on third-party entities or external components, that significantly lower the security. In this study, we propose a trustless verifiable fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. With succinctness and non-interactive of zero knowledge proof, our framework not only guarantees audit integrity, but also clearly enhance security, enabling fair ML models to be publicly auditable and any client to verify audit results without extra trust assumption. Our evaluation on various machine learning models and real-world datasets shows that our framework achieves practical performance.},
	language = {en},
	number = {2},
	urldate = {2025-10-31},
	journal = {International Journal of Advanced Computer Science and Applications (IJACSA)},
	author = {Tang, Gui and Tan, Wuzheng and Cai, Mei},
	month = feb,
	year = {2023},
	note = {Publisher: The Science and Information (SAI) Organization Limited},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\U796UHJ9\\Tang et al. - 2023 - Privacy-Preserving and Trustless Verifiable Fairness Audit of Machine Learning Models.pdf:application/pdf},
}

@article{mittal_responsible_2024,
	title = {On responsible machine learning datasets emphasizing fairness, privacy and regulatory norms with examples in biometrics and healthcare},
	volume = {6},
	copyright = {2024 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-024-00874-y},
	doi = {10.1038/s42256-024-00874-y},
	abstract = {Artificial Intelligence (AI) has seamlessly integrated into numerous scientific domains, catalysing unparalleled enhancements across a broad spectrum of tasks; however, its integrity and trustworthiness have emerged as notable concerns. The scientific community has focused on the development of trustworthy AI algorithms; however, machine learning and deep learning algorithms, popular in the AI community today, intrinsically rely on the quality of their training data. These algorithms are designed to detect patterns within the data, thereby learning the intended behavioural objectives. Any inadequacy in the data has the potential to translate directly into algorithms. In this study we discuss the importance of responsible machine learning datasets through the lens of fairness, privacy and regulatory compliance, and present a large audit of computer vision datasets. Despite the ubiquity of fairness and privacy challenges across diverse data domains, current regulatory frameworks primarily address human-centric data concerns. We therefore focus our discussion on biometric and healthcare datasets, although the principles we outline are broadly applicable across various domains. The audit is conducted through evaluation of the proposed responsible rubric. After surveying over 100 datasets, our detailed analysis of 60 distinct datasets highlights a universal susceptibility to fairness, privacy and regulatory compliance issues. This finding emphasizes the urgent need for revising dataset creation methodologies within the scientific community, especially in light of global advancements in data protection legislation. We assert that our study is critically relevant in the contemporary AI context, offering insights and recommendations that are both timely and essential for the ongoing evolution of AI technologies.},
	language = {en},
	number = {8},
	urldate = {2025-10-31},
	journal = {Nature Machine Intelligence},
	author = {Mittal, Surbhi and Thakral, Kartik and Singh, Richa and Vatsa, Mayank and Glaser, Tamar and Canton Ferrer, Cristian and Hassner, Tal},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Scientific community, Scientific data},
	pages = {936--949},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\GY3A58MB\\Mittal et al. - 2024 - On responsible machine learning datasets emphasizing fairness, privacy and regulatory norms with exa.pdf:application/pdf},
}

@misc{ganescu_trust_2024-1,
	title = {Trust the {Process}: {Zero}-{Knowledge} {Machine} {Learning} to {Enhance} {Trust} in {Generative} {AI} {Interactions}},
	shorttitle = {Trust the {Process}},
	url = {http://arxiv.org/abs/2402.06414},
	doi = {10.48550/arXiv.2402.06414},
	abstract = {Generative AI, exemplified by models like transformers, has opened up new possibilities in various domains but also raised concerns about fairness, transparency and reliability, especially in fields like medicine and law. This paper emphasizes the urgency of ensuring fairness and quality in these domains through generative AI. It explores using cryptographic techniques, particularly Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance fairness and accuracy while protecting model privacy. Applying ZKPs to Machine Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables independent validation of AI-generated content without revealing sensitive model information, promoting transparency and trust. ZKML enhances AI fairness by providing cryptographic audit trails for model predictions and ensuring uniform performance across users. We introduce snarkGPT, a practical ZKML implementation for transformers, to empower users to verify output accuracy and quality while preserving model privacy. We present a series of empirical results studying snarkGPT's scalability and performance to assess the feasibility and challenges of adopting a ZKML-powered approach to capture quality and performance fairness problems in generative AI models.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Ganescu, Bianca-Mihaela and Passerat-Palmbach, Jonathan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06414 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted at PPAI-24: The 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence 2024},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\G36ASHH9\\Ganescu and Passerat-Palmbach - 2024 - Trust the Process Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\V6WKHEY5\\2402.html:text/html},
}

@article{toreini_fairness_2024,
	title = {Fairness as a {Service} ({FaaS}): verifiable and privacy-preserving fairness auditing of machine learning systems},
	volume = {23},
	issn = {1615-5270},
	shorttitle = {Fairness as a {Service} ({FaaS})},
	url = {https://doi.org/10.1007/s10207-023-00774-z},
	doi = {10.1007/s10207-023-00774-z},
	abstract = {Providing trust in machine learning (ML) systems and their fairness is a socio-technical challenge, and while the use of ML continues to rise, there is lack of adequate processes and governance practices to assure their fairness. In this paper, we propose FaaS, a novel privacy-preserving, end-to-end verifiable solution, that audits the algorithmic fairness of ML systems. FaaS offers several features, which are absent from previous designs. The FAAS protocol is model-agnostic and independent of specific fairness metrics and can be utilised as a service by multiple stakeholders. FAAS uses zero knowledge proofs to assure the well-formedness of the cryptograms and provenance in the steps of the protocol. We implement a proof of concept of the FaaS architecture and protocol using off-the-shelf hardware, software, and datasets and run experiments to demonstrate its practical feasibility and to analyse its performance and scalability. Our experiments confirm that our proposed protocol is scalable to large-scale auditing scenarios (e.g. over 1000 participants) and secure against various attack vectors.},
	language = {en},
	number = {2},
	urldate = {2025-10-31},
	journal = {International Journal of Information Security},
	author = {Toreini, Ehsan and Mehrnezhad, Maryam and van Moorsel, Aad},
	month = apr,
	year = {2024},
	keywords = {Auditing, Fairness, Machine learning, Trust, Zero knowledge proofs},
	pages = {981--997},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\XFN3PZ4S\\Toreini et al. - 2024 - Fairness as a Service (FaaS) verifiable and privacy-preserving fairness auditing of machine learnin.pdf:application/pdf},
}

@misc{yuan_quantitative_2025-1,
	title = {Quantitative {Auditing} of {AI} {Fairness} with {Differentially} {Private} {Synthetic} {Data}},
	url = {http://arxiv.org/abs/2504.21634},
	doi = {10.48550/arXiv.2504.21634},
	abstract = {Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Yuan, Chih-Cheng Rex and Wang, Bow-Yaw},
	month = apr,
	year = {2025},
	note = {arXiv:2504.21634 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\XEHDW2MH\\Yuan and Wang - 2025 - Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\5RW7N2N6\\2504.html:text/html},
}

@article{othman_recalibrating_2025,
	title = {Recalibrating {Human}–{Machine} {Relations} through {Bias}-{Aware} {Machine} {Learning}: {Technical} {Pathways} to {Fairness} and {Trust}},
	volume = {5},
	copyright = {Copyright (c) 2025},
	issn = {2634-3584},
	shorttitle = {Recalibrating {Human}–{Machine} {Relations} through {Bias}-{Aware} {Machine} {Learning}},
	url = {https://posthumanism.co.uk/jp/article/view/1091},
	doi = {10.63332/joph.v5i4.1091},
	abstract = {Considering the importance of artificial intelligence (AI) in decision-making processes in various fields such as health, law and finance, the concern for bias and fairness of decision making has increased. This paper presents an extensive discussion of bias-aware machine learning(Ml) such as fairness-aware modeling, detection and mitigation. The paper demonstrates aspects of fairness, different forms of algorithmic bias including intersectional bias and how biased systems impact society. The paper turns to appreciation of dentieth, Trust Dynamics, Legal and Regulatory Frameworks And in the Context of Promoting Transparency: Exploring the Role of Explainable AI (XAI). Taking into account the current advances for combatting bias, also pre-processing, in-processing, and post-processing methods, for instance, draw on examples from major domains of interest. Apart from the improvements AIs have achieved, existing challenges involve little attention to relationship among different identities, poor frameworks in place for implementation and operation in other parts of the world, inadequate abuse detection mechanisms among others. Regarding this, we present some of the research questions that focus on the notions of transparency, privacy protected fairness audits, and shared control with the aim of guiding the growth of fair, responsible, and competent AI systems.},
	language = {en},
	number = {4},
	urldate = {2025-10-31},
	journal = {Journal of Posthumanism},
	author = {Othman, Esam and Mahafdah, Rund},
	month = apr,
	year = {2025},
	keywords = {Algorithmic Fairness, Bias-Aware Machine Learning, Explainable AI (XAI), Intersectional Bias, Trust in AI Systems},
	pages = {448--466},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\95JE5357\\Othman and Mahafdah - 2025 - Recalibrating Human–Machine Relations through Bias-Aware Machine Learning Technical Pathways to Fai.pdf:application/pdf},
}

@article{arcolezi_group_nodate,
	title = {Group {Fairness} {Under} {Obfuscated} {Sensitive} {Information}},
	abstract = {In the era of Big Data, the development of AI systems presents both opportunities and challenges, particularly concerning privacy and fairness. While Differential Privacy (DP) has emerged as a robust methodology for preserving privacy in real-world applications, its local variant (LDP) specifically addresses trust issues by removing the reliance on a centralized server. Equally critical, conducting fairness audits of AI systems helps identify and mitigate discriminatory outcomes in machine learning. Although the relationship between DP and fairness is inherently multifaceted, this paper offers a detailed empirical examination of how collecting multidimensional sensitive attributes under LDP affects fairness in binary classification tasks. Our findings reveal that LDP can slightly improve fairness without substantially degrading model performance–challenging the notion that DP necessarily exacerbates unfairness. We demonstrate these results by evaluating seven state-of-the-art LDP protocols on three benchmark datasets, using established group fairness metrics. Moreover, we propose a novel privacy budget allocation scheme that incorporates varying domain sizes of sensitive attributes, achieving a superior privacy-utility-fairness trade-off compared to existing solutions.},
	language = {en},
	author = {Arcolezi, Héber Hwang and Makhlouf, Karima and Palamidessi, Catuscia},
	file = {PDF:C\:\\Users\\mattia\\Zotero\\storage\\5XYT94DA\\Arcolezi et al. - Group Fairness Under Obfuscated Sensitive Information.pdf:application/pdf},
}

@article{arcolezi_group_nodate-1,
	title = {Group {Fairness} {Under} {Obfuscated} {Sensitive} {Information}},
	abstract = {In the era of Big Data, the development of AI systems presents both opportunities and challenges, particularly concerning privacy and fairness. While Differential Privacy (DP) has emerged as a robust methodology for preserving privacy in real-world applications, its local variant (LDP) specifically addresses trust issues by removing the reliance on a centralized server. Equally critical, conducting fairness audits of AI systems helps identify and mitigate discriminatory outcomes in machine learning. Although the relationship between DP and fairness is inherently multifaceted, this paper offers a detailed empirical examination of how collecting multidimensional sensitive attributes under LDP affects fairness in binary classification tasks. Our findings reveal that LDP can slightly improve fairness without substantially degrading model performance–challenging the notion that DP necessarily exacerbates unfairness. We demonstrate these results by evaluating seven state-of-the-art LDP protocols on three benchmark datasets, using established group fairness metrics. Moreover, we propose a novel privacy budget allocation scheme that incorporates varying domain sizes of sensitive attributes, achieving a superior privacy-utility-fairness trade-off compared to existing solutions.},
	language = {en},
	author = {Arcolezi, Héber Hwang and Makhlouf, Karima and Palamidessi, Catuscia},
	file = {PDF:C\:\\Users\\mattia\\Zotero\\storage\\YTZY37JL\\Arcolezi et al. - Group Fairness Under Obfuscated Sensitive Information.pdf:application/pdf},
}

@incollection{barhamgi_alignment_2025,
	address = {Singapore},
	title = {On the {Alignment} of {Group} {Fairness} with {Attribute} {Privacy}},
	volume = {15437},
	isbn = {978-981-96-0566-8 978-981-96-0567-5},
	url = {https://link.springer.com/10.1007/978-981-96-0567-5_24},
	abstract = {Machine learning (ML) models have been adopted for applications with high-stakes decision-making like healthcare and criminal justice. To ensure trustworthy ML models, the new AI regulations (e.g., AI Act) have established several pillars such as privacy, safety and fairness that model design must take into account. Designing such models requires an understanding of the interactions between fairness deﬁnitions with diﬀerent notions of privacy. Speciﬁcally, the interaction of group fairness (i.e., protection against discriminatory behaviour across demographic subgroups) with attribute privacy (i.e., resistance to attribute inference attacks—AIAs), has not been comprehensively studied. In this paper, we study in depth, both theoretically and empirically, the alignment of group fairness with attribute privacy in a blackbox setting. We ﬁrst propose AdaptAIA, which outperforms existing AIAs on real-world datasets with class imbalances in sensitive attributes. We then show that group fairness theoretically bounds the success of AdaptAIA, which depends on the choice of fairness metrics (e.g., demographic parity or equalized odds). Through our empirical study, we show that attribute privacy can be achieved from group fairness at no additional cost other than the already existing trade-oﬀ with utility. Our work has several implications: i) group fairness acts as a defense against AIAs, which is currently lacking, ii) practitioners do not need to explicitly train models for both fairness and privacy to meet regulatory requirements, iii) AdaptAIA can be used for blackbox auditing of group fairness.},
	language = {en},
	urldate = {2025-10-31},
	booktitle = {Web {Information} {Systems} {Engineering} – {WISE} 2024},
	publisher = {Springer Nature Singapore},
	author = {Aalmoes, Jan and Duddu, Vasisht and Boutet, Antoine},
	editor = {Barhamgi, Mahmoud and Wang, Hua and Wang, Xin},
	year = {2025},
	doi = {10.1007/978-981-96-0567-5_24},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {333--348},
	file = {PDF:C\:\\Users\\mattia\\Zotero\\storage\\9NG332HM\\Aalmoes et al. - 2025 - On the Alignment of Group Fairness with Attribute Privacy.pdf:application/pdf},
}

@article{foalem_logging_2025,
	title = {Logging requirement for continuous auditing of responsible machine learning-based applications},
	volume = {30},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-025-10656-8},
	doi = {10.1007/s10664-025-10656-8},
	abstract = {Machine learning (ML) is increasingly used across various industries to automate decision-making processes. However, concerns about the ethical and legal compliance of ML models have arisen due to their lack of transparency, fairness, and accountability. Monitoring, particularly through logging, is a widely used technique in traditional software systems that could be leveraged to assist in auditing ML-based applications. Logs provide a record of an application’s behavior, which can be used for continuous auditing, debugging, and analyzing both the behavior and performance of the application. In this study, we investigate the logging practices of ML practitioners to capture responsible ML-related information in ML applications. We analyzed 85 ML projects hosted on GitHub, leveraging 20 responsible ML libraries that span principles such as privacy, transparency \& explainability, fairness, and security \& safety. Our analysis revealed important differences in the implementation of responsible AI principles. For example, out of 5,733 function calls analyzed, privacy accounted for 89.3\% (5,120 calls), while fairness represented only 2.1\% (118 calls), highlighting the uneven emphasis on these principles across projects. Furthermore, our manual analysis of 44,877 issue discussions revealed that only 8.1\% of the sampled issues addressed responsible AI principles, with transparency \& explainability being the most frequently discussed principles (32.2\% of all issues related to responsible AI principles). Additionally, a survey conducted with ML practitioners provided direct insights into their perspectives, informing our exploration of ways to enhance logging practices for more effective, responsible ML auditing. We discovered that while privacy, model interpretability \& explainability, fairness, and security \& safety are commonly considered, there is a gap in how metrics associated with these principles are logged. Specifically, crucial fairness metrics like group and individual fairness, privacy metrics such as epsilon and delta, and explainability metrics like SHAP values are not considered current logging practices. The insights from this study highlight the need for ML practitioners and logging tool developers to adopt enhanced logging strategies that incorporate a broader range of responsible AI metrics. This adjustment will facilitate the development of auditable and ethically responsible ML applications, ensuring they meet emerging regulatory and societal expectations. These specific insights offer actionable guidance for improving the accountability and trustworthiness of ML systems.},
	language = {en},
	number = {4},
	urldate = {2025-10-31},
	journal = {Empirical Software Engineering},
	author = {Foalem, Patrick Loic and Silva, Leuson Da and Khomh, Foutse and Li, Heng and Merlo, Ettore},
	month = apr,
	year = {2025},
	keywords = {Auditing, Fairness, Machine learning, Accountability, Empirical, GitHub repository, Logging, Responsible ML, Transparency},
	pages = {97},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\G8T2ZPRA\\Foalem et al. - 2025 - Logging requirement for continuous auditing of responsible machine learning-based applications.pdf:application/pdf},
}

@misc{arcolezi_fair_2025,
	title = {Fair {Play} for {Individuals}, {Foul} {Play} for {Groups}? {Auditing} {Anonymization}'s {Impact} on {ML} {Fairness}},
	shorttitle = {Fair {Play} for {Individuals}, {Foul} {Play} for {Groups}?},
	url = {http://arxiv.org/abs/2505.07985},
	doi = {10.48550/arXiv.2505.07985},
	abstract = {Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as \$k\$-anonymity, \${\textbackslash}ell\$-diversity, and \$t\$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to four orders of magnitude. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Arcolezi, Héber H. and Alishahi, Mina and Bendoukha, Adda-Akram and Kaaniche, Nesrine},
	month = may,
	year = {2025},
	note = {arXiv:2505.07985 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\5GJP2YZQ\\Arcolezi et al. - 2025 - Fair Play for Individuals, Foul Play for Groups Auditing Anonymization's Impact on ML Fairness.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\7D5G3SX4\\2505.html:text/html},
}

@inproceedings{givens_centering_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Centering disability perspectives in algorithmic fairness, accountability, \& transparency},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3375686},
	doi = {10.1145/3351095.3375686},
	abstract = {It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in "anonymized" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the "long tail" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt "reasonable accommodations" for qualified individuals with a disability. But what is a "reasonable accommodation" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.},
	urldate = {2025-10-31},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Givens, Alexandra Reeve and Morris, Meredith Ringel},
	month = jan,
	year = {2020},
	pages = {684},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\PK6RMBPJ\\Givens and Morris - 2020 - Centering disability perspectives in algorithmic fairness, accountability, & transparency.pdf:application/pdf},
}

@misc{lycklama_holding_2024,
	title = {Holding {Secrets} {Accountable}: {Auditing} {Privacy}-{Preserving} {Machine} {Learning}},
	shorttitle = {Holding {Secrets} {Accountable}},
	url = {http://arxiv.org/abs/2402.15780},
	doi = {10.48550/arXiv.2402.15780},
	abstract = {Recent advancements in privacy-preserving machine learning are paving the way to extend the benefits of ML to highly sensitive data that, until now, have been hard to utilize due to privacy concerns and regulatory constraints. Simultaneously, there is a growing emphasis on enhancing the transparency and accountability of machine learning, including the ability to audit ML deployments. While ML auditing and PPML have both been the subjects of intensive research, they have predominately been examined in isolation. However, their combination is becoming increasingly important. In this work, we introduce Arc, an MPC framework for auditing privacy-preserving machine learning. At the core of our framework is a new protocol for efficiently verifying MPC inputs against succinct commitments at scale. We evaluate the performance of our framework when instantiated with our consistency protocol and compare it to hashing-based and homomorphic-commitment-based approaches, demonstrating that it is up to 10{\textasciicircum}4x faster and up to 10{\textasciicircum}6x more concise.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Lycklama, Hidde and Viand, Alexander and Küchler, Nicolas and Knabenhans, Christian and Hithnawi, Anwar},
	month = sep,
	year = {2024},
	note = {arXiv:2402.15780 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	annote = {Comment: 25 pages},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\AK6CQR39\\Lycklama et al. - 2024 - Holding Secrets Accountable Auditing Privacy-Preserving Machine Learning.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\RDZMSBPU\\2402.html:text/html},
}

@misc{toreini_verifiable_2023-1,
	title = {Verifiable {Fairness}: {Privacy}-preserving {Computation} of {Fairness} for {Machine} {Learning} {Systems}},
	shorttitle = {Verifiable {Fairness}},
	url = {http://arxiv.org/abs/2309.06061},
	doi = {10.48550/arXiv.2309.06061},
	abstract = {Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Toreini, Ehsan and Mehrnezhad, Maryam and Moorsel, Aad van},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06061 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	annote = {Comment: accepted in International Workshop on Private, Secure, and Trustworthy AI (PriST-AI), ESORICS'23 workshop},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\F5KB7WL4\\Toreini et al. - 2023 - Verifiable Fairness Privacy-preserving Computation of Fairness for Machine Learning Systems.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\L8DQP7SE\\2309.html:text/html},
}

@inproceedings{park_fairness_2022,
	address = {Virtual Event, Lyon France},
	title = {Fairness {Audit} of {Machine} {Learning} {Models} with {Confidential} {Computing}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512244},
	doi = {10.1145/3485447.3512244},
	abstract = {Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-ofthe-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.},
	language = {en},
	urldate = {2025-10-31},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {ACM},
	author = {Park, Saerom and Kim, Seongmin and Lim, Yeon-sup},
	month = apr,
	year = {2022},
	pages = {3488--3499},
	file = {PDF:C\:\\Users\\mattia\\Zotero\\storage\\7IGJSAWR\\Park et al. - 2022 - Fairness Audit of Machine Learning Models with Confidential Computing.pdf:application/pdf},
}

@inproceedings{park_fairness_2022-1,
	address = {Virtual Event, Lyon France},
	title = {Fairness {Audit} of {Machine} {Learning} {Models} with {Confidential} {Computing}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512244},
	doi = {10.1145/3485447.3512244},
	language = {en},
	urldate = {2025-10-31},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {ACM},
	author = {Park, Saerom and Kim, Seongmin and Lim, Yeon-sup},
	month = apr,
	year = {2022},
	pages = {3488--3499},
}

@inproceedings{park_fairness_2022-2,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Fairness {Audit} of {Machine} {Learning} {Models} with {Confidential} {Computing}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512244},
	doi = {10.1145/3485447.3512244},
	abstract = {Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-of-the-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.},
	urldate = {2025-10-31},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Park, Saerom and Kim, Seongmin and Lim, Yeon-sup},
	month = apr,
	year = {2022},
	pages = {3488--3499},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\XDMDM2RZ\\Park et al. - 2022 - Fairness Audit of Machine Learning Models with Confidential Computing.pdf:application/pdf},
}

@misc{weng_deepchain_2018,
	title = {{DeepChain}: {Auditable} and {Privacy}-{Preserving} {Deep} {Learning} with {Blockchain}-based {Incentive}},
	shorttitle = {{DeepChain}},
	url = {https://eprint.iacr.org/2018/679},
	abstract = {Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning
tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this paper, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a DeepChain prototype and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.},
	urldate = {2025-10-31},
	author = {Weng, Jiasi and Weng, Jian and Zhang, Jilian and Li, Ming and Zhang, Yue and Luo, Weiqi},
	year = {2018},
	note = {Publication info: Preprint. MINOR revision.},
	keywords = {Blockchain, Deep learning, Incentive, Privacy-preserving training},
	annote = {We revised the paper by adjusting the structure of the article.},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\96HKPN4A\\Weng et al. - 2018 - DeepChain Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive.pdf:application/pdf},
}

@misc{toreini_verifiable_2023-2,
	title = {Verifiable {Fairness}: {Privacy}-preserving {Computation} of {Fairness} for {Machine} {Learning} {Systems}},
	shorttitle = {Verifiable {Fairness}},
	url = {http://arxiv.org/abs/2309.06061},
	doi = {10.48550/arXiv.2309.06061},
	abstract = {Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Toreini, Ehsan and Mehrnezhad, Maryam and Moorsel, Aad van},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06061 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	annote = {Comment: accepted in International Workshop on Private, Secure, and Trustworthy AI (PriST-AI), ESORICS'23 workshop},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\ULLLJ37A\\Toreini et al. - 2023 - Verifiable Fairness Privacy-preserving Computation of Fairness for Machine Learning Systems.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\3S4HQKMZ\\2309.html:text/html},
}

@article{laine_ethics-based_2024,
	title = {Ethics-based {AI} auditing: {A} systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders},
	volume = {61},
	issn = {0378-7206},
	shorttitle = {Ethics-based {AI} auditing},
	url = {https://www.sciencedirect.com/science/article/pii/S037872062400051X},
	doi = {10.1016/j.im.2024.103969},
	abstract = {This systematic literature review synthesizes the conceptualizations of ethical principles in AI auditing literature and the knowledge contributions to the stakeholders of AI auditing. We explain how the literature discusses fairness, transparency, non-maleficence, responsibility, privacy, trust, beneficence, and freedom/autonomy. Conceptualizations vary along social/technical- and process/outcome-oriented dimensions. The main stakeholders of ethics-based AI auditing are system developers and deployers, the wider public, researchers, auditors, AI system users, and regulators. AI auditing provides three types of knowledge contributions to stakeholders: 1) guidance; 2) methods, tools, and frameworks; and 3) awareness and empowerment.},
	number = {5},
	urldate = {2025-10-31},
	journal = {Information \& Management},
	author = {Laine, Joakim and Minkkinen, Matti and Mäntymäki, Matti},
	month = jul,
	year = {2024},
	keywords = {Auditing, AI ethics, Artificial intelligence, AI auditing, AI governance, Ethics-based AI auditing, Systematic literature review},
	pages = {103969},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\LGQXMZEX\\Laine et al. - 2024 - Ethics-based AI auditing A systematic literature review on conceptualizations of ethical principles.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\CWHLIQX5\\S037872062400051X.html:text/html},
}

@article{mastrogiovanni_ai-driven_2025,
	title = {{AI}-{Driven} data governance for smart cities: balancing privacy, efficiency, and public trust},
	volume = {6},
	issn = {2675-5459},
	shorttitle = {{AI}-{Driven} data governance for smart cities},
	url = {https://ojs.southfloridapublishing.com/ojs/index.php/jdev/article/view/5810},
	doi = {10.46932/sfjdv6n9-029},
	abstract = {The integration of Artificial Intelligence (AI) in smart cities has transformed urban governance, enhancing efficiency in public services, infrastructure management, and decision-making. However, the widespread use of AI for data collection and analysis raises significant challenges related to privacy, algorithmic bias, transparency, and public trust. Without proper governance, AI systems risk exacerbating inequalities, infringing on citizen rights, and reducing accountability in automated decision-making. This paper explores how AI-driven frameworks can enhance data governance while ensuring privacy protection, algorithmic fairness, and citizen empowerment. Key strategies include federated learning to enable decentralized data processing, differential privacy to protect individual identities, and explainable AI to increase transparency in automated decisions. Additionally, bias detection mechanisms and algorithmic audits are essential to prevent discrimination in AI-driven urban systems. Public trust is crucial in smart city initiatives, requiring citizen engagement models, participatory AI councils, and transparent data-sharing policies. Case studies illustrate how open data initiatives, AI-driven services, and digital innovation can enhance public service delivery and civic engagement when guided by strong governance and ethical data management. The paper proposes a comprehensive governance framework integrating privacy-centric AI, fairness-aware algorithms, and public engagement strategies to ensure sustainable, transparent, and accountable AI-driven urban ecosystems. The findings suggest that aligning technological innovation with inclusive policies and capacity-building not only improves urban efficiency and resilience but also builds public trust and empowerment. By aligning technological advancements with ethical and legal safeguards, smart cities can optimize AI’s potential while maintaining public trust and regulatory compliance.},
	language = {en},
	number = {9},
	urldate = {2025-10-31},
	journal = {South Florida Journal of Development},
	author = {Mastrogiovanni, Sergio},
	month = sep,
	year = {2025},
	keywords = {Data privacy, AI Governance, Algorithmic bias, Ethical AI, Public trust, Smart cities},
	pages = {e5810--e5810},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\T36D4CQL\\Mastrogiovanni - 2025 - AI-Driven data governance for smart cities balancing privacy, efficiency, and public trust.pdf:application/pdf},
}

@misc{clavell_demographic_2025,
	title = {Demographic {Benchmarking}: {Bridging} {Socio}-{Technical} {Gaps} in {Bias} {Detection}},
	shorttitle = {Demographic {Benchmarking}},
	url = {http://arxiv.org/abs/2501.15985},
	doi = {10.48550/arXiv.2501.15985},
	abstract = {Artificial intelligence (AI) models are increasingly autonomous in decision-making, making pursuing responsible AI more critical than ever. Responsible AI (RAI) is defined by its commitment to transparency, privacy, safety, inclusiveness, and fairness. But while the principles of RAI are clear and shared, RAI practices and auditing mechanisms are still incipient. A key challenge is establishing metrics and benchmarks that define performance goals aligned with RAI principles. This paper describes how the ITACA AI auditing platform developed by Eticas.ai tackles demographic benchmarking when auditing AI recommender systems. To this end, we describe a Demographic Benchmarking Framework designed to measure the populations potentially impacted by specific AI models. The framework serves us as auditors as it allows us to not just measure but establish acceptability ranges for specific performance indicators, which we share with the developers of the systems we audit so they can build balanced training datasets and measure and monitor fairness throughout the AI lifecycle. It is also a valuable resource for policymakers in drafting effective and enforceable regulations. Our approach integrates socio-demographic insights directly into AI systems, reducing bias and improving overall performance. The main contributions of this study include:1. Defining control datasets tailored to specific demographics so they can be used in model training; 2. Comparing the overall population with those impacted by the deployed model to identify discrepancies and account for structural bias; and 3. Quantifying drift in different scenarios continuously and as a post-market monitoring mechanism.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Clavell, Gemma Galdon and González-Sendino, Rubén and Vazquez, Paola},
	month = jan,
	year = {2025},
	note = {arXiv:2501.15985 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\FRFKR33F\\Clavell et al. - 2025 - Demographic Benchmarking Bridging Socio-Technical Gaps in Bias Detection.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\V2Q5IJA9\\2501.html:text/html},
}

@article{sharma_ai_2025,
	title = {{AI} {Ethics} {Compliance} {System} ({AECS})},
	volume = {7},
	copyright = {Creative Commons Attribution-ShareAlike 4.0 International License},
	issn = {2582-2160},
	url = {https://www.ijfmr.com/research-paper.php?id=50414},
	doi = {10.36948/ijfmr.2025.v07i04.50414},
	number = {4},
	urldate = {2025-10-31},
	journal = {IJFMR - International Journal For Multidisciplinary Research},
	author = {Sharma, Prapti},
	month = jul,
	year = {2025},
	note = {Publisher: IJFMR},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\RRQKB3KM\\Sharma - 2025 - AI Ethics Compliance System (AECS).pdf:application/pdf},
}

@phdthesis{kim_privacy_2023,
	title = {Privacy {Auditing} of {Tabular} {Synthetic} {Data} {Generators} {Using} {Membership} {Inference} {Attacks}},
	url = {https://escholarship.org/uc/item/8b51r154},
	abstract = {Synthetic data is a promising technology with numerous benefits for data sharing and machine learning workflows, such as augmenting available data, bolstering fairness, and implementing privacy. As synthetic data becomes more and more prevalent, understanding its methodology and strengths and weaknesses in regards to the above promises becomes crucial. This thesis in particular explores the privacy implications of generative models for synthetic data by investigating whether these models deliver on their proposed privacy guarantees. We analyze the increasingly popular GAN-based methods for generating synthetic data, including CTGAN, DP-CTGAN, and PATE-GAN. To investigate the privacy delivered by these methods, we focus on the approach of adversarial privacy auditing, which utilizes a toolbox of adversarial attacks to detect privacy leakage in (differentially private) algorithms. We aim to extend previous work in privacy auditing, which typically focuses on general machine learning algorithms, to the scarcely examined case of synthetic data generators by analyzing a range of models and datasets. Our goal is to demonstrate the need for further exploration and application of privacy auditing in the scenario of synthetic data, provide insights by comparing behavior across different datasets, and offer simulation results for future investigations into various privacy preservation patterns. For example, experimental results on various datasets and target data records reveal differences in privacy outcomes, highlighting the important role of the data, independent of the synthetic data generator, in privacy preservation. The findings highlight the importance of data-centric privacy evaluations and the need for further work to achieve a truly tight privacy analysis. This research hopes to serve as a foundation for further studying adversarial privacy auditing and possibly for the development of robust defenses against privacy leakage in the future.},
	language = {en},
	urldate = {2025-10-31},
	school = {UCLA},
	author = {Kim, Nicklaus},
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\ZEUMJME5\\Kim - 2023 - Privacy Auditing of Tabular Synthetic Data Generators Using Membership Inference Attacks.pdf:application/pdf},
}

@article{sunkara_ethical_2024,
	title = {Ethical and regulatory implications of {AI} in cybersecurity surveillance},
	volume = {24},
	copyright = {Copyrights to World Journal of Advanced Research and Reviews},
	issn = {2581-9615},
	url = {https://wjarr.com/content/ethical-and-regulatory-implications-ai-cybersecurity-surveillance},
	doi = {10.30574/wjarr.2024.24.2.3571},
	abstract = {The high adoption rate of Artificial Intelligence (AI) in cybersecurity surveillance has played a major role in increasing detection of threats, behaviour, as well as incident response actions. Nevertheless, the technology has essential ethical and regulatory issues that concern privacy, consent, bias of algorithms, and accountability. The submitted paper researches the ethical issues and regulatory gaps, which arise due to the implementation of AI-based surveillance solutions in the area of both the private and the public sphere. Based on an interdisciplinary exploration of existing literature and regulatory approaches (including the EU GDPR and AI Act and the CCPA in the U.S.) as well as prominent examples of case studies, this paper will analyze the way in which these technologies threaten customary standards of transparency, fairness, and civil liberties. Analysis shows that the current regulations tend to be inconsistent, reactive, and poorly placed to handle the transparency and freedom of AI surveillance systems. It also points out the danger of increasing social disparities by means of unregulated algorithmic profiling. To alleviate such challenges, the paper proposes the multi-stakeholder methodology which implies to harmonize policies, incorporate the process of explainable AI, enforce the regular algorithmic audits, and introduce the privacy-preserving AI methodologies. Finally, the paper recommends proactive ethical governance and adaptive regulatory innovation so that cybersecurity surveillance technologies may work in the betterment of society without affecting the rights of individuals.},
	language = {en},
	number = {2},
	urldate = {2025-10-31},
	journal = {World Journal of Advanced Research and Reviews},
	author = {Sunkara, Goutham},
	year = {2024},
	note = {Last Modified: 2025-07-16T08:34+05:30
Publisher: World Journal of Advanced Research and Reviews},
	pages = {2895--2905},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\PPWQB66L\\Sunkara - 2024 - Ethical and regulatory implications of AI in cybersecurity surveillance.pdf:application/pdf},
}

@article{tariq_ethical_2025,
	title = {Ethical {Imperatives} in {AI} {Design}: {A} {Comprehensive} {Framework} for {Risk} {Mitigation} and {Responsible} {Innovation}},
	volume = {1},
	shorttitle = {Ethical {Imperatives} in {AI} {Design}},
	doi = {10.71346/utj.v1i2.23},
	abstract = {As artificial intelligence (AI) becomes increasingly integral to critical sectors, the gap between abstract ethical principles and their concrete technical implementation presents a significant barrier to responsible innovation. This paper addresses this challenge by introducing a comprehensive framework designed to embed ethical considerations directly into the AI development lifecycle. The primary objective is to provide an operational methodology for proactive risk mitigation and the construction of verifiably trustworthy systems. Our proposed framework is structured around a core set of guiding principles, including fairness, transparency, accountability, and privacy. It advocates a multi-layered risk mitigation strategy that spans the design, development, deployment, and governance phases of AI systems. This approach integrates specific methodologies and tools, such as Ethical Impact Assessments, bias auditing techniques, Explainable AI (XAI) methods, and privacy-preserving technologies. The key contribution is a unified, actionable architecture that bridges the operationalization and fragmentation gaps currently plaguing the field. By systematically connecting high-level ethical goals to specific engineering practices and auditable checkpoints, this framework offers a practical pathway for developers and organizations to foster responsible AI and mitigate potential societal harms, ensuring technology remains aligned with human values.},
	journal = {Ubiquitous Technology Journal},
	author = {Tariq, Bilal and Ashraf, Muhammad and Rashid, Umar},
	month = jun,
	year = {2025},
	pages = {61--73},
}

@article{raza_power_2025,
	title = {The {Power} of {Artificial} {Intelligence} in {Recruitment}: {A} {Theoretical} {Analysis} of {Current} {AI}-{Based} {Recruitment} {Strategies}},
	volume = {09},
	shorttitle = {The {Power} of {Artificial} {Intelligence} in {Recruitment}},
	doi = {10.55041/IJSREM49671},
	abstract = {This report critically examines the burgeoning integration of Artificial Intelligence (AI) within recruitment processes, dissecting the inherent tension between aspirations for heightened efficiency and the crucial imperative of maintaining ethical standards. By examining AI tools used in pre-screening, candidate engagement, and evaluation through the diverse frameworks of Human Capital Theory, Organizational Justice Theory, the Technology Acceptance Model (TAM), and Critical Theory, this study highlights underlying concerns such as algorithmic bias, the risk of impersonal or dehumanized candidate experiences, and the diminishing protection of applicant privacy. It contributes a novel and comprehensive framework meticulously designed for evaluating AI recruitment strategies, integrating disparate theoretical perspectives to furnish practical guidance for both Human Resource (HR) professionals and AI vendors. Emphasizing the necessity of rigorous ethical audits, algorithmic transparency, the indispensable role of human oversight, and a steadfast commitment to responsible AI development and deployment, the report advocates for proactive measures to ensure fairness, inclusivity, and a positive candidate experience. Furthermore, it identifies promising avenues for future research, including longitudinal studies to assess long-term impacts on diversity and the development of robust and reliable fairness metrics Keywords: AI, Recruitment, Algorithmic Bias, Fairness, Candidate Experience, Human Resources, Ethics, Theoretical Framework, Organizational Justice, Human Capital Theory, Technology Acceptance Model, Critical Theory.},
	journal = {INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT},
	author = {Raza, Sayeed},
	month = jun,
	year = {2025},
	pages = {1--9},
}

@misc{noauthor_power_nodate,
	title = {The {Power} of {Artificial} {Intelligence} in {Recruitment}: {A} {Theoretical} {Analysis} of {Current} {AI}-{Based} {Recruitment} {Strategies} – {IJSREM}},
	shorttitle = {The {Power} of {Artificial} {Intelligence} in {Recruitment}},
	url = {https://ijsrem.com/download/the-power-of-artificial-intelligence-in-recruitment-a-theoretical-analysis-of-current-ai-based-recruitment-strategies/},
	language = {en-US},
	urldate = {2025-10-31},
	file = {Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\9XIY76EZ\\the-power-of-artificial-intelligence-in-recruitment-a-theoretical-analysis-of-current-ai-based-.html:text/html},
}

@article{yenuganti_designing_2025,
	title = {Designing ethical and transparent {AI} for regulated enterprise environments},
	volume = {15},
	copyright = {Copyrights to World Journal of Advanced Engineering Technology and Sciences},
	issn = {2582-8266},
	url = {https://journalwjaets.com/content/designing-ethical-and-transparent-ai-regulated-enterprise-environments},
	doi = {10.30574/wjaets.2025.15.3.0963},
	abstract = {The deployment of Artificial Intelligence systems within regulated enterprise environments presents significant challenges in maintaining ethical standards while achieving operational objectives. This article addresses the critical need for transparent and accountable AI architectures that satisfy regulatory requirements across sectors including financial services, healthcare, and telecommunications. The framework presented encompasses four foundational principles: transparency and explainability, fairness and non-discrimination, accountability and auditability, and privacy protection. Implementation strategies include the development of auditable AI pipelines with comprehensive data governance, policy-driven constraint systems that automate compliance enforcement, human-in-the-loop validation mechanisms for critical oversight, and transparent decision communication interfaces for end-user understanding. The architectural solutions demonstrate how organizations can successfully balance innovation with regulatory compliance through systematic integration of ethical considerations into AI system design. These implementations provide measurable improvements in compliance rates, stakeholder trust, and operational reliability while maintaining competitive advantages in AI-driven business processes.},
	language = {en},
	number = {3},
	urldate = {2025-10-31},
	journal = {World Journal of Advanced Engineering Technology and Sciences},
	author = {Yenuganti, Goutham},
	year = {2025},
	pages = {776--782},
	file = {Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\HPMMHAU4\\designing-ethical-and-transparent-ai-regulated-enterprise-environments.html:text/html},
}

@article{ade-ibijola_ethical_2025,
	title = {Ethical {Imperatives} in {AI} {Design} for {Risk} {Mitigation} and {Responsible} {Innovation}},
	volume = {1},
	doi = {10.71346/utj.v1i2.20},
	abstract = {Artificial Intelligence has become integral to modern systems across critical sectors, yet its widespread deployment raises serious ethical and risk-related concerns that remain inadequately addressed in current design practices. This paper identifies the urgent need for a structured approach that bridges the gap between normative ethics and technical implementation in AI systems. In response, it introduces a comprehensive framework that supports ethical AI design and proactive risk mitigation. The framework is built upon guiding principles including fairness, transparency, accountability, privacy, robustness, and human oversight. It incorporates a multi-layered strategy that embeds these principles into the design, development, deployment, and governance phases of AI. Methodologies such as bias auditing, explain ability techniques, privacy-preserving computation, and ethical impact assessments are detailed as core tools within this structure. The framework’s applicability is demonstrated through domain-specific case studies, showcasing how it addresses real-world challenges in autonomous vehicles, healthcare diagnostics, financial services, recommendation systems, and large language models. Key contributions of this research include a replicable structure for ethical compliance, the operationalization of abstract ethical goals, and actionable guidance for integrating social values into AI pipelines. This work concludes by affirming the critical role of ethical frameworks in guiding responsible AI innovation and calls for sustained interdisciplinary efforts to align future technological progress with human and societal well-being.},
	journal = {Ubiquitous Technology Journal},
	author = {Ade-Ibijola, Abejide and Nakatumba-Nabende, Joyce},
	month = jun,
	year = {2025},
	pages = {33--45},
}

@article{ravva_automated_2025,
	title = {Automated compliance verification for {AI} models in enterprise {Cloud} {MLOps} {Pipelines}},
	volume = {26},
	copyright = {Copyrights to World Journal of Advanced Research and Reviews},
	issn = {2581-9615},
	url = {https://journalwjarr.com/content/automated-compliance-verification-ai-models-enterprise-cloud-mlops-pipelines},
	doi = {10.30574/wjarr.2025.26.3.2167},
	abstract = {Ensuring that AI models used in business intelligence systems comply with regulations represents a critical governance challenge as rapid development cycles enabled by MLOps on cloud platforms accelerate model deployment. Manual verification processes prove slow, error-prone, and unscalable in this environment. This article explores techniques and frameworks for automating compliance verification directly within cloud-based MLOps pipelines, investigating the integration of automated checks for fairness, explainability, privacy protection, and robustness testing. The integration of these verification capabilities as mandatory gates in the CI/CD pipeline transforms compliance from a periodic manual activity to an integral part of the development workflow. A reference architecture is proposed that leverages cloud-native services to enforce compliance checks, addressing the challenges of defining quantifiable metrics for complex regulations while enhancing the speed, reliability, and auditability of AI model governance in enterprise cloud environments. The proposed implementation demonstrates how organizations can balance regulatory adherence with innovation velocity, enabling responsible AI deployment at scale.},
	language = {en},
	number = {3},
	urldate = {2025-10-31},
	journal = {World Journal of Advanced Research and Reviews},
	author = {Ravva, Karthik},
	year = {2025},
	pages = {1035--1042},
	file = {Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\9L3Q2RKY\\automated-compliance-verification-ai-models-enterprise-cloud-mlops-pipelines.html:text/html},
}

@article{oyebode_chain--trust_2025,
	title = {Chain-of-{Trust} {AI}: {Zero}-{Knowledge} {Verified} {Federated} {Reinforcement} and {Generative} {Learning} for {Interpretable}, {Bias}-{Free} {Decision}-{Making} in {Decentralized} {Complex} {Systems}},
	volume = {15},
	copyright = {Copyrights to International Journal of Science and Research Archive},
	issn = {2582-8185},
	shorttitle = {Chain-of-{Trust} {AI}},
	url = {https://journalijsra.com/index.php/content/chain-trust-ai-zero-knowledge-verified-federated-reinforcement-and-generative-learning},
	doi = {10.30574/ijsra.2025.15.2.1613},
	abstract = {The rapid growth of artificial intelligence (AI) in decentralized systems such as healthcare, financial networks, and autonomous transportation has underscored the critical need for interpretability, fairness, and verifiable trust in decision-making. Traditional federated learning frameworks, while addressing data privacy and scalability, often suffer from bias propagation, opaque model behaviors, and limited mechanisms for ensuring accountability. This article introduces Chain-of-Trust AI, a novel paradigm that integrates zero-knowledge proofs (ZKPs), federated reinforcement learning (FRL), and generative learning models to create an interpretable, bias-free, and verifiable decision-making framework for complex distributed environments. The proposed framework leverages FRL to enable adaptive coordination across heterogeneous agents while maintaining local data sovereignty. Generative learning models, such as variational autoencoders, provide transparent causal representations that support bias detection and enhance interpretability of reinforcement-driven policies. ZKPs are embedded as cryptographic guarantees to verify model updates and decision outcomes without exposing sensitive information, thus ensuring compliance, trust, and transparency across decentralized networks. Methodologically, the framework is evaluated through MATLAB-based multi-agent simulations, benchmarking performance in terms of interpretability, fairness indices, convergence stability, and verification overhead. Theoretical analyses confirm convergence under heterogeneous reward structures, cryptographic soundness of proofs, and bias reduction capabilities through generative regularization. Case studies in decentralized healthcare diagnostics, financial fraud detection, and autonomous vehicular coordination highlight the practical scalability and robustness of Chain-of-Trust AI. By uniting reinforcement learning, generative interpretability, and zero-knowledge verification, this work pioneers a secure, auditable, and ethically aligned AI architecture for decentralized complex systems, advancing both technical rigor and governance in distributed intelligence.},
	language = {en},
	number = {2},
	urldate = {2025-10-31},
	journal = {International Journal of Science and Research Archive},
	author = {Oyebode, Oyegoke},
	year = {2025},
	pages = {1876--1896},
	file = {Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\ZRM8TYEW\\chain-trust-ai-zero-knowledge-verified-federated-reinforcement-and-generative-learning.html:text/html},
}

@article{ayobami_digital_2024,
	title = {Digital {Procurement} 4.0: {Redesigning} {Government} {Contracting} {Systems} with {AI}-{Driven} {Ethics}, {Compliance}, and {Performance} {Optimization}},
	volume = {10},
	issn = {2456-3307},
	shorttitle = {Digital {Procurement} 4.0},
	url = {https://ijsrcseit.com/index.php/home/article/view/CSEIT24102138},
	doi = {10.32628/CSEIT24102138},
	abstract = {The advent of Digital Procurement 4.0 marks a transformative shift in government contracting systems, integrating artificial intelligence (AI), data analytics, and automation to enhance transparency, efficiency, and ethical compliance. This study explores the redesign of public procurement frameworks using AI-driven models that ensure not only cost-effectiveness but also adherence to legal, ethical, and performance standards. Traditional procurement systems often grapple with inefficiencies, corruption, lack of accountability, and delayed service delivery. Digital Procurement 4.0 presents an opportunity to counter these limitations through predictive analytics, blockchain-based audit trails, robotic process automation (RPA), and intelligent contract management systems. This paper proposes a comprehensive AI-driven framework that embeds real-time risk detection, compliance verification, vendor performance monitoring, and ethical safeguards throughout the procurement lifecycle. By integrating natural language processing (NLP) for contract analysis, machine learning algorithms for bid evaluation, and automated compliance checkers, governments can ensure fairness, reduce fraud, and promote value-for-money outcomes. Moreover, digital twin technologies enable simulations that forecast procurement outcomes under varying socio-economic scenarios, thus enhancing strategic decision-making. The research draws on recent case studies from digitally advanced governments, demonstrating how AI integration has improved procurement efficiency by up to 45\%, reduced fraud incidences by 30\%, and enhanced stakeholder trust. Additionally, the study outlines a regulatory and governance blueprint to mitigate algorithmic bias and ensure accountability in AI-led procurement systems. Particular emphasis is placed on ethical algorithm design, data transparency, and participatory oversight mechanisms involving civil society and independent watchdogs. Ultimately, this paper underscores the national importance of adopting Digital Procurement 4.0 in public sector governance. As public expenditure accounts for over 12\% of global GDP, optimizing this function through technology has widespread implications for fiscal sustainability, public trust, and socio-economic development. This research offers policy recommendations, implementation strategies, and a roadmap for governments aiming to build ethical, efficient, and AI-enabled contracting ecosystems.},
	language = {en},
	number = {2},
	urldate = {2025-10-31},
	journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
	author = {Ayobami, Amusa Tolulope and Mike-Olisa, Uchenna and Ogeawuchi, Jeffrey Chidera and Abayomi, Abraham Ayodeji and Agboola, Oluwademilade Aderemi},
	month = apr,
	year = {2024},
	keywords = {AI in Government Contracting, Automated Bidding, Compliance Optimization, Contracting System Reform, Digital Procurement 4.0, Ethical Procurement, Predictive Compliance, Procurement Transparency, Public Sector Innovation, Vendor Performance Analytics},
	pages = {834--865},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\QFE6D2K5\\Ayobami et al. - 2024 - Digital Procurement 4.0 Redesigning Government Contracting Systems with AI-Driven Ethics, Complianc.pdf:application/pdf},
}

@article{jha_ethical_2025,
	title = {Ethical {AI} {Audits} for {Observability} {Systems}: {Ensuring} {Equitable} {Resilience} in {Cloud} {Infrastructure}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2709-104X},
	shorttitle = {Ethical {AI} {Audits} for {Observability} {Systems}},
	url = {https://al-kindipublisher.com/index.php/jcsts/article/view/9583},
	doi = {10.32996/jcsts.2025.7.4.67},
	abstract = {The integration of artificial intelligence into cloud observability systems has revolutionized infrastructure monitoring while simultaneously introducing equity challenges that disproportionately affect underserved populations. These AI-driven systems, predominantly trained on data from high-density urban environments, frequently exhibit biased performance that manifests as prolonged resolution times and decreased detection accuracy in rural and developing regions. As cloud infrastructure increasingly underpins critical services such as healthcare, education, and financial systems, these disparities represent significant barriers to digital inclusion for billions of users worldwide. This article presents ethical AI auditing as a comprehensive framework to identify, quantify, and mitigate these biases through three key components: synthetic data generation to represent underserved scenarios, fairness metrics implementation to establish quantitative benchmarks, and bias mitigation techniques to correct algorithmic disparities. Case studies across European cloud providers, global content delivery networks, and emergency response systems demonstrate substantial improvements in service equity following audit implementation. Despite challenges related to resource requirements, performance trade-offs, privacy considerations, and evolving regulatory landscapes, ethical AI audits offer a viable path toward equitable cloud resilience that benefits both marginalized users and service providers through expanded market reach, enhanced reputation, and improved regulatory compliance.},
	language = {en},
	number = {4},
	urldate = {2025-10-31},
	journal = {Journal of Computer Science and Technology Studies},
	author = {Jha, Nishant Nisan},
	month = may,
	year = {2025},
	keywords = {Algorithmic bias mitigation, Equitable cloud infrastructure, Fairness metrics implementation, Geographic bias in AI, Synthetic data generation},
	pages = {573--579},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\R3RJ4DYN\\Jha - 2025 - Ethical AI Audits for Observability Systems Ensuring Equitable Resilience in Cloud Infrastructure.pdf:application/pdf},
}

@misc{chandran_ezpc_2017,
	title = {{EzPC}: {Programmable}, {Efficient}, and {Scalable} {Secure} {Two}-{Party} {Computation} for {Machine} {Learning}},
	shorttitle = {{EzPC}},
	url = {https://eprint.iacr.org/2017/1109},
	abstract = {We present EZPC: a secure two-party computation (2PC) framework that generates efficient 2PC protocols from high-level, easy-to-write, programs. EZPC provides formal correctness and security guarantees while maintaining performance and scalability. Previous language frameworks, such as CBMC-GC, ObliVM, SMCL, and Wysteria, generate protocols that use either arithmetic or boolean circuits exclusively. Our compiler is the first to generate protocols that combine both arithmetic sharing and garbled circuits for better performance. We empirically demonstrate that the protocols generated by our framework match or outperform (up to 19x) recent works that provide hand-crafted protocols for various functionalities such as secure prediction and matrix factorization.},
	urldate = {2025-10-31},
	author = {Chandran, Nishanth and Gupta, Divya and Rastogi, Aseem and Sharma, Rahul and Tripathi, Shardul},
	year = {2017},
	note = {Publication info: Preprint. MINOR revision.},
	keywords = {Secure Computation, secure machine learning prediction},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\2APLWFS6\\Chandran et al. - 2017 - EzPC Programmable, Efficient, and Scalable Secure Two-Party Computation for Machine Learning.pdf:application/pdf},
}

@misc{pentyala_privfair_2022-1,
	title = {{PrivFair}: a {Library} for {Privacy}-{Preserving} {Fairness} {Auditing}},
	shorttitle = {{PrivFair}},
	url = {http://arxiv.org/abs/2202.04058},
	doi = {10.48550/arXiv.2202.04058},
	abstract = {Machine learning (ML) has become prominent in applications that directly affect people's quality of life, including in healthcare, justice, and finance. ML models have been found to exhibit discrimination based on sensitive attributes such as gender, race, or disability. Assessing if an ML model is free of bias remains challenging to date, and by definition has to be done with sensitive user characteristics that are subject of anti-discrimination and data protection law. Existing libraries for fairness auditing of ML models offer no mechanism to protect the privacy of the audit data. We present PrivFair, a library for privacy-preserving fairness audits of ML models. Through the use of Secure Multiparty Computation (MPC), PrivFair protects the confidentiality of the model under audit and the sensitive data used for the audit, hence it supports scenarios in which a proprietary classifier owned by a company is audited using sensitive audit data from an external investigator. We demonstrate the use of PrivFair for group fairness auditing with tabular data or image data, without requiring the investigator to disclose their data to anyone in an unencrypted manner, or the model owner to reveal their model parameters to anyone in plaintext.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Pentyala, Sikha and Melanson, David and Cock, Martine De and Farnadi, Golnoosh},
	month = may,
	year = {2022},
	note = {arXiv:2202.04058 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\BZJPAPF3\\Pentyala et al. - 2022 - PrivFair a Library for Privacy-Preserving Fairness Auditing.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\S5JNFB9K\\2202.html:text/html},
}

@misc{bourree_p2nia_2025-1,
	title = {{P2NIA}: {Privacy}-{Preserving} {Non}-{Iterative} {Auditing}},
	shorttitle = {{P2NIA}},
	url = {http://arxiv.org/abs/2504.00874},
	doi = {10.48550/arXiv.2504.00874},
	abstract = {The emergence of AI legislation has increased the need to assess the ethical compliance of high-risk AI systems. Traditional auditing methods rely on platforms' application programming interfaces (APIs), where responses to queries are examined through the lens of fairness requirements. However, such approaches put a significant burden on platforms, as they are forced to maintain APIs while ensuring privacy, facing the possibility of data leaks. This lack of proper collaboration between the two parties, in turn, causes a significant challenge to the auditor, who is subject to estimation bias as they are unaware of the data distribution of the platform. To address these two issues, we present P2NIA, a novel auditing scheme that proposes a mutually beneficial collaboration for both the auditor and the platform. Extensive experiments demonstrate P2NIA's effectiveness in addressing both issues. In summary, our work introduces a privacy-preserving and non-iterative audit scheme that enhances fairness assessments using synthetic or local data, avoiding the challenges associated with traditional API-based audits.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Bourrée, Jade Garcia and Lautraite, Hadrien and Gambs, Sébastien and Tredan, Gilles and Merrer, Erwan Le and Rottembourg, Benoît},
	month = apr,
	year = {2025},
	note = {arXiv:2504.00874 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 8 figures},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\NQTN9NTC\\Bourrée et al. - 2025 - P2NIA Privacy-Preserving Non-Iterative Auditing.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\ECCYDLFJ\\2504.html:text/html},
}

@article{ramsaran_responsible_2025,
	title = {Responsible {AI} in {Revenue} {Lifecycle} {Automation}: {Design} {Patterns} for {Fairness}, {Compliance}, and {Control}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2709-104X},
	shorttitle = {Responsible {AI} in {Revenue} {Lifecycle} {Automation}},
	url = {https://al-kindipublishers.org/index.php/jcsts/article/view/9806},
	doi = {10.32996/jcsts.2025.7.5.39},
	abstract = {This article presents a comprehensive framework for implementing responsible artificial intelligence in revenue lifecycle automation systems. As organizations increasingly deploy AI to enhance revenue operations through contract analysis, pricing optimization, and approval workflows, they face complex ethical considerations and compliance challenges. The framework addresses these challenges through five interconnected domains: fairness in algorithmic decision-making, explainability and transparency, data governance and privacy, human-in-the-loop controls, and compliance and auditability. Drawing from real-world implementations across financial services, technology, and regulated industries, the article outlines practical design patterns that balance innovation with ethical considerations. Case studies demonstrate how organizations have successfully applied these principles to contract intelligence and dynamic pricing systems, achieving both business value and ethical implementation. The article provides a phased implementation roadmap and explores current challenges and future research directions. By embedding responsible AI principles into revenue operations, organizations can mitigate risks while maximizing business value, ensuring systems operate equitably, transparently, and in alignment with organizational values and regulatory requirements.},
	language = {en},
	number = {5},
	urldate = {2025-10-31},
	journal = {Journal of Computer Science and Technology Studies},
	author = {Ramsaran, Jayaprakash},
	month = jun,
	year = {2025},
	keywords = {Algorithmic fairness, contract intelligence, ethical governance, explainable AI, revenue automation},
	pages = {314--327},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\WVQCMKWE\\Ramsaran - 2025 - Responsible AI in Revenue Lifecycle Automation Design Patterns for Fairness, Compliance, and Contro.pdf:application/pdf},
}

@article{yazdinejad_ap2fl_2024-2,
	title = {{AP2FL}: {Auditable} {Privacy}-{Preserving} {Federated} {Learning} {Framework} for {Electronics} in {Healthcare}},
	volume = {70},
	issn = {1558-4127},
	shorttitle = {{AP2FL}},
	url = {https://ieeexplore.ieee.org/document/10261257},
	doi = {10.1109/TCE.2023.3318509},
	abstract = {The growing application of machine learning (ML) techniques in healthcare has led to increased interest in federated learning (FL), which enables the secure and private training of robust ML models. However, conventional FL methods often fall short of providing adequate privacy protection and face challenges in handling non-independent and identically distributed (Non-IID) training data. These shortcomings are of significant concern when employing FL in electronic devices in healthcare. To address these issues, we propose an Auditable Privacy-Preserving Federated Learning (AP2FL) model tailored for electronics in healthcare settings. By leveraging Trusted Execution Environments (TEE), AP2FL ensures secure training and aggregation processes on both client and server sides, effectively mitigating data leakage risks. To manage Non-IID data within the proposed framework, we incorporate the Active Personalized Federated Learning (ActPerFL) model and Batch Normalization (BN) techniques to consolidate user updates and identify data similarities. Additionally, we introduce an auditing mechanism in AP2FL that reveals the contribution of each client to the FL process, facilitating the updating of the global model following diverse data types and distributions. In other words, it ensures the FL process’s integrity, transparency, fairness, and robustness. Our results demonstrate that the proposed AP2FL model outperforms existing methods in accuracy and effectively eliminates privacy leakage.},
	number = {1},
	urldate = {2025-10-31},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Yazdinejad, Abbas and Dehghantanha, Ali and Srivastava, Gautam},
	month = feb,
	year = {2024},
	keywords = {Data models, auditing, Data privacy, FL, healthcare, Load modeling, Medical services, non-IID, Privacy, Servers, Training},
	pages = {2527--2535},
}

@misc{zhou_property_2021,
	title = {Property {Inference} {Attacks} {Against} {GANs}},
	url = {http://arxiv.org/abs/2111.07608},
	doi = {10.48550/arXiv.2111.07608},
	abstract = {While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN's training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.},
	urldate = {2025-10-31},
	publisher = {arXiv},
	author = {Zhou, Junhao and Chen, Yufei and Shen, Chao and Zhang, Yang},
	month = nov,
	year = {2021},
	note = {arXiv:2111.07608 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	annote = {Comment: To Appear in NDSS 2022},
	file = {Preprint PDF:C\:\\Users\\mattia\\Zotero\\storage\\FK8IPYQ6\\Zhou et al. - 2021 - Property Inference Attacks Against GANs.pdf:application/pdf;Snapshot:C\:\\Users\\mattia\\Zotero\\storage\\MAM23TD7\\2111.html:text/html},
}

@article{alshareet_novel_2025,
	title = {A {Novel} {Framework} for {Integrating} {Blockchain}-{Driven} {Federated} {Learning} with {Neural} {Networks} in {E}-{Commerce}},
	volume = {33},
	issn = {1573-7705},
	url = {https://doi.org/10.1007/s10922-025-09928-x},
	doi = {10.1007/s10922-025-09928-x},
	abstract = {Federated learning (FL) enables collaborative model training without raw data sharing; however, its decentralized architecture remains vulnerable to inference attacks, malicious updates, and opaque governance. To address these challenges, we introduce an end-to-end framework integrating FL with permissioned blockchain technology, systematically guided by TRIZ innovation principles, ensuring verifiable, privacy-preserving, and ethically accountable machine learning collaborations. Our framework integrates multi-layered security measures, including encrypted local model updates, blockchain-based smart-contract consensus mechanisms for secure global aggregation, and an immutable complaint-redress system that transparently records grievances, initiates forensic audits, and documents remedial actions. Employing iterative ARIZ cycles, we effectively resolve the contradiction between strict data locality and collective model intelligence. The proposed pipeline comprises eight structured phases: cryptographic initialization, heterogeneous data preparation, dual-model instantiation, client-side optimization, weighted model aggregation, ledger anchoring, proactive dispute resolution, and comprehensive performance evaluation. Experimental evaluations on textual datasets with deliberately designed non-IID distributions demonstrate stable convergence, reduced communication overhead, and robust predictive accuracy across diverse client scenarios. Validation using recurrent neural networks and linear models for e-commerce sentiment analysis, clinical note triage, and vehicular telemetry illustrates the framework’s domain-agnostic versatility. Privacy analyses using gradient-inversion attacks and efficiency benchmarks under varied bandwidth and participation levels confirm robustness. Comparative analysis reveals our approach offers enhanced adaptability, richer analytical capabilities, and explicit ethical integration compared to existing blockchain-enhanced FL solutions. Ultimately, this research proposes a pathway towards transparent, human-centered artificial intelligence systems, harmonizing regulatory compliance, organizational objectives, and societal trust without compromising technical performance. Future studies will explore alternative blockchain consensus mechanisms, formal fairness assessments, and pilot deployments in healthcare and financial sectors.},
	language = {en},
	number = {3},
	urldate = {2025-10-31},
	journal = {Journal of Network and Systems Management},
	author = {Alshareet, Osama and Awasthi, Anjali},
	month = may,
	year = {2025},
	keywords = {Blockchain, Artificial intelligence (AI), Ethical collaboration, Federated learning (FL), Machine learning (ML), TRIZ},
	pages = {56},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\2MR2HKI8\\Alshareet and Awasthi - 2025 - A Novel Framework for Integrating Blockchain-Driven Federated Learning with Neural Networks in E-Com.pdf:application/pdf},
}

@misc{noauthor_addressing_nodate,
	title = {Addressing {Regulatory} {Requirements} on {Explanations} for {Automated} {Decisions} with {Provenance}—{A} {Case} {Study} {\textbar} {Digital} {Government}: {Research} and {Practice}},
	url = {https://dl.acm.org/doi/abs/10.1145/3436897},
	urldate = {2025-10-31},
}

@article{huynh_addressing_2021,
	title = {Addressing {Regulatory} {Requirements} on {Explanations} for {Automated} {Decisions} with {Provenance}—{A} {Case} {Study}},
	volume = {2},
	url = {https://dl.acm.org/doi/10.1145/3436897},
	doi = {10.1145/3436897},
	abstract = {AI-based automated decisions are increasingly used as part of new services being deployed to the general public. This approach to building services presents significant potential benefits, such as the reduced speed of execution, increased accuracy, lower cost, and ability to adapt to a wide variety of situations. However, equally significant concerns have been raised and are now well documented such as concerns about privacy, fairness, bias, and ethics. On the consumer side, more often than not, the users of those services are provided with no or inadequate explanations for decisions that may impact their lives. In this article, we report the experience of developing a socio-technical approach to constructing explanations for such decisions from their audit trails, or provenance, in an automated manner. The work has been carried out in collaboration with the UK Information Commissioner’s Office. In particular, we have implemented an automated Loan Decision scenario, instrumented its decision pipeline to record provenance, categorized relevant explanations according to their audience and their regulatory purposes, built an explanation-generation prototype, and deployed the whole system in an online demonstrator.},
	number = {2},
	urldate = {2025-10-31},
	journal = {Digit. Gov.: Res. Pract.},
	author = {Huynh, Trung Dong and Tsakalakis, Niko and Helal, Ayah and Stalla-Bourdillon, Sophie and Moreau, Luc},
	month = jan,
	year = {2021},
	pages = {16e:1--16e:14},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\CDFJ2MK9\\Huynh et al. - 2021 - Addressing Regulatory Requirements on Explanations for Automated Decisions with Provenance—A Case St.pdf:application/pdf},
}

@article{kroll_accountable_2017,
	title = {Accountable {Algorithms}},
	volume = {165},
	url = {https://scholarship.law.upenn.edu/penn_law_review/vol165/iss3/3},
	number = {3},
	journal = {University of Pennsylvania Law Review},
	author = {Kroll, Joshua and Huey, Joanna and Barocas, Solon and Felten, Edward and Reidenberg, Joel and Robinson, David and Yu, Harlan},
	month = jan,
	year = {2017},
	pages = {633},
	file = {text/html Attachment:C\:\\Users\\mattia\\Zotero\\storage\\Q3CWBGN6\\3.html:text/html},
}

@article{asvadishirehjini_ensuring_2021,
	title = {Ensuring {Integrity}, {Privacy}, and {Fairness} for {Machine} {Learning} {Using} {Trusted} {Execution} {Environments}},
	url = {https://utd-ir.tdl.org/items/868bed09-9a5b-49eb-800e-a930a22cb04c},
	abstract = {In this day and age, numerous decision-making systems increasingly rely on machine learning (ML) and deep learning to deliver cutting-edge technologies to the members of society. Due to potential security, privacy and bias issues with respect to these ML methods, currently, end users cannot fully trust these systems with their private data, and their prediction outcome. For instance, in many cases, it is not clear how an individual’s medical record is being used for building tools for medical diagnosis? Is the data always encrypted at rest? When they are decrypted, is there a guarantee that only a trusted application can have access to the private data to eliminate potential misuse? Throughout this dissertation, solutions that leverage various security and integrity capabilities provided by hardware assisted Trusted Execution Environments (TEE) are proposed to make these ML based systems more reliable and trustworthy so that end users can have a greater trust in these systems. As a starting point, we first address the privacy and integrity issues in ML model learning in the cloud setting. Training of a deep learning model that only relies on a TEE is not very attractive to businesses that need to continuously train their models in a remote cloud setting. This is due to the fact that special hardware such as Graphical Processing Units (GPU) are much more efficient in training ML models compared to CPU based TEEs. In this dissertation, we propose an integrity-preserving solution that combines TEEs, and GPUs in order to provide an efficient solution. In this solution, we focus on the ML model training task using the efficient GPU while ensuring the detect any deviation from the ML model learning protocol with a high probability using the TEE capabilities. Using our solution, we can ascertain (with high probability) the model is trained with the correct training dataset using the correct training hyperparameters, and correct code execution flow. Once we provide an integrity preserving ML model training solution, we focus on how to use the learned ML model privately and securely in practice. To provide privacy-preserving inference on sensitive data, wherein ML model owner and data owner do not trust each other, the dissertation proposes a solution that the inference task is run inside a TEE and the result is sent to the data owner(s). The most important benefit of our solution is that the data owner can ensure their data will not be used for any other purposes in the future and no information other than the agreed model inference result is disclosed. Furthermore, we show the efficacy of our solution in the context of genomic data analysis. Next we focus on the bias and unfairness embedded in certain ML models. It is has been reported that the ML models can unfairly treat certain subgroups, and it is hard to test for such issues in application deployment settings where both the ML model and the input data to the ML model is sensitive (i.e., both the model and the data cannot be disclosed to public for auditing directly). This dissertation proposes a privacy-preserving solution for fairness analytics using TEEs. In this setting, the model owner and the fairness test set owner do not trust each other, therefore they do not want their input to be disclosed. The end goal is for the fairness analyst to conduct tests about the quality and fairness of the model’s outcome with respect to a set of predefined minority groups or subgroups and compare and contrast them with privileged group(s). This way, models can be analyzed, and the analyst can shed light on the potential latent biases in the ML model in a privacy-preserving manner. Even if the ML model is trained, and deployed securely, due to data poisoning, the final model may still contain hidden backdoors (which in the literature is referred to as trojan attacks). Finally, in this dissertation, we develop novel techniques to detect such attacks. We design experiments that first creates a multitude of models that carry a trojan, and another set that does not have any trojan. Then, we build classifiers to see if we can tell them apart. Our results show that ML models could be used to detect trojan attacks against other ML models.},
	language = {en},
	urldate = {2025-10-31},
	author = {Asvadishirehjini, Aref},
	month = dec,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\mattia\\Zotero\\storage\\64KP8HWZ\\Asvadishirehjini - 2021 - Ensuring Integrity, Privacy, and Fairness for Machine Learning Using Trusted Execution Environments.pdf:application/pdf},
}
