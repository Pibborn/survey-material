"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Responsible AI in the Enterprise: Practical AI risk management for explainable, auditable, and safe models with hyperscalers and Azure OpenAI","A. Masood; H. Dawe; D. E. Adeli",NA; NA; NA,"Responsible AI in the Enterprise: Practical AI risk management for explainable, auditable, and safe models with hyperscalers and Azure OpenAI","","2023","","","","","Build and deploy your AI models successfully by exploring model governance, fairness, bias, and potential pitfalls Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn ethical AI principles, frameworks, and governanceUnderstand the concepts of fairness assessment and bias mitigationIntroduce explainable AI and transparency in your machine learning modelsBook DescriptionResponsible AI in the Enterprise is a comprehensive guide to implementing ethical, transparent, and compliant AI systems in an organization. With a focus on understanding key concepts of machine learning models, this book equips you with techniques and algorithms to tackle complex issues such as bias, fairness, and model governance. Throughout the book, you’ll gain an understanding of FairLearn and InterpretML, along with Google What-If Tool, ML Fairness Gym, IBM AI 360 Fairness tool, and Aequitas. You’ll uncover various aspects of responsible AI, including model interpretability, monitoring and management of model drift, and compliance recommendations. You’ll gain practical insights into using AI governance tools to ensure fairness, bias mitigation, explainability, privacy compliance, and privacy in an enterprise setting. Additionally, you’ll explore interpretability toolkits and fairness measures offered by major cloud AI providers like IBM, Amazon, Google, and Microsoft, while discovering how to use FairLearn for fairness assessment and bias mitigation. You’ll also learn to build explainable models using global and local feature summary, local surrogate model, Shapley values, anchors, and counterfactual explanations. By the end of this book, you’ll be well-equipped with tools and techniques to create transparent and accountable machine learning models.What you will learnUnderstand explainable AI fundamentals, underlying methods, and techniquesExplore model governance, including building explainable, auditable, and interpretable machine learning modelsUse partial dependence plot, global feature summary, individual condition expectation, and feature interactionBuild explainable models with global and local feature summary, and influence functions in practiceDesign and build explainable machine learning pipelines with transparencyDiscover Microsoft FairLearn and marketplace for different open-source explainable AI tools and cloud platformsWho this book is forThis book is for data scientists, machine learning engineers, AI practitioners, IT professionals, business stakeholders, and AI ethicists who are responsible for implementing AI models in their organizations.","","9781803249667","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251167.pdf&bkn=10251167&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Proposing an Interactive Audit Pipeline for Visual Privacy Research","J. DeHart; C. Xu; C. Grant; L. Egede","School of Computer Science, University of Oklahoma, Norman, Oklahoma, USA; School of Computer Science, University of Oklahoma, Norman, Oklahoma, USA; School of Computer Science, University of Oklahoma, Norman, Oklahoma, USA; Human Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","1249","1255","In an ideal world, deployed machine learning models will enhance our society. We hope that those models will provide unbiased and ethical decisions that will benefit everyone. However, this is not always the case; issues arise during the data preparation process throughout the steps leading to the models’ deployment. The continued use of biased datasets and biased processes will adversely damage communities and increase the cost to fix the problem later. In this work, we walk through the decision making process that a researcher should consider before, during, and after a system deployment to understand the broader impacts of their research in the community. Throughout this paper, we discuss fairness, privacy, and ownership issues in the machine learning pipeline, assert the need for a responsible human-over-the-loop methodology to bring accountability into machine learning pipeline, and finally, reflect on the need to explore research agendas that have harmful societal impacts. We examine visual privacy research and draw lessons that can apply broadly to artificial intelligence. Our goal is to provide a systematic analysis of the machine learning pipeline for visual privacy and bias issues. With this pipeline, we hope to raise stakeholder (e.g., researchers, modelers, corporations) awareness as these issues propagate in the various machine learning phases.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671478","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671478","visual privacy;fairness;human-over-the-loop","Training;Visualization;Privacy;Data privacy;Pipelines;Decision making;Machine learning","","1","","64","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Ethical Challenges and Frameworks in AI-Driven Software Development and Testing","A. Donvir; G. Sharma","Application Development, Wayne, NJ, USA; AI Test Automation Solution Architect, Atlanta, GA, USA",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00569","00576","Artificial Intelligence (AI) has revolutionized and transformed the landscape of software development and testing by introducing new efficiencies and capabilities through advancements like Generative AI (GenAI) and Large Language Models (LLMs). While these technologies bring major benefits in terms of productivity, personalization, and innovation, they also raise critical ethical challenges, such as biases, lack of transparency, data privacy concerns, and potential negative societal impacts. This paper examines the ethical considerations involved in developing such advanced AI systems as well using AI systems within software development and testing. It explores existing ethical frameworks and principles provided by leading organizations, emphasizing core concepts like human-centered design, accountability, transparency, fairness, and privacy. Practical strategies for integrating ethical practices throughout the AI development lifecycle are discussed, with a strong emphasis on the need for continuous ethical evaluation. The paper explores the ethical landscape of AI in software development, addressing challenges like algorithmic bias, data security, and broader societal impacts. Real-world case studies presented in the paper demonstrate the consequences of neglecting ethical considerations. Looking forward, the paper suggests future directions, including the development of unified ethical standards, collaborative ethical auditing, regulatory advancements, and higher societal engagement.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903892","Artificial Intelligence (AI);Ethical AI;Software Development;Software Testing;Generative AI (GenAI);Large Language Models (LLMs);Ethical Frameworks;Human-Centered Design;Accountability;Transparency;Fairness and Non-Discrimination;Data Privacy;Responsible AI;Bias Detection;Explainable AI (XAI);Ethical Auditing;Regulatory Frameworks;Societal Engagement;Case Studies in AI Ethics","Ethics;Technological innovation;Data privacy;Standards organizations;Software algorithms;Collaboration;Stakeholders;Artificial intelligence;Software development management;Testing","","1","","26","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Managing Transparency Fairness Accountability in AI for Sustainable Human Resource Management","R. Setiawati; A. Kusmara; T. Kuusk","Management Department, BINUS Business School Undergraduate Program, Bina Nusantara University, Jakarta, Indonesia; Department of Economy, Darma Persada University, Jakarta, Indonesia; Department of Computer Science, Ilearning Incorporation, Barranquilla, Colombia",2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT),"24 Sep 2025","2025","","","1","7","The proliferation of Artificial Intelligence (AI) in Human Resource Management (HRM) offers significant efficiencies but concurrently introduces critical ethical challenges regarding transparency, fairness, and accountability, thereby impacting sustainable workforce management. The novelty of this study lies in its exploration of strategies for effectively managing these ethical dimensions in AI-driven HRM, with a focus on identifying practical solutions to mitigate algorithmic bias and enhance transparency. The research focuses on identifying ethical issues, such as algorithmic bias and lack of transparency in AI-assisted recruitment, performance evaluation, and overall employee management. Adopting a Systematic Literature Review (SLR) methodology, this paper analyzes recent publications (2019-2024) from Scopus to synthesize existing knowledge on AI ethics in HRM. Key findings reveal a significant lack of standardized best practices and audit trails, with opaque AI-driven decisions often undermining trust and fairness. If not properly designed, AI algorithms can perpetuate biases embedded in historical data, leading to discriminatory outcomes, as evidenced by cases at companies like Amazon and HireVue. The study concludes that organizations must proactively implement robust governance frameworks, including ethics training, the development of transparent and fair-by-design algorithms, regular audits, and mechanisms for human oversight. Integrating frameworks such as Fairness, Accountability, and Transparency (FAT) and Ethical AI by Design is crucial for ensuring that AI applications in HRM are ethically sound, legally compliant, and contribute to sustainable and equitable workforce management.","","979-8-3315-9429-9","10.1109/ICCIT65724.2025.11166968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11166968","AI advancements;AI-driven decisions;AI utilization;ethics in HRM;transparent algorithms","Performance evaluation;Training;Ethics;Data privacy;Standards organizations;Decision making;Companies;Artificial intelligence;Recruitment;Systematic literature review","","","","31","IEEE","24 Sep 2025","","","IEEE","IEEE Conferences"
"Publicly Auditable Privacy-Preserving Electoral Rolls","P. Agrawal; M. P. Jhanwar; S. V. Sharma; S. Banerjee","Department of Computer Science and Engineering, Indian Institute of Technology Delhi, New Delhi, India; Department of Computer Science and Center for Digitalisation, AI and Society, Ashoka University, Sonipat, India; Department of Computer Science and Engineering, Indian Institute of Technology Delhi, New Delhi, India; Department of Computer Science and Center for Digitalisation, AI and Society, Ashoka University, Sonipat, India",2024 IEEE 37th Computer Security Foundations Symposium (CSF),"20 Sep 2024","2024","","","217","232","While existing literature on electronic voting has extensively addressed verifiability of voting protocols, the vulnerability of electoral rolls in large public elections remains a critical concern. To ensure integrity of electoral rolls, the current practice is to either make electoral rolls public or share them with the political parties. However, this enables construction of detailed voter profiles and selective targeting and manipulation of voters, thereby undermining the fundamental principle of free and fair elections. In this paper, we study the problem of designing publicly auditable yet privacy-preserving electoral rolls. We first formulate a threat model and provide formal security definitions. We then present a protocol for creation, maintenance and usage of electoral rolls that mitigates the threats. Eligible voters can verify their inclusion, whereas political parties and auditors can statistically audit the electoral roll. Further, the audit can also detect polling-day ballot stuffing and denials to eligible voters by malicious polling officers. The entire electoral roll is never revealed, which prevents any large-scale systematic voter targeting and manipulation.","2374-8303","979-8-3503-6203-9","10.1109/CSF61375.2024.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664400","electronic voting;eligibility;electoral rolls;auditability","Threat modeling;Privacy;Protocols;Systematics;Maintenance;Fraud;Protection","","","","40","IEEE","20 Sep 2024","","","IEEE","IEEE Conferences"
"ML-Driven Audit Risk Assessment with Differential Privacy","Z. Zhang; G. Mohi-Ud-Din; Y. Xiong; X. Zhang; X. L. Lin; C. Ai; H. Min","School of Information Engineering, Nanchang Hangkong University, Nanchang, P.R China; School of Software, Nanchang University, Nanchang, P.R China; School of Information Engineering, Nanchang Hangkong University, Nanchang, P.R China; College of Computer and Information Engineering, Jiangxi Normal University, Nanchang, P.R China; School of Software, Nanchang University, Nanchang, P.R China; School of Software, Nanchang University, Nanchang, P.R China; Network and Information Technology Center, Jiangxi University of Traditional Chinese Medicine, Nanchang, P.R China",2024 11th International Conference on Soft Computing & Machine Intelligence (ISCMI),"28 Jan 2025","2024","","","257","266","In contemporary auditing practices, the ability to effectively identify and assess potential fraudulent companies is crucial for maintaining market fairness and transparency. Previous research has leveraged various machine learning techniques to detect fraudulent activities in financial data. However, these models often lack robustness and fail to protect the privacy of the financial information used for training. Here we introduces a novel approach by incorporating differential privacy into the training process, ensuring data protection without compromising model performance. To enhance model performance, we employed interquartile range filtering for outlier removal and optimized feature selection using Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Principal Component Analysis (PCA). For model training, we employed Logistic Regression, Random Forest, and Decision Trees, with hyperparameters meticulously fine-tuned through grid search and Bayesian optimization techniques to achieve optimal performance. To address privacy concerns, we implemented differential privacy using the Laplace mechanism, effectively safeguarding sensitive financial data. Our empirical results demonstrate that the enhanced models maintain high accuracy and robustness, providing a reliable tool for fraud detection while ensuring financial data privacy.","2640-0146","979-8-3315-1812-7","10.1109/ISCMI63661.2024.10851519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851519","Predictive modeling;Audit risk analysis;GA;PSO;Random Forest;Decision Tree","Training;Differential privacy;Analytical models;Adaptation models;Predictive models;Feature extraction;Robustness;Random forests;Principal component analysis;Genetic algorithms","","","","27","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Deploying Blockchains to Simplify AI Algorithm Auditing","A. Butt; A. Z. Junejo; S. Ghulamani; G. Mahdi; A. Shah; D. Khan","Computer Science, SZABIST, Hyderabad, Pakistan; Computer and Information Sciences, Universiti Teknologi, PETRONAS, Seri Iskandar, Malaysia; Computer Science, SZABIST, Hyderabad, Pakistan; Kimberly Clark Corporation, Roswell, GA, USA; Kulliyah of Information and Communication Technology, IIUM, Malaysia; Computer and Information Sciences, Universiti Teknologi, PETRONAS, Seri Iskandar, Malaysia",2023 IEEE 8th International Conference on Engineering Technologies and Applied Sciences (ICETAS),"18 Dec 2023","2023","","","1","6","Artificial Intelligence has largely occupied various sectors in the world. A huge number of business companies have incorporated several machine learning algorithms for day-to-day decision making. With increasing applications of AI algorithms, the concerns regarding its outcomes have also increased due to bias. In AI algorithms, bias occurs due to multiple reasons including incomplete data, skewed data, human error and so on. These algorithms have the tendency to amplify partially and discrimination in the results instead of benefiting them. This makes it compulsory for the algorithms to be audited. Currently, AI algorithm auditing processes have several challenges including tendency of biases to be deeply ingrained into the system, making these difficult to mitigate; lack of transparency in decision making and many more. This study presents the emerging technology of blockchains to be a viable solution to the existing problem. It comprehensively discusses the suitability of blockchains for transparency in the process of algorithm auditing which is bound to easily capture the issue and the layer consisting it. Consequently, the process of algorithm auditing will be more convenient and more productive. Moreover, this review also discusses some potential challenges that need to be addressed and some future recommendations for this integration.","2769-4518","979-8-3503-2709-0","10.1109/ICETAS59148.2023.10346420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346420","Algorithm Auditing;Algorithmic Bias;Blockchain Networks;Fairness in AI;Digital Transparency","Machine learning algorithms;Soft sensors;Scalability;Decision making;Smart contracts;Standardization;Blockchains","","5","","26","IEEE","18 Dec 2023","","","IEEE","IEEE Conferences"
"Responsible AI for Government Program Evaluation and Performance Audits","D. F. Fonner; F. P. Coyle","SMU DataArts, Southern Methodist University, Dallas, USA; Lyle School of Engineering, Southern Methodist University, Dallas, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","8222","8224","With the proliferation of user-friendly generative-AI tools, many government agencies, such as the National Science Foundation and the National Institutes of Health, have started exploring how these tools could improve one of their primary functions: managing grant programs to distribute funds for research. Government bans on the use of generative-AI for initial grant application evaluation limits the use of this technology. However, this research shows that a post hoc evaluative approach within the domain of Responsible AI provides a means by which government agencies can evaluate the big data collected through their grant programs. This proposed framework, Responsible AI for Evaluation (RAI-E), achieves this goal by curating big governmental administrative data using grant evaluation rubrics followed by model fine-tuning for agency-specific evaluation tasks. Convergence between model- and human-generated evaluations then enables robust diagnosis of grant program equity and fairness via Responsible AI and algorithm auditing methods. Preliminary experiments show improved model performance through the instruction-tuning of open source language models for evaluating government grant applications. Further research will explore the auditing of the underlying algorithms, devising a method for algorithm-in-the-loop frameworks to aid government administrators in evaluating their human-driven program performance.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825518","Responsible AI;Algorithm Auditing;Government Administration;Public Policy","Data privacy;Government;Decision making;Big Data;Data models;National Institutes of Health;Security;Artificial intelligence;Convergence","","","","20","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Responsible Generative AI for Software Development Life Cycle","J. A. Shah; G. Bajpai","Cyber Security Engineering Canada DevOps Community of Pracitce & IEEE YP Toronto, Ontario, Canada; Sofware Development & AI Engineering Canada DevOps Community of Pracitce Ottawa, Ontario, Canada",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0056","0061","Software practitioners are driving a paradigm shift in software engineering practices by integrating generative AI technology into software development and lifecycle management. Integration of generative AI to plan, design, develop, test and maintain software brings productivity gains and enables rapid software releases, however it also presents ethical challenges. This paper examines strategies for developing software through integration of responsible Generative AI that endures, emphasizing primarily the ethical considerations, and the responsible use of Generative technology. It covers the benefits and challenges of collaborative development with responsible Generative AI technologies. The paper focuses on responsible use of generative AI considerations which are likely to induce software integrity and trust. The paper presents best practices, audits, assessments and benchmarking concepts for Gen AI integrated software development and lifecycle management. Subsequently, the paper highlights the importance of safeguarding the integrity of the software development lifecycle through incorporating responsible AI principles, mainly fairness, bias mitigation, privacy and data security, transparency and accountability. Lastly, presenting recommendation for built-in and add-on capabilities for responsible use of GenAI integration into SDLC which paves the way to the trusted ecosystem of GenAI integration for software practitioners.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105309","Artificial Intelligence;Generative AI;Software Development Life Cycle;Software Engineering;Security","Productivity;Ethics;Technological innovation;Generative AI;Reviews;Biological system modeling;Organizations;Software;Software development management;Software engineering","","","","13","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"AP2FL: Auditable Privacy-Preserving Federated Learning Framework for Electronics in Healthcare","A. Yazdinejad; A. Dehghantanha; G. Srivastava","Cyber Science Lab, Canada Cyber Foundry, University of Guelph, Guelph, ON, Canada; Cyber Science Lab, Canada Cyber Foundry, University of Guelph, Guelph, ON, Canada; Department of Mathematics and Computer Science, Brandon University, Brandon, MB, Canada",IEEE Transactions on Consumer Electronics,"29 Apr 2024","2024","70","1","2527","2535","The growing application of machine learning (ML) techniques in healthcare has led to increased interest in federated learning (FL), which enables the secure and private training of robust ML models. However, conventional FL methods often fall short of providing adequate privacy protection and face challenges in handling non-independent and identically distributed (Non-IID) training data. These shortcomings are of significant concern when employing FL in electronic devices in healthcare. To address these issues, we propose an Auditable Privacy-Preserving Federated Learning (AP2FL) model tailored for electronics in healthcare settings. By leveraging Trusted Execution Environments (TEE), AP2FL ensures secure training and aggregation processes on both client and server sides, effectively mitigating data leakage risks. To manage Non-IID data within the proposed framework, we incorporate the Active Personalized Federated Learning (ActPerFL) model and Batch Normalization (BN) techniques to consolidate user updates and identify data similarities. Additionally, we introduce an auditing mechanism in AP2FL that reveals the contribution of each client to the FL process, facilitating the updating of the global model following diverse data types and distributions. In other words, it ensures the FL process’s integrity, transparency, fairness, and robustness. Our results demonstrate that the proposed AP2FL model outperforms existing methods in accuracy and effectively eliminates privacy leakage.","1558-4127","","10.1109/TCE.2023.3318509","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2020-05363); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261257","Privacy;FL;auditing;non-IID;healthcare","Medical services;Data models;Servers;Load modeling;Training;Privacy;Data privacy","","68","","32","IEEE","22 Sep 2023","","","IEEE","IEEE Journals"
"DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-Based Incentive","J. Weng; J. Weng; J. Zhang; M. Li; Y. Zhang; W. Luo","Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China",IEEE Transactions on Dependable and Secure Computing,"27 Aug 2021","2021","18","5","2438","2455","Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this article, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a prototype of DeepChain and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.","1941-0018","","10.1109/TDSC.2019.2952332","National Natural Science Foundation of China(grant numbers:61825203,U1736203,61732021); Guangdong Provincial Special Funds for Applied Technology Research and Development and Transformation of Key Scientific and Technological Achievements(grant numbers:2016B010124009); Science and Technology Program of Guangzhou of China(grant numbers:201802010061); Guangdong Provincial Basic and Applied Research Major Programme(grant numbers:2019B030302008); National Key Research and Development Program of China(grant numbers:2018YFB1402600); National Key Research and Development Program of China(grant numbers:2018YFB1003701); National Natural Science Foundation of China(grant numbers:61972177); Communication and Computer Network Lab of Guangdong(grant numbers:CCNL201903); National Key Research and Development Program of China(grant numbers:2017YFB0802203); Jinan University; National Natural Science Foundation of China(grant numbers:61872153); National Natural Science Foundation of China(grant numbers:61877029); Applied Technology Research and Development and Transformation of Key Scientific and Technological Achievements(grant numbers:2017B010124002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894364","Deep learning;privacy-preserving training;blockchain;incentive","Deep learning;Training;Servers;Blockchain;Collaboration;Training data;Data models","","352","","73","IEEE","8 Nov 2019","","","IEEE","IEEE Journals"
"Using AI and IoT to Promote Intra-Party Democracy for Indian Electoral Reforms: Emerging Issues","S. Kumar; P. Kumar; A. Kumar; M. Sharma; A. Anand; V. R. Malik","College of Law, IIMT University, Meerut, U.P; College of Law, IIMT University, Meerut, U.P; College of Law, IIMT University, Meerut, U.P; College of Law, IIMT University, Meerut, U.P; College of Law, IIMT University, Meerut, U.P; College of Law, IIMT University, Meerut, U.P",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","6","The convergence of artificial intelligence (AI) and the Internet of Things (IoT) opens transformative opportunities for the furtherance of intra-party democracy and electoral reforms in India. This paper delves into the developing problems and issues that are experienced with such convergence. The paper discusses the potential of ICT tools in bringing transparency and accountability into the electoral system as a mitigating way of issues like criminal and money politics. Core issues of the Indian electoral system that very much need to be given serious attention relate to problems like muscle power, money power, criminalization, misuse of government machinery, and casteism. It needs to address these prime importances in reforming the electoral landscape to ensure free and fair elections. It also underscores how electoral reforms can bring changes to the criminal, corrupt, and monetary influence on politics for strengthening democratic values in India. These will involve a critical examination of the electoral system and make the election process more inclusive for representative democracy of the diverse Indian populace.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10725468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10725468","Artificial Intelligence (AI);Internet of Things (IoT);Intra-party democracy;Electoral reforms;Electronic Voting Machines (EVM);Voter Verifiable Paper Audit Trail (VVPAT);Biometric authentication","Privacy;Voting;Machine learning;Muscles;Real-time systems;Internet of Things;Security;Artificial intelligence;Machinery;Convergence","","","","20","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"An Auditing Framework for Analyzing Fairness of Spatial-Temporal Federated Learning Applications","A. Mashhadi; A. Tabaraei; Y. Zhan; R. M. Parizi","Computing and Software Systems, University of Washington, Bothell, WA, USA; Computer and Information Technology Department, Sadjad University of Technology, Mashhad, Iran; Department of Engineering, Imperial College London, London, England; College of Computing and Software Engineering, Kennesaw State University, GA, USA",2022 IEEE World AI IoT Congress (AIIoT),"13 Jul 2022","2022","","","699","707","Federated learning enables remote devices such as smartphones to train statistical models while ensuring that data remains private and secure. Performing privacy-preserving data analysis becomes increasingly crucial as our model is potentially being trained within heterogeneous and massive networks. While federated learning offers the potential to boost diversity in many existing models through on-device learning and enabling a wider range of users to participate, developing fair federated learning models is a challenging task. Throughout this paper, we propose a fairness auditing system for FL models that rely on spatial-temporal data. Borrowing tenets from mobility literature, we propose a set of metrics to define individual fairness using spatial-temporal data. We also introduce a set of approaches for measuring these metrics in distributed settings, as well as building a framework that can monitor the fairness of FL models dynamically.","","978-1-6654-8453-4","10.1109/AIIoT54504.2022.9817283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817283","Federated learning;Fairness;Auditing;Spatial-temporal data","Performance evaluation;Analytical models;Data analysis;Buildings;Collaborative work;Data models;Task analysis","","","","50","IEEE","13 Jul 2022","","","IEEE","IEEE Conferences"
"Ensuring Ethical Use of AI in Project Management","D. Stanton",Bradley University; University of Arkansas; Cranfield University,Project Management with AI For Dummies,"","2025","","","197","211","Summary <p>This chapter explores the key ethical considerations project managers must address when using AI, including understanding AI ethics, promoting fairness and transparency in AI‐driven decisions, and navigating ethical dilemmas in AI‐powered projects. AI has the power to significantly influence decision‐making processes within projects, from resource allocation to hiring decisions. Therefore, it's critical for project managers to ensure that AI is used ethically, with a strong emphasis on fairness, privacy, and accountability. Tools like Fiddler AI help project managers audit AI systems for fairness by continuously monitoring AI‐driven decisions, detecting biased patterns, and providing insights into how AI models reach their conclusions. One effective way to ensure fairness and transparency in AI‐driven decisions is to involve stakeholders in the decision‐making process. Transparency in AI ensures that the decision‐making process is clear and understandable to all stakeholders.</p>","","9781394320868","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10964473.pdf&bkn=10964418&pdfType=chapter","","Artificial intelligence;Ethics;Project management;Stakeholders;Privacy;Guidelines;Decision making;Monitoring;Security;Global Positioning System","","","","","","14 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Ethical Considerations in Artificial Intelligence (AI) Applications in Smart Grid","K. Hayawi; S. Shahriar; A. R. Al-Ali","College of Interdisciplinary Studies, Zayed University, Abu Dhabi, United Arab Emirates; School of Computer Science, University of Guelph, Guelph, Ontario, Canada; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates","2025 7th International Youth Conference on Radio Electronics, Electrical and Power Engineering (REEPE)","30 Apr 2025","2025","","","1","6","The rapid development of artificial intelligence (AI) tools, particularly advanced systems like large language models and creative AI programs, has raised urgent questions about how we should ethically develop and manage these technologies. As the global adoption of smart grid and digital power infrastructure accelerates, the role of AI becomes increasingly significant in optimizing operations like forecasting, maintenance, and energy distribution. Despite several works in the literature relating to ethical AI in domains like medicine and agriculture, a framework and recommendation are missing from the power grid lens. This study introduces a framework of ethical principles tailored to the smart grid context. Through this framework, we identify key methods and strategies, present case studies, and discuss challenges associated with ethical AI implementation in smart grids. Our research reveals a constant tug-of-war: while AI can revolutionize how grids operate; we can't ignore the risks of prioritizing efficiency over fairness. We also discuss significant challenges and outline future research directions for ethical AI use in power grid systems.","2831-7262","979-8-3315-3183-6","10.1109/REEPE63962.2025.10971096","American University of Sharjah; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971096","Ethical AI;smart grids;trustworthy AI;power systems;responsible artificial intelligence","Ethics;Privacy;Refining;Smart grids;Safety;Stakeholders;Resource management;Artificial intelligence;Standards;Testing","","1","","41","IEEE","30 Apr 2025","","","IEEE","IEEE Conferences"
"Ethical AI: Towards Defining a Collective Evaluation Framework","A. K. Sharma; D. Kyosev; J. Kunkel","Mathematics and Computer Science, Georg-August-Universität Göttingen, Göttingen, Germany; Legal Affairs, Alis Grave Nil Private Limited, Burgas, Bulgaria; Mathematics and Computer Science, Georg-August-Universität Göttingen, Göttingen, Germany","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","26 Aug 2025","2025","","","1665","1670","Artificial Intelligence (AI) is transforming sectors such as healthcare, finance, and autonomous systems, offering powerful tools for innovation. Yet its rapid integration raises urgent ethical concerns related to data ownership, privacy, and systemic bias. Issues like opaque decision-making, misleading outputs, and unfair treatment in high-stakes domains underscore the need for transparent and accountable AI systems.This article addresses these challenges by proposing a modular ethical assessment framework built on ontological blocks of meaning—discrete, interpretable units that encode ethical principles such as fairness, accountability, and ownership. By integrating these blocks with FAIR (Findable, Accessible, Interoperable, Reusable) principles, the framework supports scalable, transparent, and legally aligned ethical evaluations, including compliance with the EU AI Act.Using a real-world use case in AI-powered investor profiling, the paper demonstrates how the framework enables dynamic, behavior-informed risk classification. The findings suggest that ontological blocks offer a promising path toward explainable and auditable AI ethics, though challenges remain in automation and probabilistic reasoning.","2836-3795","979-8-3315-7434-5","10.1109/COMPSAC65507.2025.00344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11126684","Responsible AI;Ethical AI;Machine Ethics;AI Acts;Explainability;Ontology;Workflows;Transparency","Ethics;Technological innovation;Machine ethics;Decision making;Finance;Medical services;Ontologies;Probabilistic logic;Software;Artificial intelligence","","","","41","CCBY","26 Aug 2025","","","IEEE","IEEE Conferences"
"Revolutionizing Talent Acquisition in Human Resources with Advanced Deep Learning-Based Resume Screening and Candidate Matching Algorithms","D. G. V; T. Erkinjon; L. H. Alzubaidi; K. Vijayakumar; K. R. Singh; A. K. Rahuman","Department of Mechanical Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences- (SIMATS), Chennai, India; Department of Audit, Tashkent State University of Economics, Tashkent, Republic of Uzbekistan; Department of Computer Techniques Engineering, College of technical engineering, The Islamic University, Najaf, Iraq; Flextronics Technologies India Pvt.Ltd.; Department of Computer Science, Karpagam Academy of Higher Education, Coimbatore, India; Department of Electronics and Communication Engineering, PSNA College of Engineering and Technology, India",2025 International Conference on Automation and Computation (AUTOCOM),"16 Apr 2025","2025","","","1289","1293","In general, this paper outlines a research agenda to transform the approach to talent management in HR using deep learning strategies for enhanced resume analysis and candidate matching. The above work proposed and developed a system that exploits the most advanced tools of natural language processing, and recent advances in neural networks to correctly parse resumes and the candidates to jobs descriptions. The system deals with the issues of unstructured resume data, the efficient extraction of the required data, and assessment of the candidate on his/her fitness in the given company in terms of his/her technical competencies and compatibility with the company's work culture. Substantial real life data analysis presented in the paper proves high efficiency and equity of the applications. This research also incorporates the ethical questions as well as the ways of dealing with data privacy regulations and the biases in the historical data by applying the explainable AI methods. A questionnaire to the users of the system, which are the HR professionals further verifies the real-world application and usability of the system based on concepts such as ease of use, precision and time economy. In conclusion, the study adds knowledge to the process of establishing ethical and optimal AI implementations in the field of HRM and results in the improvement of talent recruitment strategies and practices promoting fair hiring decisions.","","979-8-3315-4237-5","10.1109/AUTOCOM64127.2025.10957186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10957186","Deep learning;resume screening;candidate matching;natural language processing;HR;talent acquisition;neural networks","Deep learning;Surveys;Ethics;Explainable AI;Resumes;Neural networks;Companies;Transforms;Natural language processing;Usability","","","","17","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"Fortifying Federated Learning Towards Trustworthiness via Auditable Data Valuation and Verifiable Client Contribution","K. N. Kumar; R. R. Jha; C. K. Mohan; R. B. Tallamraju","IIT Hyderabad, India; IIT Patna, India; IIT Hyderabad, India; Sahaj AI Software Pvt Ltd., India",2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"13 Aug 2025","2025","","","4999","5009","Ensuring auditability and verifiability in Federated Learning (FL) is both challenging and essential to guarantee that local data remains untampered and client updates are trustworthy. Recent FL frameworks assess client contributions through a trusted central server using various client selection and aggregation techniques. However, reliance on a central server can create a single point of failure, making it vulnerable to privacy-centric attacks and limiting its ability to audit and verify client-side data contributions due to restricted access. In addition, data quality and fairness evaluations are often inadequate, failing to distinguish between high-impact contributions and those from low-quality or poisoned data. To address these challenges, we propose Federated Auditable and Verifiable Data valuation (FAVD), a privacy-preserving method that ensures auditability and verifiability of client contributions through data valuation, independent of any central authority or predefined training algorithm. FAVD utilizes shared local data density functions to construct a global density function, aligning data contributions and facilitating effective valuation prior to local model training. This proactive approach improves transparency in data valuation and ensures that only benign updates are generated, even in the presence of malicious data. Further, to mitigate privacy risks associated with sharing data density functions, we add Gaussian noise to each client’s local density function before sharing it with the server. We theoretically demonstrate the convergence, auditability, and verifiability of FAVD, along with its resilience against data poisoning threats. Our experiments on five diverse benchmarks, including three medical datasets, show that FAVD achieves significant performance gains, accurate data valuation, and fair client contributions under threat, highlighting its reliability as a trustworthy FL approach.","2575-7075","979-8-3315-4364-8","10.1109/CVPR52734.2025.00471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11093960","federated learning;data valuation;auditability;verifiability;data poisoning threat","Training;Data privacy;Federated learning;Gaussian noise;Density functional theory;Servers;Security;Cost accounting;Resilience;Convergence","","","","60","IEEE","13 Aug 2025","","","IEEE","IEEE Conferences"
"Strategies and Consequences of AI-Enhanced Predictive Models for Early Identification of Students at Risk","Rosdiana; E. Sunandar; A. Purnama; A. H. Arribathi; D. A. Yusuf; O. P. M. Daeli","Master of Information Technology, University of Raharja, Tangerang, Indonesia; Master of Information System, University of Raharja, Tangerang, Indonesia; Department of Information Technology, Bina Bangsa University, Tangerang, Indonesia; Master of Management, University of Raharja, Tangerang, Indonesia; Department of Information Technology, Bosindo, Tangerang, Indonesia; Department of Digital Business, UR Mart, Tangerang, Indonesia",2024 3rd International Conference on Creative Communication and Innovative Technology (ICCIT),"8 Oct 2024","2024","","","1","6","This research proposes using predictive models supported by artificial intelligence (AI) to identify potential risks faced by students early. Leveraging AI’s data analysis and machine learning capabilities, the study aims to detect students at risk of academic and social challenges before they escalate. Integrating AI techniques helps uncover complex data patterns often missed by conventional methods. The evaluation and implementation results of this model can guide educational institutions in optimizing more targeted and timely interventions. While AI holds great potential for improving early detection and intervention for at-risk students, its application raises important considerations. The use of student personal data in predictive models requires strict privacy protection and transparency in data usage and analysis. Additionally, potential biases in AI models necessitate a thorough understanding of their development to prevent exacerbating existing disparities or discrimination in the education system. This study underscores the need for comprehensive policy frameworks and clear ethical guidelines when implementing AI in educational settings. By upholding values of inclusion, fairness, and responsibility, AI predictive models can effectively support students’ holistic development.","","979-8-3503-6749-2","10.1109/ICCIT62134.2024.10701122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701122","AI predictive models;Early detection;At-risk students;Artificial Intelligence (AI);Ethical implications","Access control;Privacy;Data protection;Machine learning;Predictive models;Regulation;Encryption;Artificial intelligence;Protection;Information integrity","","3","","40","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Ethical Considerations in the Development and Deployment of Large Language Models","M. A. Khaldy; Y. Gheraibia","Department of Business Intelligence and Data Analytics, University of Petra, Amman, Jordan; School of Computer Science and Informatics, De Montfort University, Leicester, UK",2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA),"2 Jun 2025","2025","","","1","6","Large Language Models (LLMs) have emerged as a powerful tool with the potential to revolutionize various industries and aspects of human life. However, their rapid development and deployment raise significant ethical concerns. This paper delves into the key ethical considerations that must be addressed to ensure the responsible and beneficial use of LLMs. We explore issues such as bias and fairness, privacy and security, transparency and accountability, and the potential for misuse. Addressing bias requires careful data curation, algorithmic fairness techniques, and regular audits. Protecting privacy necessitates strong data privacy and security measures, as well as robust security protocols. To enhance transparency, developing techniques to interpret and explain LLM outputs is crucial. Mitigating the potential for misuse involves developing tools to detect and filter harmful content, implementing responsible AI practices, and fostering a proactive approach to addressing societal implications. By examining these challenges, we aim to foster a thoughtful and ethical approach to LLM development and deployment. A collaborative effort between researchers, developers, policymakers, and the public is essential to ensure that LLMs are used responsibly and ethically.","","979-8-3315-2365-7","10.1109/ICCIAA65327.2025.11013722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11013722","LLM;Misinformation;Societal Impacts;Privacy Concerns;Ethics Integration","Hands;Ethics;Technological innovation;Privacy;Data privacy;Reviews;Large language models;Psychology;Security;Stakeholders","","","","17","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"From Transaction to Transformation: AI and Machine Learning in FinTech","Y. Al Ahmed; A. Osman; A. H. Ahmed; D. Omar Salem; Y. Al Ahmad","Faculty of Engineering, Al Ain University, Abu Dhabi, UAE; Faculty of Engineering, Al Ain University, Abu Dhabi, UAE; Faculty of Engineering, Al Ain University, Abu Dhabi, UAE; School of Business, University of Central Lancashire, Preston, UK; Yarmouck University, Irbid, Jordan",2025 5th Intelligent Cybersecurity Conference (ICSC),"2 Sep 2025","2025","","","187","196","Artificial Intelligence technology and Machine Learning operate in FinTech industries by automating processes while giving complete risk assessments, plus private customized solutions. This research studies how AI and ML technology support modern FinTech business functions, specifically fraud prevention, business credit rating, automated investment advice, and automatic market trades. The research conducts the empirical evaluation of Large Language Model integration within Zero Trust security structures by using adversarial testing together with case studies and explainability assessment methods. Research indicates that LLMs improve threat detection performance by 21% and traditional systems yet they fail in 38% of cases when prompt injection happens. The study highlights the need for adversarial training combined with fairness auditing and regulatory oversight in order to establish AI deployment safety in cybersecurity. The research pairs contemporary studies with specific examples to uncover AI and ML's main advantages of better results, less manual work, and better services for customers. This study researches both data protection risks and official rules while looking at the weaknesses of programmed systems. Through research the paper determines how applications based on AI machine learning propel financial services toward automated systems that adjust their behavior for optimal results. The analysis generates new ideas to develop FinTech innovation models and provides useful directions for business creators plus oversight agencies plus financial establishments. This study brings a new method to view AI/ML integration while showing the need to align ethical and legal rules in future FinTech systems.","","979-8-3503-9292-0","10.1109/ICSC65596.2025.11140522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11140522","FinTech;AI;ML;Fraud Detection;Loan Prediction;Customer Segmentation","Training;Ethics;Fintech;Computational modeling;Threat assessment;Fraud;Zero Trust;Artificial intelligence;Computer security;Business","","","","23","IEEE","2 Sep 2025","","","IEEE","IEEE Conferences"
"Trustworthy AI: A Fuzzy-Multiple Method for Evaluating Ethical Principles in AI Regulations","O. Adamyk; O. Chereshnyuk; B. Adamyk; S. Rylieiev","Loughborough Business School, Loughborough University, Loughborough, United Kingdom; Department of Financial Control and Audit, West Ukrainian National University, Ternopil, Ukraine; Aston Business School, Aston University, Birmingham, United Kingdom; Department of Finance, Accounting and Taxation Chernivtsi Institute of Trade and Economics of the State University of Trade and Economics, Chernivtsi, Ukraine",2023 13th International Conference on Advanced Computer Information Technologies (ACIT),"17 Oct 2023","2023","","","608","613","In this study, we investigated the ethical principles of trustworthy AI and differentiated five prime factors essential for developing trust in AI and most widely presented in regulatory guidelines worldwide. By utilizing Fuzzy Logic Toolbox in MATLAB 9.4, we evaluated the impact of primary ethical principles on trustworthy AI systems in a systematic and structured manner. We discovered that the principle of Fairness and Non-discrimination is the most influential for the development of trustworthy AI, as it is the most represented in the regulatory guidelines. The proposed model offers two main benefits for developers and deployers of AI systems, including predicting the potential public trust in AI systems and assessment compliance with the regulatory frameworks. To ensure the continued trustworthiness of AI systems, the model should be used at all stages of the software life circle, including during development, before placing the system on the market, and at the stage of use to monitor compliance with the safeguards declared to users.","2770-5226","979-8-3503-1167-9","10.1109/ACIT58437.2023.10275505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275505","artificial intelligence;regulation;trustworthy AI;ethical principles of AI;trust","Ethics;Systematics;Profitability;Government;Predictive models;Mathematical models;Regulation","","1","","16","IEEE","17 Oct 2023","","","IEEE","IEEE Conferences"
"Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective","X. Luo; Y. Jiang; F. Wei; Y. Wu; X. Xiao; B. C. Ooi","School of Computing, National University of Singapore, Cluny Road, Singapore; School of Computing, National University of Singapore, Cluny Road, Singapore; Alibaba Group, Hangzhou, China; School of Computing, National University of Singapore, Cluny Road, Singapore; School of Computing, National University of Singapore, Cluny Road, Singapore; School of Computing, National University of Singapore, Cluny Road, Singapore",IEEE Transactions on Information Forensics and Security,"12 Sep 2024","2024","19","","8109","8124","Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined. In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execute fairness poisoning attacks to undermine the receiver’s downstream models by manipulating the training data distribution of the diffusion model. Meanwhile, the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer’s dataset. Our experiments conducted on real-world datasets demonstrate remarkable attack performance on different types of diffusion models, which highlights the critical importance of robust data auditing and privacy protection protocols in pertinent applications.","1556-6021","","10.1109/TIFS.2024.3453555","National Research Foundation, Singapore, under its AI Singapore Program (AISG)(grant numbers:AISG3-RP-2022-029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663440","Diffusion model;property inference;fairness poisoning","Diffusion models;Data models;Receivers;Accuracy;Training;Noise;Mathematical models","","4","","67","IEEE","2 Sep 2024","","","IEEE","IEEE Journals"
"Ethical Decision‐Making in GenAI Cybersecurity","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","225","254","Summary <p>Chapter 9 explores the ethical decision‐making processes essential for integrating generative artificial intelligence (GenAI) into cybersecurity. It addresses the ethical challenges that arise at the intersection of advanced artificial intelligence (AI) technologies and cybersecurity, emphasizing privacy, data integrity, and security. The chapter outlines specific ethical dilemmas, provides practical approaches for ethical decision‐making, and highlights key principles and frameworks to ensure responsible AI deployment. Through case studies, it explores real‐world applications and the complexities involved in balancing technological advancements with ethical imperatives, ultimately advocating for a collaborative approach to foster trust and uphold societal values.</p>","","9781394279319","10.1002/9781394279326.ch9","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897094.pdf&bkn=10896969&pdfType=chapter","","Ethics;Computer security;Privacy;Law;Government;Standards organizations;General Data Protection Regulation;Cyber espionage;Transforms;Testing","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Cybersecurity: Understanding the Digital Fortress","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","17","46","Summary <p>Cybersecurity is vital in safeguarding computers, networks, programs, and data against unauthorized access and damage. As digital systems become increasingly integral to modern life, cybersecurity ensures the protection of sensitive information and maintains operational continuity. This chapter explores the fundamental types of cybersecurity, including network security, application security, information security, and operational security. It dives into the significant financial impact of cybercrime globally and regionally, highlighting the unique challenges faced by different industries such as financial services, health care, government, e‐commerce, and critical infrastructure. The chapter also examines the role of artificial intelligence (AI) and generative AI (GenAI) in enhancing cybersecurity measures, addressing ethical considerations, and navigating the global regulatory landscape to fortify digital defenses.</p>","","9781394279319","10.1002/9781394279326.ch2","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897112.pdf&bkn=10896969&pdfType=chapter","","Security;Firewalls (computing);Computer crime;Training;Structured Query Language;Encryption;Regulation;Personnel;Network security;NIST","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Accountability for GenAI for Cybersecurity","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","203","224","Summary <p>Chapter 8 examines accountability in the realm of generative artificial intelligence (GenAI) within cybersecurity, emphasizing the need for a clear delineation of responsibility for artificial intelligence (AI) system actions, decisions, and outcomes. The chapter explores the importance of human oversight, legal implications, and the challenges of assigning liability in AI‐driven environments. It discusses the role of governance structures, ethical guidelines, and regulatory frameworks in ensuring accountability. Through case studies and emerging technologies like blockchain and federated learning, the chapter provides insights into balancing innovation with accountability. The conclusion highlights the collective responsibility of stakeholders to advance GenAI technologies ethically and securely.</p>","","9781394279319","10.1002/9781394279326.ch8","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897119.pdf&bkn=10896969&pdfType=chapter","","Ethics;Artificial intelligence;Computer security;Decision making;Stakeholders;Data breach;Computer crime;Companies;Biological system modeling;Training","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Ethical Design and Development","M. R. Islam","Geroge Mason Univeristy, Fairfax, Virginia, United States","Generative AI, Cybersecurity, and Ethics","","2025","","","163","178","Summary <p>As generative artificial intelligence (GenAI) increasingly integrates into various aspects of our lives, its ethical design and development become crucial. This chapter underscores the necessity of embedding ethical considerations in the GenAI development process through stakeholder engagement, transparency, and continuous monitoring. It highlights the roles of developers, ethicists, legal experts, and users in creating fair and accountable GenAI systems. Emphasis is placed on addressing biases, ensuring robustness and security, adhering to regulatory compliance, and fostering human‐centric design. Through interdisciplinary collaboration and comprehensive ethical training, the chapter outlines strategies to align GenAI technologies with societal values and ethical standards.</p>","","9781394279319","10.1002/9781394279326.ch6","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10897175.pdf&bkn=10896969&pdfType=chapter","","Ethics;Artificial intelligence;Training;Stakeholders;Prevention and mitigation;Predictive models;Robustness;Guidelines;Documentation;Decision making","","","","","","20 Feb 2025","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Blockchain and NFTs for Trusted Ownership, Trading, and Access of AI Models","A. Battah; M. Madine; I. Yaqoob; K. Salah; H. R. Hasan; R. Jayaraman","Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Industrial and Systems Engineering, Khalifa University, Abu Dhabi, United Arab Emirates",IEEE Access,"28 Oct 2022","2022","10","","112230","112249","The demand for high-quality Artificial Intelligence (AI) models is ever-increasing in this digital era. However, most of the existing methods leveraged for managing the ownership, trading, and access of AI models fall short of providing traceability, transparency, audit, security, and trustful features. In this paper, we propose a solution based on blockchain and Non-fungible Tokens (NFTs) to manage ownership rights and exchange of AI models in a transparent, traceable, auditable, secure, and trustworthy manner. Smart contracts are employed to enforce ownership, ease of access, and exchange policies for the unique NFT linked to an AI model. We use decentralized storage of the InterPlanetary File System (IPFS) and proxy re-encryption oracles to securely fetch, store, and share data related to AI models. We present algorithms along with their implementation, testing, and validation details. The proposed solution is evaluated using cost and security analyses to show its affordability and resiliency against security threats and attacks. All smart contract codes are made publicly available on GitHub.","2169-3536","","10.1109/ACCESS.2022.3215660","Khalifa University of Science and Technology(grant numbers:RCII-2019-002–Center for Digital Supply Chain and Operations Management,CIRA-2019-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924181","Blockchain;decentralized storage;non-fungible tokens (NFTs);oracles;provenance;proxy re-encryption;smart contracts","Artificial intelligence;Data models;Collaboration;Security;Computational modeling;Smart contracts;Data privacy;Blockchains;Nonfungible tokens","","33","","54","CCBY","19 Oct 2022","","","IEEE","IEEE Journals"
"Accuracy and Fairness in Pupil Detection Algorithm","O. N. Kulkarni; V. Patil; V. K. Singh; P. K. Atrey","Albany Lab for Privacy and Security, College of Engineering and Applied Sciences University at Albany, State University of New York, Albany, USA; Albany Lab for Privacy and Security, College of Engineering and Applied Sciences University at Albany, State University of New York, Albany, USA; Rutgers University, USA; Albany Lab for Privacy and Security, College of Engineering and Applied Sciences University at Albany, State University of New York, Albany, USA",2021 IEEE Seventh International Conference on Multimedia Big Data (BigMM),"17 Dec 2021","2021","","","17","24","Despite being highly accurate, widely used multimedia analysis algorithms can suffer from bias, affecting users’ trust in them. For instance, algorithms for face recognition, pedestrian detection, and image search have recently been reported to be biased. In this paper, we move the discussion on algorithmic fairness to a new domain, namely, pupil detection. We audit a widely used OpenCV algorithm for pupil detection from the perspective of fairness and accuracy. The algorithm is audited using two different datasets: a single-person image dataset (CelebA), and a group image dataset (Images of Groups). In both datasets, we found the OpenCV pupil detection algorithm to provide reasonably high accuracy but also yield statistically significant bias with respect to gender. The results provide the first empirical evidence for the existence of gender bias in pupil detection algorithms in both single person as well as group image settings. These results could have a deleterious impact on downstream multimedia applications such as identity authentication, attention assessment, and e-learning. Finally, we discuss the process of appropriately choosing the pupil detection algorithm parameters that may help reduce bias while maintaining high accuracy in various multimedia applications.","","978-1-6654-3414-0","10.1109/BigMM52142.2021.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643325","Algorithms;Accuracy;Algorithmic Fairness;Face Detection;Gaze Tracking;Computer Vision;Pupil Detection;Machine Learning;Statistical Analysis","Electronic learning;Face recognition;Conferences;Employment;Education;Authentication;Big Data","","2","","51","IEEE","17 Dec 2021","","","IEEE","IEEE Conferences"
"Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts","S. Sankaran","Research and Innovation Group, Tata Consultancy Services, Chennai, India","2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)","24 Sep 2025","2025","","","1","9","As artificial intelligence (AI) continues to reshape economies and societies, building trust in these systems—by addressing bias, opacity, and accountability—remains a global challenge. ISO standards such as ISO/IEC 24027 and 24368 aim to embed fairness, explainability, and risk control into AI development. However, their effectiveness varies across legal and policy landscapes, including the EU’s risk-tiered AI Act, China’s focus on social stability, and the U.S.’s decentralized regulatory model. This study introduces a Comparative Risk-Impact Assessment Framework to evaluate how well ISO standards mitigate ethical AI risks across these diverse environments and offers recommendations to enhance their global relevance. By aligning ISO provisions with the EU AI Act and analyzing AI governance in twelve jurisdictions—including the UK, Canada, India, Japan, Singapore, South Korea, Brazil, and South Africa— we establish a comparative baseline for ethical alignment. Case studies from the EU, Colorado, and China reveal key shortcomings: ISO compliance often lacks enforceability (e.g., Colorado) and fails to accommodate local values, such as China’s emphasis on privacy and data sovereignty. To address these issues, we recommend mandatory ethical risk audits, region-specific annexes to ISO standards, and an integrated privacy-risk module. Our framework offers a scalable method for harmonizing AI governance with ethical standards, synthesizing global regulatory trends while allowing for local adaptation. These insights support regulators and standards bodies in refining ISO’s role in global AI oversight, enabling more consistent and context-sensitive deployment of trustworthy AI systems worldwide.","","979-8-3315-3562-9","10.1109/ACDSA65407.2025.11166403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11166403","AI Standardization;Trustworthy AI;Regulatory frameworks;Risk assessment;Ethical AI","Ethics;Privacy;Systematics;Regulators;ISO Standards;ISO;Stability criteria;Refining;Risk management;Artificial intelligence","","","","60","IEEE","24 Sep 2025","","","IEEE","IEEE Conferences"
"Embracing the Crowd: Robust Federated Learning for Edge Intelligence","D. Yu; Z. Xie; X. Zhang; Y. Yuan; Y. Zou; X. Cheng","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Software & Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China",2024 International Conference on Meta Computing (ICMC),"9 Jul 2025","2024","","","259","266","With the rapid growth of edge intelligence, federated learning technologies have received a great deal of attention. Compared with traditional centralized machine learning based on cloud computing, federated learning in edge environment uses mobile edge devices to cooperatively train machine learning models. However, computational ability and transmission conditions are also varied among FL participants. Thus, robustness is one of the key issues for FL in edge scenarios. This article aims to focus on robust federated learning from the following four aspects: resilience to data variability and distribution, fault tolerance, preserving training performance against various attacks and proactive audit function. In this article, we provide an overview of the development history of federated learning for edge intelligence, and we gradually introduce it from three aspects: distributed machine learning, federated learning, and distributed federated learning. Then, in view of the threat problem of robust federated learning, relevant work is studied from three aspects: noisy data, noisy communication, and targeted and untargeted attacks. Then, relevant research is conducted on the building robust and reliable FL on the edge, focusing on three aspects: robust estimation, benefits against targeted and untargeted attacks, and cryptographic audits. Finally, we discuss the future directions and open problems for federated learning.","","979-8-3503-5599-4","10.1109/ICMC60390.2024.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11062771","Federated learning;Edge intelligence;Robustness","Training;Privacy;Federated learning;Smart healthcare;Fault tolerant systems;Focusing;Robustness;Noise measurement;History;Resilience","","","","54","IEEE","9 Jul 2025","","","IEEE","IEEE Conferences"
"Is My Data in Your AI? Membership Inference Test (MINT) Applied to Face Biometrics","D. DeAlcala; A. Morales; J. Fierrez; G. Mancera; R. Tolosana; J. Ortega-Garcia","Universidad Autónoma de Madrid, Madrid, Spain; Universidad Autónoma de Madrid, Madrid, Spain; Universidad Autónoma de Madrid, Madrid, Spain; Universidad Autónoma de Madrid, Madrid, Spain; Universidad Autónoma de Madrid, Madrid, Spain; Universidad Autónoma de Madrid, Madrid, Spain",IEEE Access,"23 Sep 2025","2025","13","","163805","163819","This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when an Audited Model is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforce privacy and fairness in several AI applications, e.g., revealing if sensitive or private data was used for training or tuning Large Language Models (LLMs).","2169-3536","","10.1109/ACCESS.2025.3608951","Project BBforTAI through MICINN/FEDER(grant numbers:PID2021-127641OB-I00); Project HumanCAIC through MICINN(grant numbers:TED2021-131787B-I00); Project M2RAI through MICIU/FEDER(grant numbers:PID2024-160053OB-I00); Project Cátedra ENIA UAM-VERIDAS (NextGenerationEU PRTR)(grant numbers:TSI-100927-2023-2); ELLIS Unit Madrid; FPU Fellowship from Spanish MIU(grant numbers:FPU21/05785); Madrid Government (Comunidad de Madrid-Spain) through the Multiannual Agreement with Universidad Autónoma de Madrid in the line of Excellence for the University Teaching Staff in the context of V PRICIT (Regional Program of Research and Technological Innovation); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11159206","Audit;fairness;face recognition;membership inference;MIA;MINT;reliability","Artificial intelligence;Training;Data models;Face recognition;Training data;Databases;Vectors;Regulation;Glass box;Data privacy","","1","","93","CCBYNCND","11 Sep 2025","","","IEEE","IEEE Journals"
"Risks to Financial Inclusion by Anti-Money Laundering and Financial Counterterrorism Algorithms","K. Grillaert; J. Scarpino","Ethoscope, Managing Partner, Milwaukee, Wisconsin; TrustEngine, VP Information Security Assessed Intelligence, CEO, Canton, Ohio","2025 IEEE International Symposium on Ethics in Engineering, Science, and Technology (ETHICS)","4 Aug 2025","2025","","","1","5","This paper examines the risks that anti-money laundering and countering the financing of terrorism algorithms pose to financial inclusion. Artificial intelligence can enhance the detection of illicit financial activity, yet it may also perpetuate existing biases, leading to the disproportionate exclusion of dis-advantaged groups. Federated learning expands dataset diversity and preserves privacy, but it also introduces challenges related to transparency, fairness, and bias in AI -driven financial compliance systems, requiring mitigation strategies aligned with financial inclusion.","2996-3648","979-8-3315-3228-4","10.1109/ETHICS65148.2025.11098437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11098437","artificial intelligence;financial services;feder-ated learning;ethics;bias;auditing","Ethics;Data privacy;Systematics;Federated learning;Terrorism;Prevention and mitigation;Current measurement;Transforms;Learning (artificial intelligence);Financial services","","","","29","IEEE","4 Aug 2025","","","IEEE","IEEE Conferences"
"AI Algorithms for Personalized Reproductive Health Treatment Plans","L. Anggraini; V. Wildan; F. N. Azizah; R. Supriati; M. Madani; R. F. Terizla","Dept. of Midwifery, University of Yarsi Pratama, Tangerang, Indonesia; Dept. of Midwifery, University of Yarsi Pratama, Tangerang, Indonesia; Dept. of Midwifery, University of Yarsi Pratama, Tangerang, Indonesia; Faculty of Informatics Engineering, University of Raharja, Tangerang, Indonesia; Faculty of Computer System, University of Raharja, Tangerang, Indonesia; Informatics Engineering, Rey Incorporation, San Diego, USA",2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT),"24 Sep 2025","2025","","","1","6","The integration of Artificial Intelligence (AI) in reproductive healthcare has ushered in a new era of personalized medicine, offering more accurate and tailored treatment strategies for conditions such as infertility, Polycystic Ovary Syndrome (PCOS), and pregnancy related complications. This study investigates the role of AI algorithms in creating personalized treatment plans aimed at improving reproductive health outcomes. Using a quantitative approach, data from 150 patients across three fertility clinics were analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM). The study focused on four key variables Data Quality, Algorithm Sophistication, Treatment Success, and Patient Satisfaction. The findings reveal that high-quality data and sophisticated AI algorithms significantly influence treatment success and patient satisfaction, with AI-driven approaches yielding superior outcomes compared to traditional methods. Specifically, advanced AI models like deep learning outperformed simpler algorithms, resulting in higher success rates. Ethical considerations, including mitigating algorithmic bias and ensuring data privacy, were addressed through stratified sampling and fairness audits. The results underscore the importance of leveraging high quality data and advanced AI models to enhance reproductive health treatments. This work provides actionable insights for healthcare providers, emphasizing the need for improved data management and AI integration to optimize patient outcomes. Future research should explore the long-term impacts of AI in reproductive healthcare and address challenges related to model generalization and patient engagement.","","979-8-3315-9429-9","10.1109/ICCIT65724.2025.11166786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11166786","Augmented Reality;Digital Banking;User Engagement;Usability;Structural Equation Modeling","Deep learning;Ethics;Data privacy;Data integrity;Infertility;Medical services;Prediction algorithms;Mathematical models;Data models;Artificial intelligence","","","","50","IEEE","24 Sep 2025","","","IEEE","IEEE Conferences"
"gMINT: Gradiant-based Membership Inference Test Applied to Image Models","D. DeAlcala; A. Morales; J. Fierrez; G. Mancera; R. Tolosana","Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain; Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain; Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain; Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain; Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain",2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"15 Sep 2025","2025","","","2772","2781","Membership Inference Test (MINT) is a method designed to determine whether specific data samples were used during model training. This paper explores the use of gradient based information for MINT (gMINT). In particular, we take advantage of the gradient updates applied to each weight during the learning process, which we refer to as Weight Modifiers. We systematically analyze different types of Weight Modifiers and propose various approaches to process them effectively. Our experiments are carried out on multiple state-of-the-art architectures and four benchmark datasets, demonstrating that our method achieves near perfect detection (100% in most scenarios). These results highlight the effectiveness of Weight Modifiers as a key cue for Membership Inference, further advancing the field of model auditing and privacy assessment in AI systems.","2160-7516","979-8-3315-9994-2","10.1109/CVPRW67362.2025.00261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11147874","studies of xai for cv and related topics (e.g.;fairness;transparency;interpretability;trust);applications of xai for cv","Training;Data privacy;Computer vision;Conferences;Computational modeling;Training data;Computer architecture;Data models;Pattern recognition;Artificial intelligence","","","","49","IEEE","15 Sep 2025","","","IEEE","IEEE Conferences"
"IEEE Standard for the Procurement of Artificial Intelligence and Automated Decision Systems","",,IEEE Std 3119-2025,"22 May 2025","2025","","","1","177","The standard helps procurement teams reduce risks in artificial intelligence systems (AIS) by using tailored risk management practices when purchasing AIS. Specific process steps for AIS problem definition, solicitation preparation, vendor and solution evaluation, contract negotiation, and contract monitoring are described. Risk management methodologies that augment customary activities and tasks performed during the procurement life cycle are provided including identifying, analyzing, evaluating, prioritizing, mitigating, and controlling unique AIS risks that can detract from unique AIS benefits. How procurement teams can use and apply practical AIS tools and metrics is also provided. The standard focuses explicitly on AIS risks (when compared to traditional, non-AIS technology) and is designed to address the purchase of commercial AI products and services procured using a formal contract or contract framework.","","979-8-8557-2201-7","10.1109/IEEESTD.2025.11011522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011522","ADS;AI;AIS;AI performance;AI solution evaluation;AI systems;AI vendor evaluation;artificial intelligence;acquisition;automated decision systems;commercial AI products and services;contract monitoring;contract negotiation;data;data governance;due diligence;high-risk;human rights;IEEE 3119;impact assessment;metrics;procurement;public engagement;public interest;responsible AI;responsible procurement;risk assessment;risk identification;risk management;risk mitigation;sociotechnical;solicitation;systems;technology","IEEE Standards;Artificial intelligence;Data governance;Social implications of technology;Human factors;Decision making;Automation;Procurement;Contracts;Commercialization","","","","84","","22 May 2025","","","IEEE","IEEE Standards"
"Benchmark Dataset Dynamics, Bias and Privacy Challenges in Voice Biometrics Research","C. Rusti; A. Leschanowsky; C. Quinlan; M. Pnacekova; L. Gorce; W. T. Hutiri","University of Southern California, United States; Fraunhofer IIS, Germany; University of Toronto, Canada; York University, Canada; Open North, Canada; Delft University of Technology, The Netherlands",2023 IEEE International Joint Conference on Biometrics (IJCB),"1 Mar 2024","2023","","","1","10","Speaker recognition is a widely used voice-based biometric technology with applications in various industries, including banking, education, recruitment, immigration, law enforcement, healthcare, and well-being. However, while dataset evaluations and audits have improved data practices in face recognition and other computer vision tasks, the data practices in speaker recognition have gone largely unquestioned. Our research aims to address this gap by exploring how dataset usage has evolved over time and what implications this has on bias, fairness and privacy in speaker recognition systems. Previous studies have demonstrated the presence of historical, representation, and measurement biases in popular speaker recognition benchmarks. In this paper, we present a longitudinal study of speaker recognition datasets used for training and evaluation from 2012 to 2021. We survey close to 700 papers to investigate community adoption of datasets and changes in usage over a crucial time period where speaker recognition approaches transitioned to the widespread adoption of deep neural networks. Our study identifies the most commonly used datasets in the field, examines their usage patterns, and assesses their attributes that affect bias, fairness, and other ethical concerns. Our findings suggest areas for further research on the ethics and fairness of speaker recognition technology.","2474-9699","979-8-3503-3726-6","10.1109/IJCB57857.2023.10449225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449225","","Training;Surveys;Ethics;Data privacy;Benchmark testing;Speaker recognition;Task analysis","","1","","44","IEEE","1 Mar 2024","","","IEEE","IEEE Conferences"
"Auditing saliency cropping algorithms","A. Birhane; V. U. Prabhu; J. Whaley",University College Dublin & Lero; UnifyID Labs; UnifyID Labs,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"15 Feb 2022","2022","","","1515","1523","In this paper, we audit saliency cropping algorithms used by Twitter, Google and Apple to investigate issues pertaining to the male-gaze cropping phenomenon as well as race-gender biases that emerge in post-cropping survival ratios of face-images constituting 3 × 1 grid images. In doing so, we present the first formal empirical study which suggests that the worry of a male-gaze-like image cropping phenomenon on Twitter is not at all far-fetched and it does occur with worryingly high prevalence rates in real-world full-body single-female-subject images shot with logo-littered backdrops. We uncover that while all three saliency cropping frameworks considered in this paper do exhibit acute racial and gender biases, Twitter’s saliency cropping framework uniquely elicits high male-gaze cropping prevalence rates. In order to facilitate reproducing the results presented here, we are open-sourcing both the code and the datasets that we curated at shorturl.at/iuzK9. We hope the computer vision community and saliency cropping researchers will build on the results presented here and extend these investigations to similar frameworks deployed in the real world by other companies such as Microsoft and Facebook.","2642-9381","978-1-6654-0915-5","10.1109/WACV51458.2022.00158","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706880","Explainable AI;Fairness;Accountability;Privacy and Ethics in Vision Datasets;Evaluation and Comparison of Vision Algorithms;Deep Learning;Human-Computer Interaction;Segmentation;Grouping and Shape","Computer vision;Codes;Social networking (online);Blogs;Companies;Internet","","7","","27","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"Blockchain-Based Privacy-Preserving and Sustainable Data Query Service Over 5G-VANETs","L. -Y. Yeh; N. -X. Shen; R. -H. Hwang","Department of Information Management, National Central University, Taoyuan, Taiwan; Department of Computer Science Information Engineering, National Chung Cheng University, Chiayi, Taiwan; Department of Computer Science Information Engineering, National Chung Cheng University, Chiayi, Taiwan",IEEE Transactions on Intelligent Transportation Systems,"14 Sep 2022","2022","23","9","15909","15921","Intelligent Transport Systems (ITSs) play an important role in future smart city design to improve traffic safety and traffic congestion by sharing data collected by vehicles. For sharing the traffic data with other vehicles, the vehicular sensory data are usually uploaded to the cloud server. However, existing data sharing systems for VANETs cannot provide selective data with sufficient privacy protection. Moreover, some schemes also cannot ensure stable data accessibility and the integrity of retrieved data. On the other hand, with the improvements such as lower latency, higher capacity, and increased bandwidth, 5G technology brings more possibilities to future applications. The join of the software-defined networks (SDNs) also offers efficient and effective network management. This paper proposes a primitive vehicular communication system named blockchain-based privacy-preserving and sustainable data query service. The proposed scheme is designed to realize stable data accessibility by leveraging smart contracts and blockchain oracle. With the help of 5G technology and P2P file-sharing system, InterPlanetary File System (IPFS), the proposed scheme aims to support video downloading files with searchable capability and fairness. An incentive token mechanism is also equipped. The merit of auditability is ensured by Ethereum blockchain platform to support the accountability. Besides, we also evaluate its networking performance via SUMO and NS-3 simulators. Our simulation results show that the request-response delay of BPSDQS is less than existing blockchain-based proxy re-encryption (PRE) scheme. Our simulation results also showed that the average request-response delay in our scheme can saving up to 98%.","1558-0016","","10.1109/TITS.2022.3146322","Ministry of Science and Technology of Taiwan, R.O.C.(grant numbers:MOST 108-2221-E-194-019-MY3,MOST 108-2221-E-008-100-MY3,MOST 110-2218-E415-001-MBK); Advanced Institute of Manufacturing with High-tech Innovations (AIM- HI) from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project; Ministry of Education (MOE) in Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9703241","Vehicular ad-hoc network (VANET);blockchain;software defined network (SDN);proxy re-encryption;InterPlanetary File System (IPFS)","Blockchains;Streaming media;5G mobile communication;Cryptography;Smart contracts;Servers;Encryption","","42","","33","IEEE","3 Feb 2022","","","IEEE","IEEE Journals"
"Estimating Example Difficulty using Variance of Gradients","C. Agarwal; D. D'souza; S. Hooker","MDSR Lab, Adobe; ML Collective; Google Research",2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"27 Sep 2022","2022","","","10358","10368","In machine learning, a question of great interest is understanding what examples are challenging for a model to classify. Identifying atypical examples ensures the safe de-ployment of models, isolates samples that require further human inspection and provides interpretability into model behavior. In this work, we propose Variance of Gradients (VoG) as a valuable and efficient metric to rank data by difficulty and to surface a tractable subset of the most chal-lenging examples for human-in-the-loop auditing. We show that data points with high VoG scores are far more difficult for the model to learn and over-index on corrupted or mem-orized examples. Further, restricting the evaluation to the test set instances with the lowest VoG improves the model's generalization performance. Finally, we show that VoG is a valuable and efficient ranking for out-of-distribution detection.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879352","Transparency;fairness;accountability;privacy and ethics in vision","Measurement;Ethics;Computer vision;Computational modeling;Machine learning;Inspection;Human in the loop","","37","","73","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"The Role of AI in Healthcare Policy Development and Management","S. Yadav; P. Mane; V. K. Swarnkar; S. k. Rawandale; A. S. Patil; K. S. Katke; N. Bhat","Entrepreneurship Mindset Curriculum, State Council of Educational Research and Training, Delhi; Bharati Vidyapeeth (Deemed to be University), Institute of Management and Entrepreneurship Development, Pune; Department of Computer Science & Engineering, Bharti Vishwavidyalaya, Durg, Chhattisgarh; Dean Industry Institute Interaction, PCET's Pimpri Chinchwad College of Engineering, Pune, Maharashtra, India; E&TC department, JSPM NTC, Pune; Department of Computer Engineering, Sanjivani College of engineering Kopergaon Ahmednagar; Software Developer, Master of Science, california state university long beach, CA, USA",2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),"15 Apr 2024","2023","1","","1","6","This study looks at how artificial intelligence (AI) is incorporated into healthcare policy, with an emphasis on how it affects administrative procedures, treatment optimization, especially clinical choice support. Using a deductive methodology and interpretive thinking as a philosophy, an exploratory design with secondary data collecting was used. In the present-day environment, AI is being used more frequently in healthcare legislation, especially within medical settings. Clinical decision-making systems driven by AI have shown to offer a great promise for lowering diagnostic mistakes and increasing treatment precision. Improving patient outcomes through individualized AI-driven treatments for treatment improvement appears promising. But there are issues with algorithm prejudices as well as information privacy, among other ethical and legal issues. Establishing thorough guidelines, ongoing education, and auditing procedures are among the suggestions that are made. In order to guarantee fair and efficient integration of health care legislation, future research should prioritize longitudinal research and improving artificial intelligence algorithms","","979-8-3503-3091-5","10.1109/ICAIIHI57871.2023.10489810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489810","Artificial Intelligence;Healthcare Policy;Clinical Decision Support;Treatment Optimization;Ethical Considerations","Industries;Technological innovation;Privacy;Philosophical considerations;Law;Legislation;Medical services","","1","","30","IEEE","15 Apr 2024","","","IEEE","IEEE Conferences"
"Blockchain-Based Anti-Key-Leakage Key Aggregation Searchable Encryption for IoT","J. Niu; X. Li; J. Gao; Y. Han","School of Mathematics and Statistics, Xidian University, Xi’an, China; School of Mathematics and Statistics, Xidian University, Xi’an, China; School of Telecommunication and Engineering, Xidian University, Xi’an, China; School of Mathematics and Statistics, Xidian University, Xi’an, China",IEEE Internet of Things Journal,"11 Feb 2020","2020","7","2","1502","1518","The Internet of Things (IoT) makes our life more intelligent. Its combination with the cloud server can solve big data processing problems to meet users' needs and bring us great convenience. However, there are two challenges we need to face: data sharing and key leakage. To solve the above challenges, attribute-based encryption (ABE) is used to achieve data sharing combined with searchable encryption (SE). Most of the existing attribute-based searchable encryption (SE) schemes are inefficient and not suitable for IoT devices because of the large amount of attributes and keys. The key-leakage problem is serious in practice which very little literature focused on it. In order to address both problems, in this article, we propose a key aggregation searchable encryption (KASE) scheme based on the blockchain with auxiliary input (AI), which is capable of achieving secure data sharing on the encrypted data. Our scheme is presented through a novel chosen plaintext attack (CPA) secure scheme. We prove our scheme is chosen ciphertext attack (CCA) secure against key leakage under the decisional Diffie-Hellman assumption and Goldreich-Leivin theorem. Moreover, we adopt the proposed scheme to establish a data-sharing system based on blockchain, which improves search efficiency and connects the global ecology. In addition, extensive performance evaluations are conducted, and the results indicate our scheme is really efficient in cloud-computing-enhanced IoT.","2327-4662","","10.1109/JIOT.2019.2956322","Guangxi Key Laboratory of Cryptography and Information Security(grant numbers:GCIS201802); National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800601); National Natural Science Foundation of China(grant numbers:61303217,61502372); Natural Science Foundation of Shaanxi Province(grant numbers:2013JQ8002,2014JQ8313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8915860","Blockchain;chosen ciphertext attack (CCA) secure;cloud computing;data sharing;key leakage;outsourcing","Encryption;Cloud computing;Internet of Things;Aggregates","","49","","41","IEEE","27 Nov 2019","","","IEEE","IEEE Journals"
"Learning Markets: An AI Collaboration Framework Based on Blockchain and Smart Contracts","L. Ouyang; Y. Yuan; F. -Y. Wang","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China",IEEE Internet of Things Journal,"8 Aug 2022","2022","9","16","14273","14286","Artificial intelligence (AI) has been witnessed to provide valuable solutions to all walks of life. However, data island and computing resources limitations in the centralized AI architectures have increased their technical barriers, and thus distributed AI collaboration in data, models, and resources has attracted intensive research interests. Since the existing trust-based collaboration models are no longer applicable for the large-scale distributed collaboration among trustless machines in open and dynamic environments, this article proposes a novel decentralized AI collaboration framework, i.e., learning markets (LM), in which blockchain provides a trustless environment for collaboration and transaction, while smart contracts serve as software-defined agents to encapsulate and process scalable collaboration relationships and market mechanisms. LM can not only help those participants without mutual trust realize collaborative mining with dynamic and quantitative rewards but also build an AI market with natural auditability and traceability for trading trusted and verified models. We implement and comprehensively analyze LM based on the Ethereum interplenary file system platform (IPFS), and the results prove that it has advantages in collaboration fairness, transparency, security, decentralization and universality. Based on our collaboration framework, distributed AI contributors are expected to cooperate and complete those learning tasks that cannot be done previously due to lack of complete data, sufficient computing resources and state-of-the-art models.","2327-4662","","10.1109/JIOT.2020.3032706","National Key Research and Development Program of China(grant numbers:2018AAA0101401); Science and Technology Development Fund, Macau SAR(grant numbers:0050/2020/A1); National Natural Science Foundation of China(grant numbers:61533019,71702182); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234516","Artificial intelligence (AI) collaboration;blockchain;ensemble learning;federated learning (FL);smart contracts","Collaboration;Artificial intelligence;Data models;Computational modeling;Smart contracts;Blockchain;Distributed databases","","41","","38","IEEE","21 Oct 2020","","","IEEE","IEEE Journals"
"Enabling Secure and Flexible Streaming Media With Blockchain Incentive","T. Li; A. Yang; J. Weng; M. -R. Chen; X. Luo; X. Chen; C. Jiang","College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China",IEEE Internet of Things Journal,"8 Jan 2024","2024","11","2","1966","1980","As a typical application of mobile crowdsourcing, streaming media has been attracting increasing attention in recent years. However, traditional streaming media platforms, such as Netflix, Disney+, and Hulu, may suffer some problems like inflexible billing modes, lacking sustainability in the incentive mechanisms, and management censorship. These problems may lead to a decrease in user participation rate, which will directly affect the interests of streaming media platforms. To address these issues, we propose a secure, efficient, and flexible streaming media platform framework based on blockchain and well-designed smart contracts. In particular, we design a new billing model based on a pay-as-you-go strategy and a new incentive mechanism with a probabilistic payment technique. To improve the fairness of our incentive model, we introduce a secondary fee refund protocol where a user’s second consecutive payment could be refunded, which in turn can attract more users to participate in the platform. Since blockchain has the natural properties of decentralization and transparency, the proposed framework is resistant to censorship and enables the transactions to be publicly auditable. Based on the proposed framework, we have implemented two streaming media platform schemes. Scheme I relies primarily on smart contracts to implement the framework’s functionality, while Scheme II moves the main flow of the framework to off-chain channels. As the execution of smart contracts requires transaction fees, Scheme I is more expensive but can provide much more security and accountability as well. Scheme II can execute the transaction process much faster and with only a small transaction fee. Finally, we deployed these two schemes on Ropsten and conduct a series of experiments. The results show the effectiveness and efficiency of the proposed schemes.","2327-4662","","10.1109/JIOT.2023.3305048","National Key R&D Program of China(grant numbers:2020YFB1005600); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101090004,2020B0101360001); National Natural Science Foundation of China(grant numbers:62072215,61825203,62332007,U22B2028); Major Program of Guangdong Basic and Applied Research Project(grant numbers:2019B030302008); Guangdong Provincial Science and Technology Project(grant numbers:2021A0505030033); Science and Technology Major Project of Tibetan Autonomous Region of China(grant numbers:XZ202201ZD0006G); National Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Key Laboratory of Data Security and Privacy Preserving, Guangdong Hong Kong Joint Laboratory for Data Security and Privacy Protection, and Engineering Research Center of Trustworthy AI, Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10216958","Blockchain;fairness;probability payment;smart contract;streaming media","Streaming media;Blockchains;Smart contracts;Media;Protocols;Probabilistic logic;Security","","2","","48","IEEE","14 Aug 2023","","","IEEE","IEEE Journals"
"Data-to-Text Generation for Fair and Transparent Employee Evaluations: Methods, Challenges, and HR Applications","O. Belarache; S. Ounacer; M. Azzouazi","Faculty Of Sciences Ben, M'sick University Hassan II, Casablanca, Morocco; Faculty Of Sciences Ben, M'sick University Hassan II, Casablanca, Morocco; Faculty Of Sciences Ben, M'sick University Hassan II, Casablanca, Morocco","2025 International Conference on Circuit, Systems and Communication (ICCSC)","5 Sep 2025","2025","","","1","6","Data-to-Text (D2T) generation, a core area of Natural Language Generation (NLG), transforms structured inputs into human-readable narratives. While widely applied in domains like finance and healthcare, its potential for fair and transparent employee evaluations in human resources (HR) remains largely untapped. This paper bridges this gap by exploring how general D2T approaches-from rule-based templates to neural architectures like T5 and GPT, which are neural text generation models-could revolutionize HR practices, even as the field lacks direct studies linking these methods to performance evaluation. We outline the strengths and limitations of existing systems in handling heterogeneous data, such as performance metrics and peer reviews, while addressing critical HR concerns like data scarcity, bias amplification, and employee privacy. Though exploratory, applications like automated appraisal summaries and bias audits in feedback systems demonstrate tangible pathways for adoption. However, progress hinges on interdisciplinary collaboration to develop HR-specific benchmarks, ethical safeguards, and adaptable solutions for organizations with limited data. By reimagining D2T as a tool for accountability and equity, this work urges researchers and practitioners to prioritize human-centered innovation, ensuring automated evaluations enhance, not undermine, workplace trust and growth. These approaches could also contribute to a more transparent and trust-based workplace culture, improving employee engagement and organizational cohesion.","","979-8-3315-6528-2","10.1109/ICCSC66714.2025.11135234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11135234","Data-to-Text Generation;Natural Language Generation (NLG);Employee Performance Evaluation;Automated Feedback;AI-driven HR;Bias Mitigation;HR Analytics","Ethics;Data privacy;Technological innovation;Reviews;Prevention and mitigation;Natural language generation;Employment;Transforms;Organizations;Performance metrics","","","","30","IEEE","5 Sep 2025","","","IEEE","IEEE Conferences"
"Blockchain-Based Wireless Sensor Networks for Malicious Node Detection: A Survey","L. K. Ramasamy; F. Khan K. P.; A. L. Imoize; J. O. Ogbebor; S. Kadry; S. Rho","Centre of Excellence for Artificial Intelligence and Machine Learning, Hindusthan College of Engineering and Technology, Coimbatore, India; Dubai Men’s College, Higher Colleges of Technology, Dubai, United Arab Emirates; Department of Electrical and Electronics Engineering, University of Lagos, Lagos, Akoka, Nigeria; Department of Electrical and Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA, USA; Department of Applied Data Science, Noroff University College, Kristiansand, Norway; Department of Industrial Security, Chung-Ang University, Seoul, South Korea",IEEE Access,"23 Sep 2021","2021","9","","128765","128785","Wireless Sensor Networks (WSNs) are broadly applied for various applications in tracking and surveillance due to their ease of use and other distinctive characteristics compelled by real-time cooperation among the sensor nodes. In WSNs, security is becoming a critical issue, as the techniques for malicious node detection adopt a one-time, centralized decision-making approach. With this paradigm, errors are difficult to avoid, and reproducibility and traceability are challenging. Hence, malicious node discovery technologies in conventional WSNs cannot assure traceability and fairness of the detection method. Herein, this paper discusses an in-depth survey of a blockchain-based approach for malicious node detection, an exhaustive examination of the integration of blockchain techniques with WSNs (BWSN), and insights into this novel concept. This survey discusses the architecture, sector-wise applications, and uses of BWSN. Moreover, this survey describes malicious node detection based on BWSN in two parts: 1) the BWSN architecture for detecting the malicious nodes and 2) the smart contract aspects in malicious node detection. Next, this survey explains the contributions of blockchain for WSN data management, which involves online information aggregation and may include auditing, event logs, and storage for information analysis and offline query processing. This survey first presents the conventional WSN solutions then the blockchain-based WSN solutions for data management. Additionally, this survey discusses the contributions of blockchain for WSN security management. It first examines the centralized WSN models for security problems, followed by a discussion of the blockchain-based WSN solutions for security management, such as offering access control, preserving information integrity, guaranteeing privacy, and ensuring WSNs’ node longevity.","2169-3536","","10.1109/ACCESS.2021.3111923","Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE), The Competency Development Program for Industry Specialist(grant numbers:P0008703); Ministry of Science and ICT (MSIT), South Korea, under the Information Technology Research Center (ITRC) Support Program IITP-2021-2018-0-01799 supervised by the Institute for Information & Communications Technology Planning & Evaluation (IITP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535517","Wireless sensor networks (WSNs);blockchain technology;malicious node detection;network security management;distributed consensus algorithm","Wireless sensor networks;Blockchains;Peer-to-peer computing;Security;Computer architecture;Smart contracts;Security management","","37","","159","CCBY","10 Sep 2021","","","IEEE","IEEE Journals"
"FLChain: A Blockchain for Auditable Federated Learning with Trust and Incentive","X. Bao; C. Su; Y. Xiong; W. Huang; Y. Hu","School of Computer Science, USTC, Hefei, China; School of Computer Science, USTC, Hefei, China; School of Computer Science, USTC, Hefei, China; School of Computer Science, USTC, Hefei, China; School of Computer Science, USTC, Hefei, China",2019 5th International Conference on Big Data Computing and Communications (BIGCOM),"21 Nov 2019","2019","","","151","159","Federated learning (shorted as FL) recently proposed by Google is a privacy-preserving method to integrate distributed data trainers. FL is extremely useful due to its ensuring privacy, lower latency, less power consumption and smarter models, but it could fail if multiple trainers abort training or send malformed messages to its partners. Such misbehavior are not auditable and parameter server may compute incorrectly due to single point failure. Furthermore, FL has no incentive to attract sufficient distributed training data and computation power. In this paper, we propose FLChain to build a decentralized, public auditable and healthy FL ecosystem with trust and incentive. FLChain replace traditional FL parameter server whose computation result must be consensual on-chain. Our work is not trivial when it is vital and hard to provide enough incentive and deterrence to distributed trainers. We achieve model commercialization by providing a healthy marketplace for collaborative-training models. Honest trainer can gain fairly partitioned profit from well-trained model according to its contribution and the malicious can be timely detected and heavily punished. To reduce the time cost of misbehavior detecting and model query, we design DDCBF for accelerating the query of blockchain-documented information. Finally, we implement a prototype of our work and measure the cost of various operations.","","978-1-7281-4024-7","10.1109/BIGCOM.2019.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905038","blockchain;federated learning;incentive;decentralize;trust","Training;Costs;Federated learning;Biological system modeling;Computational modeling;Prototypes;Training data;Time measurement;Blockchains;Servers","","166","","22","IEEE","21 Nov 2019","","","IEEE","IEEE Conferences"
"Causal Analysis for Robust Interpretability of Neural Networks","O. Ahmad; N. Béreux; L. Baret; V. Hashemi; F. Lecue","Thales Digital Solutions, CortAIx, Montreal, Canada; Paris-Saclay University, Paris, France; Thales Digital Solutions, CortAIx, Montreal, Canada; AUDI AG, Ingolstadt, Germany; Inria, Sophia Antipolis, France",2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"9 Apr 2024","2024","","","4673","4682","Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model’s behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specific causal explanatory graphs that can audit model behavior and express the actual causes underlying its performance. We apply our method to vision models trained on classification tasks. On image classification tasks, we provide extensive quantitative experiments to show that our approach can capture more stable and faithful explanations than standard attribution-based methods. Furthermore, the underlying causal graphs express the neural interactions in the model, making it a valuable tool in other applications (e.g., model repair).","2642-9381","979-8-3503-1892-0","10.1109/WACV57701.2024.00462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484102","Algorithms;Explainable;fair;accountable;privacy-preserving;ethical computer vision;Algorithms;Machine learning architectures;formulations;and algorithms","Training;Phase measurement;Computational modeling;Neural networks;Noise;Predictive models;Maintenance engineering","","7","","35","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Implications of Blockchain in Industry 4.O","A. Mushtaq; I. U. Haq",Pakistan Inst. of Engineering and Applied Sciences (PIEAS); Pakistan Inst. of Engineering and Applied Sciences (PIEAS),2019 International Conference on Engineering and Emerging Technologies (ICEET),"13 May 2019","2019","","","1","5","Rapid advancements in Information Technology and industrialization methods have expedited the advent of 4th Industrial revolution also known as integrated industry industrial internet or smart manufacturing. The notion of Industry 4.0 promises unprecedented progress in next generation of manufacturing technology by fundamentally changing the ways of production and value creation with the help of digital transformation in product/service offerings and hori-zontaVvertical value chains. Industry4.0 is underpinned by a spectrum of emerging technologies such as internet of things, cloud computing, machine learning, adaptive robotics, cyber physical systems, artificial intelligence, Industrial Integration, and Service Oriented Computing. Distributed ledger technology also known as blockchain can not only affect Industry4.0 but also has its direct implications in the above mentiond set of technologies. Lack of powerful tools for visibility, accountability and auditing is a major obstacle in complex processes and supply chains of Industry 4.0. In particular, issues of cloning of products, counterfeiting, trickier maintenance and IP theft are crucial teething issues for realizing Industry 4.0, which poses unique challenges. This paper is an effort to break the ground for demonstrating and presenting the use of Blockchain technology in 4th industrial era.In this paper we explore some concepts of industry 4.0 using blockchain technology and see how blockchain enablement is beneficial for industry 4.0. we explore different areas where we can use blockchain technlogy to foster the development in Industry4.0.","2409-2983","978-1-7281-0279-5","10.1109/CEET1.2019.8711819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8711819","Industry4.0;Blockchain;Smart contracts;Supply chain;Identity management;micro payment;Internet of Things","Blockchain;Industries;Supply chains;Smart contracts;Internet of Things;Cryptography","","23","","18","IEEE","13 May 2019","","","IEEE","IEEE Conferences"
"Hybrid Blockchain-Based Resource Trading System for Federated Learning in Edge Computing","S. Fan; H. Zhang; Y. Zeng; W. Cai","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China",IEEE Internet of Things Journal,"5 Feb 2021","2021","8","4","2252","2264","By training a machine learning algorithm across multiple decentralized edge nodes, federated learning (FL) ensures the privacy of the data generated by the massive Internet-of-Things (IoT) devices. To economically encourage the participation of heterogeneous edge nodes, a transparent and decentralized trading platform is needed to establish a fair market among distinct edge companies. In this article, we propose a hybrid blockchain-based resource trading system that combines the advantages of both public and consortium blockchains. We design and implement a smart contract to facilitate an automatic, autonomous, and auditable rational reverse auction mechanism among edge nodes. Moreover, we leverage the payment channel technique to enable credible, fast, low-cost, and high-frequency payment transactions between requesters and edge nodes. Simulation results show that the proposed reverse auction mechanism can achieve the properties, including budget feasibility, truthfulness, and computational efficiency.","2327-4662","","10.1109/JIOT.2020.3028101","National Natural Science Foundation of China(grant numbers:61902333); Shenzhen Institute of Artificial Intelligence and Robotics for Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223754","Auction;blockchain;edge computing;Internet of Things (IoT);trade market","Blockchain;Internet of Things;Computational modeling;Edge computing;Peer-to-peer computing;Training;Smart contracts","","96","","39","IEEE","14 Oct 2020","","","IEEE","IEEE Journals"
"IEEE Standard Model Process for Addressing Ethical Concerns during System Design","",,IEEE Std 7000-2021,"13 Sep 2021","2021","","","1","82","A set of processes by which organizations can include consideration of ethical values throughout the stages of concept exploration and development is established by this standard. Management and engineering in transparent communication with selected stakeholders for ethical values elicitation and prioritization is supported by this standard, involving traceability of ethical values through an operational concept, value propositions, and value dispositions in the system design. Processes that provide for traceability of ethical values in the concept of operations, ethical requirements, and ethical risk-based design are described in the standard. All sizes and types of organizations using their own life cycle models are relevant to this standard. (The PDF of this standard is available in the IEEE GET program at https://ieeexplore.ieee.org/browse/standards/get-program/page/series?id=93)","","978-1-5044-7687-4","10.1109/IEEESTD.2021.9536679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536679","case for ethics;concept of operations;ethical value requirements;ethical values elicitation;ethically aligned design;IEEE 7000™;software engineering;system engineering;value-based requirements;value prioritization","IEEE Standards;Ethics;System analysis and design;Stakeholders;Engineering management;Process control;Operations research","","31","","80","","13 Sep 2021","","","IEEE","IEEE Standards"
