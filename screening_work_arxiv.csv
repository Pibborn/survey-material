Article Title,author,Abstract,note,year,month,doi,howpublished,Document Type,ID,include,reason
DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis,"Hmida, Ben, Firas AND Amich, Abderrahmen AND Kaboudi, Ata AND Eshete, Birhanu","Deep neural networks (DNNs) are increasingly being deployed in high-stakes applications, from self-driving cars to biometric authentication. However, their unpredictable and unreliable behaviors in real-world settings require new approaches to characterize and ensure their reliability.
  This paper introduces DeepProv, a novel and customizable system designed to capture and characterize the runtime behavior of DNNs during inference by using their underlying graph structure. Inspired by systemauditprovenance graphs, DeepProv models the computational information flow of a DNN's inference process through Inference Provenance Graphs (IPGs). These graphs provide a detailed structural representation of the behavior of DNN, allowing both empirical and structural analysis. DeepProv uses these insights to systematically repair DNNs for specific objectives, such as improving robustness,privacy, orfairness.
  We instantiate DeepProv with adversarial robustness as the goal of model repair and conduct extensive case studies to evaluate its effectiveness. Our results demonstrate its effectiveness and scalability across diverse classification tasks, attack scenarios, and model complexities. DeepProv automatically identifies repair actions at the node and edge-level within IPGs, significantly enhancing the robustness of the model. In particular, applying DeepProv repair strategies to just a single layer of a DNN yields an average 55% improvement in adversarial accuracy. Moreover, DeepProv complements existing defenses, achieving substantial gains in adversarial robustness. Beyond robustness, we demonstrate the broader potential of DeepProv as an adaptable system to characterize DNN behavior in other critical areas, such asprivacyauditingandfairnessanalysis.","18 pages, 9 figures, 6 tables, To appear in the 41st Annual Computer Security Applications Conference (ACSAC), 2025",2025,sep,https://doi.org/10.48550/arXiv.2509.26562,\url{https://arxiv.org/pdf/2509.26562},ARTICLE,Hmida2025,no,not auditing OF AI
"Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned","Schweighofer, Kajetan AND Brune, Barbara AND Gruber, Lukas AND Schmid, Simon AND Aufreiter, Alexander AND Gruber, Andreas AND Doms, Thomas AND Eder, Sebastian AND Mayer, Florian AND Stadlbauer, Xaver-Paul AND Schwald, Christoph AND Zellinger, Werner AND Nessler, Bernhard AND Hochreiter, Sepp","There is an increasing adoption of artificial intelligence in safety-critical applications, yet practical schemes for certifying that AI systems are safe, lawful and socially acceptable remain scarce. This white paper presents the TÃœV AUSTRIA Trusted AI framework an end-to-endauditcatalog and methodology for assessing and certifying machine learning systems. Theauditcatalog has been in continuous development since 2019 in an ongoing collaboration with scientific partners. Building on three pillars - Secure Software Development, Functional Requirements, and Ethics & DataPrivacy- the catalog translates the high-level obligations of the EU AI Act into specific, testable criteria. Its core concept of functional trustworthiness couples a statistically defined application domain with risk-based minimum performance requirements and statistical testing on independently sampled data, providing transparent and reproducible evidence of model quality in real-world settings. We provide an overview of the functional requirements that we assess, which are oriented on the lifecycle of an AI system. In addition, we share some lessons learned from the practical application of theauditcatalog, highlighting common pitfalls we encountered, such as data leakage scenarios, inadequate domain definitions, neglect of biases, or a lack of distribution drift controls. We further discuss key aspects of certifying AI systems, such as robustness, algorithmicfairness, or post-certification requirements, outlining both our current conclusions and a roadmap for future research. In general, by aligning technical best practices with emerging European standards, the approach offers regulators, providers, and users a practical roadmap for legally compliant, functionally trustworthy, and certifiable AI systems.","63 pages, 27 figures",2025,sep,https://doi.org/10.48550/arXiv.2509.08852,\url{https://arxiv.org/pdf/2509.08852},ARTICLE,Schweighofer2025,yes,
Adaptive t Design Dummy-Gate Obfuscation for Cryogenic Scale Enforcement,"Punch, Samuel AND Guha, Krishnendu","Cloud quantum services can reveal circuit structure and timing through scheduler metadata, latency patterns, and co-tenant interference. We introduce NADGO (Noise-Adaptive Dummy-Gate Obfuscation), a scheduling and obfuscation stack that enforces operationalprivacyfor gate-model workloads by applying per-interval limits on observable information leakage. To support confidentiality andfairmulti-tenancy, operators require a method toauditcompliance at acceptable overheads. NADGO combines: (i) hardware-aware t-design padding for structured cover traffic, (ii) particle-filter timing randomization to mask queue patterns, (iii) CASQUE subcircuit routing across heterogeneous backends, and (iv) a per-interval leakage estimator with locked calibration artifacts and a dual-threshold kill-switch. We prototype the approach on a 4-qubit superconducting tile with cryo-CMOS control and evaluate both depth-varied local-random circuits and small QAOA instances. Monitoring runs at a 6.3 microsecond control interval, and per-interval decisions are recorded in an append-only, hash-chainedauditlog. Across Monte Carlo (Tier 1) and cloud-hardware emulation (Tier 2) evaluations, NADGO maintains leakage within budget in nominal operation (interval-abort rate below 1 percent) and under attack yields high separation with concentrated aborts. At matched leakage targets, microbenchmarks indicate lower latency and cryogenic power consumption than static padding, while end-to-end workloads maintain competitive cost envelopes.",6,2025,aug,https://doi.org/10.48550/arXiv.2509.00812,\url{https://arxiv.org/pdf/2509.00812},ARTICLE,Punch2025,no,not auditing OF AI
Towards Enhancing Data Equity in Public Health Data Science,"Wang, Yiran AND Boyd, E., Alicia AND Rountree, Lillian AND Ren, Yi AND Nyhan, Kate AND Nagar, Ruchit AND Higginbottom, Jackson AND Ranney, L., Megan AND Parikh, Harsh AND Mukherjee, Bhramar","Data-driven decisions shape public health policies and practice, yet persistent disparities in data representation skew insights and undermine interventions. To address this, we advance a structured roadmap that integrates public health data science with computer science and is grounded in reflexivity. We adopt data equity as a guiding concept: ensuring thefairand inclusive representation, collection, and use of data to prevent the introduction or exacerbation of systemic biases that could lead to invalid downstream inference and decisions. To underscore urgency, we present three public health cases where non-representative datasets and skewed knowledge impede decisions across diverse subgroups. These challenges echo themes in two literatures: public health highlights gaps in high-quality data for specific populations, while computer science and statistics contribute criteria and metrics for diagnosing bias in data and models. Building on these foundations, we propose a working definition of public health data equity and a structured self-auditframework. Our framework integrates core computational principles (fairness, accountability, transparency, ethics,privacy, confidentiality) with key public health considerations (selection bias, representativeness, generalizability, causality, information bias) to guide equitable practice across the data life cycle, from study design and data collection to measurement, analysis, interpretation, and translation. Embedding data equity in routine practice offers a practical path for ensuring that data-driven policies, artificial intelligence, and emerging technologies improve health outcomes for all. Finally, we emphasize the critical understanding that, although data equity is an essential first step, it does not inherently guarantee information, learning, or decision equity.","111 pages, 4 figures, 1 table",2025,aug,https://doi.org/10.48550/arXiv.2508.20301,\url{https://arxiv.org/pdf/2508.20301},ARTICLE,Wang2025,yes,
DAIQ:AuditingDemographic Attribute Inference from Question in LLMs,"Panda, Srikant AND Patel, Laxmichand, Hitesh AND Al-Khalifa, Shahad AND Agarwal, Amit AND Al-Khalifa, Hend AND Al-Ghamdi, Sharefah","Large Language Models (LLMs) are known to reflect social biases when demographic attributes, such as gender or race, are explicitly present in the input. But even in their absence, these models still infer user identities based solely on question phrasing. This subtle behavior has received far less attention, yet poses serious risks: it violates expectations of neutrality, infers unintended demographic information, and encodes stereotypes that underminefairnessin various domains including healthcare, finance and education.
  We introduce Demographic Attribute Inference from Questions (DAIQ), a task and framework forauditingan overlooked failure mode in language models: inferring user demographic attributes from questions that lack explicit demographic cues. Our approach leverages curated neutral queries, systematic prompting, and both quantitative and qualitative analysis to uncover how models infer demographic information. We show that both open and closed source LLMs do assign demographic labels based solely on question phrasing.
  Prevalence and consistency of demographic inferences across diverse models reveal a systemic and underacknowledged risk: LLMs can fabricate demographic identities, reinforce societal stereotypes, and propagate harms that erodeprivacy,fairness, and trust posing a broader threat to social equity and responsible AI deployment. To mitigate this, we develop a prompt-based guardrail that substantially reduces identity inference and helps align model behavior withfairnessandprivacyobjectives.",Preprint,2025,aug,https://doi.org/10.48550/arXiv.2508.15830,\url{https://arxiv.org/pdf/2508.15830},ARTICLE,Panda2025,yes,
Beyond Internal Data: Bounding and EstimatingFairnessfrom Incomplete Data,"Ramineni, Varsha AND Rahmani, A., Hossein AND Yilmaz, Emine AND Barber, David","Ensuringfairnessin AI systems is critical, especially in high-stakes domains such as lending, hiring, and healthcare. This urgency is reflected in emerging global regulations that mandatefairnessassessments and independent biasaudits. However, procuring the necessary complete data forfairnesstesting remains a significant challenge. In industry settings, legal andprivacyconcerns restrict the collection of demographic data required to assess group disparities, andauditorsface practical and cultural challenges in gaining access to data. In practice, data relevant forfairnesstesting is often split across separate sources: internal datasets held by institutions with predictive attributes, and external public datasets such as census data containing protected attributes, each providing only partial, marginal information. Our work seeks to leverage such available separate data to estimate modelfairnesswhen complete data is inaccessible. We propose utilising the available separate data to estimate a set of feasible joint distributions and then compute the set plausiblefairnessmetrics. Through simulation and real experiments, we demonstrate that we can derive meaningful bounds onfairnessmetrics and obtain reliable estimates of the true metric. Our results demonstrate that this approach can serve as a practical and effective solution forfairnesstesting in real-world settings where access to complete data is restricted.","9 pages, 3 figures",2025,aug,https://doi.org/10.48550/arXiv.2508.13040,\url{https://arxiv.org/pdf/2508.13040},ARTICLE,Ramineni2025,yes,
The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage,"Hallinan, Skyler AND Jung, Jaehun AND Sclar, Melanie AND Lu, Ximing AND Ravichander, Abhilasha AND Ramnath, Sahana AND Choi, Yejin AND Karimireddy, Praneeth, Sai AND Mireshghallah, Niloofar AND Ren, Xiang","Membership inference attacks serves as useful tool forfairuse of language models, such as detecting potential copyright infringement andauditingdata leakage. However, many current state-of-the-art attacks require access to models' hidden states or probability distribution, which prevents investigation into more widely-used, API-access only models like GPT-4. In this work, we introduce N-Gram Coverage Attack, a membership inference attack that relies solely on text outputs from the target model, enabling attacks on completely black-box models. We leverage the observation that models are more likely to memorize and subsequently generate text patterns that were commonly observed in their training data. Specifically, to make a prediction on a candidate member, N-Gram Coverage Attack first obtains multiple model generations conditioned on a prefix of the candidate. It then uses n-gram overlap metrics to compute and aggregate the similarities of these outputs with the ground truth suffix; high similarities indicate likely membership. We first demonstrate on a diverse set of existing benchmarks that N-Gram Coverage Attack outperforms other black-box methods while also impressively achieving comparable or even better performance to state-of-the-art white-box attacks - despite having access to only text outputs. Interestingly, we find that the success rate of our method scales with the attack compute budget - as we increase the number of sequences generated from the target model conditioned on the prefix, attack performance tends to improve. Having verified the accuracy of our method, we use it to investigate previously unstudied closed OpenAI models on multiple domains. We find that more recent models, such as GPT-4o, exhibit increased robustness to membership inference, suggesting an evolving trend toward improvedprivacyprotections.",CoLM 2025,2025,aug,https://doi.org/10.48550/arXiv.2508.09603,\url{https://arxiv.org/pdf/2508.09603},ARTICLE,Hallinan2025,no,not auditing OF AI
"Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models","Pan, Jiazhen AND Jian, Bailiang AND Hager, Paul AND Zhang, Yundi AND Liu, Che AND Jungmann, Friedrike AND Li, Bran, Hongwei AND You, Chenyu AND Wu, Junde AND Zhu, Jiayuan AND Liu, Fenglin AND Liu, Yuyuan AND Bubeck, Niklas AND Wachinger, Christian AND Chen AND Chen AND Gong, Zhenyu AND Ouyang, Cheng AND Kaissis, Georgios AND Wiestler, Benedikt AND Rueckert, Daniel","Ensuring the safety and reliability of large language models (LLMs) in clinical practice is critical to prevent patient harm and promote trustworthy healthcare applications of AI. However, LLMs are advancing so rapidly that static safety benchmarks often become obsolete upon publication, yielding only an incomplete and sometimes misleading picture of model trustworthiness. We demonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming framework that continuously stress-tests LLMs can reveal significant weaknesses of current LLMs across four safety-critical domains: robustness,privacy, bias/fairness, and hallucination. A suite of adversarial agents is applied to autonomously mutate test cases, identify/evolve unsafe-triggering strategies, and evaluate responses, uncovering vulnerabilities in real time without human intervention. Applying DAS to 15 proprietary and open-source LLMs revealed a stark contrast between static benchmark performance and vulnerability under adversarial pressure. Despite a median MedQA accuracy exceeding 80\%, 94\% of previously correct answers failed our dynamic robustness tests. We observed similarly high failure rates across other domains:privacyleaks were elicited in 86\% of scenarios, cognitive-bias priming altered clinical recommendations in 81\% offairnesstests, and we identified hallucination rates exceeding 66\% in widely used models. Such profound residual risks are incompatible with routine clinical practice. By converting red-teaming from a static checklist into a dynamic stress-testaudit, DAS red-teaming offers the surveillance that hospitals/regulators/technology vendors require as LLMs become embedded in patient chatbots, decision-support dashboards, and broader healthcare workflows. Our framework delivers an evolvable, scalable, and reliable safeguard for the next generation of medical AI.",,2025,jul,https://doi.org/10.48550/arXiv.2508.00923,\url{https://arxiv.org/pdf/2508.00923},ARTICLE,Pan2025,yes,
Beyond Internal Data: Constructing Complete Datasets forFairnessTesting,"Ramineni, Varsha AND Rahmani, A., Hossein AND Yilmaz, Emine AND Barber, David","As AI becomes prevalent in high-risk domains and decision-making, it is essential to test for potential harms and biases. This urgency is reflected by the global emergence of AI regulations that emphasisefairnessand adequate testing, with some mandating independent biasaudits. However, procuring the necessary data forfairnesstesting remains a significant challenge. Particularly in industry settings, legal andprivacyconcerns restrict the collection of demographic data required to assess group disparities, andauditorsface practical and cultural challenges in gaining access to data. Further, internal historical datasets are often insufficiently representative to identify real-world biases. This work focuses on evaluating classifierfairnesswhen complete datasets including demographics are inaccessible. We propose leveraging separate overlapping datasets to construct complete synthetic data that includes demographic information and accurately reflects the underlying relationships between protected attributes and model features. We validate the fidelity of the synthetic data by comparing it to real data, and empirically demonstrate thatfairnessmetrics derived from testing on such synthetic data are consistent with those obtained from real data. This work, therefore, offers a path to overcome real-world data scarcity forfairnesstesting, enabling independent, model-agnostic evaluation offairness, and serving as a viable substitute where real data is limited.","9 pages, 6 figures",2025,jul,https://doi.org/10.48550/arXiv.2507.18561,\url{https://arxiv.org/pdf/2507.18561},ARTICLE,Ramineni2025,no,not auditing OF AI
EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data,"Alimoglu, Alper AND Erdayandi, Kamil AND Mustafa, A., Mustafa AND Cali, Ãœmit","This paper proposes a new decentralized framework, named EDGChain-E (Encrypted-Data-Git Chain for Energy), designed to manage version-controlled, encrypted energy data using blockchain and the InterPlanetary File System. The framework incorporates a Decentralized Autonomous Organization (DAO) to orchestrate collaborative data governance across the lifecycle of energy research and operations, such as smart grid monitoring, demand forecasting, and peer-to-peer energy trading. In EDGChain-E, initial commits capture the full encrypted datasets-such as smart meter readings or grid telemetry-while subsequent updates are tracked as encrypted Git patches, ensuring integrity, traceability, andprivacy. This versioning mechanism supports secure collaboration across multiple stakeholders (e.g., utilities, researchers, regulators) without compromising sensitive or regulated information. We highlight the framework's capability to maintainFAIR-compliant (Findable, Accessible, Interoperable, Reusable) provenance of encrypted data. By embedding hash-based content identifiers in Merkle trees, the system enables transparent,auditable, and immutable tracking of data changes, thereby supporting reproducibility and trust in decentralized energy applications.",,2025,jul,https://doi.org/10.48550/arXiv.2507.01615,\url{https://arxiv.org/pdf/2507.01615},ARTICLE,Alimoglu2025,no,not auditing OF AI
AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview,"Jain, Madhusudan, Aditi AND Jain, Ayush","As e-commerce rapidly integrates artificial intelligence for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI-driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around dataprivacy, algorithmic bias, and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic -- can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes. This paper examines the ethical implications of AI-driven content creation and product recommendations, emphasizing the need for frameworks to ensurefairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regularauditsof algorithms, diversifying training data, and incorporatingfairnessmetrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer dataprivacy, promoting transparency in decision-making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.",,2025,jun,https://doi.org/10.48550/arXiv.2506.17370,\url{https://arxiv.org/pdf/2506.17370},ARTICLE,Jain2025,no,survey or review
DifferentialPrivacyfor Deep Learning in Medicine,"Mohammadi, Marziyeh AND Vejdanihemmat, Mohsen AND Lotfinia, Mahshad AND Rusu, Mirabela AND Truhn, Daniel AND Maier, Andreas AND Arasteh, Tayebi, Soroosh","Differentialprivacy(DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancingprivacywith utility andfairnesshas become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs betweenprivacyguarantees, model accuracy, and subgroupfairness. We find that while DP-especially at strongprivacybudgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strictprivacy, particularly in underrepresented or complex modalities. Furthermore,privacy-induced performance gaps disproportionately affect demographic subgroups, withfairnessimpacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis orfairnessmetrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps infairnessauditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robustprivacy-preserving DL systems in medicine.",,2025,may,https://doi.org/10.48550/arXiv.2506.00660,\url{https://arxiv.org/pdf/2506.00660},ARTICLE,Mohammadi2025,no,survey or review
Ethical AI: Towards Defining a Collective Evaluation Framework,"Sharma, Kumar, Aasish AND Kyosev, Dimitar AND Kunkel, Julian","Artificial Intelligence (AI) is transforming sectors such as healthcare, finance, and autonomous systems, offering powerful tools for innovation. Yet its rapid integration raises urgent ethical concerns related to data ownership,privacy, and systemic bias. Issues like opaque decision-making, misleading outputs, and unfair treatment in high-stakes domains underscore the need for transparent and accountable AI systems. This article addresses these challenges by proposing a modular ethical assessment framework built on ontological blocks of meaning-discrete, interpretable units that encode ethical principles such asfairness, accountability, and ownership. By integrating these blocks withFAIR(Findable, Accessible, Interoperable, Reusable) principles, the framework supports scalable, transparent, and legally aligned ethical evaluations, including compliance with the EU AI Act. Using a real-world use case in AI-powered investor profiling, the paper demonstrates how the framework enables dynamic, behavior-informed risk classification. The findings suggest that ontological blocks offer a promising path toward explainable andauditableAI ethics, though challenges remain in automation and probabilistic reasoning.","6 pages, 3 figures, accepted at 8th IEEE International Workshop on Advances in Artificial Intelligence and Machine Learning (AIML 2025): Futuristic AI and ML models & Intelligent Systems",2025,may,https://doi.org/10.48550/arXiv.2506.00233,\url{https://arxiv.org/pdf/2506.00233},ARTICLE,Sharma2025,yes,
"FairPlay for Individuals, Foul Play for Groups?AuditingAnonymization's Impact on MLFairness","Arcolezi, H., HÃ©ber AND Alishahi, Mina AND Bendoukha, Adda-Akram AND Kaaniche, Nesrine","Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises criticalprivacyconcerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown thatprivacy-enhancing technologies can influence ML predictions across different subgroups, thus affectingfairdecision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and $t$-closeness, on MLfairnessremain largely unexplored. In this work, we systematicallyauditthe impact of anonymization techniques on MLfairness, evaluating both individual and groupfairness. Our quantitative study reveals that anonymization can degrade groupfairnessmetrics by up to four orders of magnitude. Conversely, similarity-based individualfairnessmetrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverseprivacysettings and data distributions, this study provides critical insights into the trade-offs betweenprivacy,fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.",,2025,may,https://doi.org/10.48550/arXiv.2505.07985,\url{https://arxiv.org/pdf/2505.07985},ARTICLE,Arcolezi2025,no,this is about auditing how well anonymization techniques work. probably not relevant
QuantitativeAuditingof AIFairnesswith DifferentiallyPrivateSynthetic Data,"Yuan, Rex, Chih-Cheng AND Wang, Bow-Yaw","Fairnessauditingof AI systems can identify and quantify biases. However, traditionalauditingusing real-world data raises security andprivacyconcerns. It exposesauditorsto security risks as they become custodians of sensitive information and targets for cyberattacks.Privacyrisks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentiallyprivatesynthetic data toauditthefairnessof AI systems. By applyingprivacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuringprivacy. This method balances the goal of rigorousfairnessauditingand the need for strongprivacyprotections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we comparefairnessmetrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve thefairnessproperties of real data. Our results demonstrate the framework's ability to enable meaningfulfairnessevaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.",,2025,apr,https://doi.org/10.48550/arXiv.2504.21634,\url{https://arxiv.org/pdf/2504.21634},ARTICLE,Yuan2025,yes,
"SA2FE: A Secure, Anonymous,Auditable, andFairEdge Computing Service Offloading Framework","Wang, Xiaojian AND Gu, Huayue AND Li, Zhouyu AND Zhou, Fangtong AND Yu, Ruozhou AND Yang, Dejun AND Xue, Guoliang","The inclusion of pervasive computing devices in a democratized edge computing ecosystem can significantly expand the capability and coverage of near-end computing for large-scale applications. However, offloading user tasks to heterogeneous and decentralized edge devices comes with the dual risk of both endangered user data security andprivacydue to the curious base station or malicious edge servers, and unfair offloading and malicious attacks targeting edge servers from other edge servers and/or users. Existing solutions to edge access control and offloading either rely on ""always-on"" cloud servers with reduced edge benefits or fail to protect sensitive user service information. To address these challenges, this paper presents SA2FE, a novel framework for edge access control, offloading and accounting. We design a rerandomizable puzzle primitive and a corresponding scheme to protect sensitive service information from eavesdroppers and ensurefairoffloading decisions, while a blind token-based scheme safeguards userprivacy, prevents double spending, and ensures usage accountability. The security of SA2FE is proved under the Universal Composability framework, and its performance and scalability are demonstrated with implementation on commodity mobile devices and edge servers.",,2025,apr,https://doi.org/10.48550/arXiv.2504.20260,\url{https://arxiv.org/pdf/2504.20260},ARTICLE,Wang2025,no,
Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts,"Sankaran, Sridharan","As artificial intelligence (AI) reshapes industries and societies, ensuring its trustworthiness-through mitigating ethical risks like bias, opacity, and accountability deficits-remains a global challenge. International Organization for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to foster responsible development by embeddingfairness, transparency, and risk management into AI systems. However, their effectiveness varies across diverse regulatory landscapes, from the EU's risk-based AI Act to China's stability-focused measures and the U.S.'s fragmented state-led initiatives. This paper introduces a novel Comparative Risk-Impact Assessment Framework to evaluate how well ISO standards address ethical risks within these contexts, proposing enhancements to strengthen their global applicability. By mapping ISO standards to the EU AI Act and surveying regulatory frameworks in ten regions-including the UK, Canada, India, Japan, Singapore, South Korea, and Brazil-we establish a baseline for ethical alignment. The framework, applied to case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO standards falter in enforcement (e.g., Colorado) and undervalue region-specific risks likeprivacy(China). We recommend mandatory riskaudits, region-specific annexes, and aprivacy-focused module to enhance ISO's adaptability. This approach not only synthesizes global trends but also offers a replicable tool for aligning standardization with ethical imperatives, fostering interoperability and trust in AI worldwide. Policymakers and standards bodies can leverage these insights to evolve AI governance, ensuring it meets diverse societal needs as the technology advances.",,2025,apr,https://doi.org/10.48550/arXiv.2504.16139,\url{https://arxiv.org/pdf/2504.16139},ARTICLE,Sankaran2025,no,survey or review
P2NIA:Privacy-Preserving Non-IterativeAuditing,"BourrÃ©e, Garcia, Jade AND Lautraite, Hadrien AND Gambs, SÃ©bastien AND Tredan, Gilles AND Merrer, Le, Erwan AND Rottembourg, BenoÃ®t","The emergence of AI legislation has increased the need to assess the ethical compliance of high-risk AI systems. Traditionalauditingmethods rely on platforms' application programming interfaces (APIs), where responses to queries are examined through the lens offairnessrequirements. However, such approaches put a significant burden on platforms, as they are forced to maintain APIs while ensuringprivacy, facing the possibility of data leaks. This lack of proper collaboration between the two parties, in turn, causes a significant challenge to theauditor, who is subject to estimation bias as they are unaware of the data distribution of the platform. To address these two issues, we present P2NIA, a novelauditingscheme that proposes a mutually beneficial collaboration for both theauditorand the platform. Extensive experiments demonstrate P2NIA's effectiveness in addressing both issues. In summary, our work introduces aprivacy-preserving and non-iterativeauditscheme that enhancesfairnessassessments using synthetic or local data, avoiding the challenges associated with traditional API-basedaudits.","19 pages, 8 figures",2025,apr,https://doi.org/10.48550/arXiv.2504.00874,\url{https://arxiv.org/pdf/2504.00874},ARTICLE,BourrÃ©e2025,yes,
Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications toFairnessandPrivacyAuditing,"Basu, Debabrota AND Chanda, Debarshi","Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic framework to learn the probability and cumulative distribution functions (PDFs and CDFs) of a sub-Weibull, i.e. almost any light- or heavy-tailed, distribution while the samples from it arrive in a stream. The idea is to reduce these problems into estimating the frequency of an \textit{appropriately chosen subset} of the support of a \textit{properly discretised distribution}. We leverage this reduction to compute mergeable summaries of distributions from the stream of samples while requiring only sublinear space relative to the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two distributions while samples arrive in streams and from multiple sources. Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities, and further extend the mergeable summaries framework to continuous distributions with possibly infinite support. Our results are tight with respect to the existing lower bounds for bounded discrete distributions. In addition, we leverage our proposed estimators of Wasserstein and TV distances to tightlyauditthefairnessandprivacyof algorithms. We empirically demonstrate the efficiency of proposed algorithms across synthetic and real-world datasets.",,2025,jun,https://doi.org/10.48550/arXiv.2503.07775,\url{https://arxiv.org/pdf/2503.07775},ARTICLE,Basu2025,yes,
Fairness-Aware Low-Rank Adaptation Under DemographicPrivacyConstraints,"Kamalaruban, Parameswaran AND Anderson, Mark AND Burrell, Stuart AND Madigan, Maeve AND Skalski, Piotr AND Sutton, David","Pre-trained foundation models can be adapted for specific tasks using Low-Rank Adaptation (LoRA). However, thefairnessproperties of these adapted classifiers remain underexplored. Existingfairness-aware fine-tuning methods rely on direct access to sensitive attributes or their predictors, but in practice, these sensitive attributes are often held under strict consumerprivacycontrols, and neither the attributes nor their predictors are available to model developers, hampering the development offairmodels. To address this issue, we introduce a set of LoRA-based fine-tuning methods that can be trained in a distributed fashion, where model developers andfairnessauditorscollaborate without sharing sensitive attributes or predictors. In this paper, we evaluate three such methods - sensitive unlearning, adversarial training, and orthogonality loss - against afairness-unaware baseline, using experiments on the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base model. We find that orthogonality loss consistently reduces bias while maintaining or improving utility, whereas adversarial training improves False Positive Rate Parity and Demographic Parity in some cases, and sensitive unlearning provides no clear benefit. In tasks where significant biases are present, distributedfairness-aware fine-tuning methods can effectively eliminate bias without compromising consumerprivacyand, in most cases, improve model utility.",,2025,mar,https://doi.org/10.48550/arXiv.2503.05684,\url{https://arxiv.org/pdf/2503.05684},ARTICLE,Kamalaruban2025,no,not auditing OF AI
Beyond Diagnostic Performance: Revealing and Quantifying Ethical Risks in Pathology Foundation Models,"Lin, Weiping AND Liu, Shen AND Zhu, Runchen AND Lin, Yixuan AND Wang, Baoshun AND Wang, Liansheng","Pathology foundation models (PFMs), as large-scale pre-trained models tailored for computational pathology, have significantly advanced a wide range of applications. Their ability to leverage prior knowledge from massive datasets has streamlined the development of intelligent pathology models. However, we identify several critical and interrelated ethical risks that remain underexplored, yet must be addressed to enable the safe translation of PFMs from lab to clinic. These include the potential leakage of patient-sensitive attributes, disparities in model performance across demographic and institutional subgroups, and the reliance on diagnosis-irrelevant features that undermine clinical reliability. In this study, we pioneer the quantitative analysis for ethical risks in PFMs, includingprivacyleakage, clinical reliability, and groupfairness. Specifically, we propose an evaluation framework that systematically measures key dimensions of ethical concern: the degree to which patient-sensitive attributes can be inferred from model representations, the extent of performance disparities across demographic and institutional subgroups, and the influence of diagnostically irrelevant features on model decisions. We further investigate the underlying causes of these ethical risks in PFMs and empirically validate our findings. Then we offer insights into potential directions for mitigating such risks, aiming to inform the development of more ethically robust PFMs. This work provides the first quantitative and systematic evaluation of ethical risks in PFMs. Our findings highlight the urgent need for ethical safeguards in PFMs and offer actionable insights for building more trustworthy and clinically robust PFMs. To facilitate future research and deployment, we will release the assessment framework as an online toolkit to support the development,auditing, and deployment of ethically robust PFMs.","33 pages,5 figure,23 tables",2025,jul,https://doi.org/10.48550/arXiv.2502.16889,\url{https://arxiv.org/pdf/2502.16889},ARTICLE,Lin2025,yes,
Testing software for non-discrimination: an updated and extendedauditin the Italian car insurance domain,"Rondina, Marco AND VetrÃ², Antonio AND Coppola, Riccardo AND Regragrui, Oumaima AND Fabris, Alessandro AND Silvello, Gianmaria AND Susto, Antonio, Gian AND Martin, De, Carlos, Juan","Context. As software systems become more integrated into society's infrastructure, the responsibility of software professionals to ensure compliance with various non-functional requirements increases. These requirements include security, safety,privacy, and, increasingly, non-discrimination.
  Motivation.Fairnessin pricing algorithms grants equitable access to basic services without discriminating on the basis of protected attributes.
  Method. We replicate a previous empirical study that used black box testing toauditpricing algorithms used by Italian car insurance companies, accessible through a popular online system. With respect to the previous study, we enlarged the number of tests and the number of demographic variables under analysis.
  Results. Our work confirms and extends previous findings, highlighting the problematic permanence of discrimination across time: demographic variables significantly impact pricing to this day, with birthplace remaining the main discriminatory factor against individuals not born in Italian cities. We also found that driver profiles can determine the number of quotes available to the user, denying equal opportunities to all.
  Conclusion. The study underscores the importance of testing for non-discrimination in software systems that affect people's everyday lives. Performing algorithmicauditsover time makes it possible to evaluate the evolution of such algorithms. It also demonstrates the role that empirical software engineering can play in making software systems more accountable.","14 pages, 1 figure",2025,feb,https://doi.org/10.48550/arXiv.2502.06439,\url{https://arxiv.org/pdf/2502.06439},ARTICLE,Rondina2025,no,not auditing OF AI
Demographic Benchmarking: Bridging Socio-Technical Gaps in Bias Detection,"Clavell, Galdon, Gemma AND GonzÃ¡lez-Sendino, RubÃ©n AND Vazquez, Paola","Artificial intelligence (AI) models are increasingly autonomous in decision-making, making pursuing responsible AI more critical than ever. Responsible AI (RAI) is defined by its commitment to transparency,privacy, safety, inclusiveness, andfairness. But while the principles of RAI are clear and shared, RAI practices andauditingmechanisms are still incipient. A key challenge is establishing metrics and benchmarks that define performance goals aligned with RAI principles. This paper describes how the ITACA AIauditingplatform developed by Eticas.ai tackles demographic benchmarking whenauditingAI recommender systems. To this end, we describe a Demographic Benchmarking Framework designed to measure the populations potentially impacted by specific AI models. The framework serves us asauditorsas it allows us to not just measure but establish acceptability ranges for specific performance indicators, which we share with the developers of the systems weauditso they can build balanced training datasets and measure and monitorfairnessthroughout the AI lifecycle. It is also a valuable resource for policymakers in drafting effective and enforceable regulations. Our approach integrates socio-demographic insights directly into AI systems, reducing bias and improving overall performance. The main contributions of this study include:1. Defining control datasets tailored to specific demographics so they can be used in model training; 2. Comparing the overall population with those impacted by the deployed model to identify discrepancies and account for structural bias; and 3. Quantifying drift in different scenarios continuously and as a post-market monitoring mechanism.",,2025,jan,https://doi.org/10.48550/arXiv.2501.15985,\url{https://arxiv.org/pdf/2501.15985},ARTICLE,Clavell2025,yes,
Ethical AI in Retail: ConsumerPrivacyandFairness,"Adanyin, Anthonette","The adoption of artificial intelligence (AI) in retail has significantly transformed the industry, enabling more personalized services and efficient operations. However, the rapid implementation of AI technologies raises ethical concerns, particularly regarding consumerprivacyandfairness. This study aims to analyze the ethical challenges of AI applications in retail, explore ways retailers can implement AI technologies ethically while remaining competitive, and provide recommendations on ethical AI practices. A descriptive survey design was used to collect data from 300 respondents across major e-commerce platforms. Data were analyzed using descriptive statistics, including percentages and mean scores. Findings shows a high level of concerns among consumers regarding the amount of personal data collected by AI-driven retail applications, with many expressing a lack of trust in how their data is managed. Also,fairnessis another major issue, as a majority believe AI systems do not treat consumers equally, raising concerns about algorithmic bias. It was also found that AI can enhance business competitiveness and efficiency without compromising ethical principles, such as dataprivacyandfairness. Dataprivacyand transparency were highlighted as critical areas where retailers need to focus their efforts, indicating a strong demand for stricter data protection protocols and ongoing scrutiny of AI systems. The study concludes that retailers must prioritize transparency,fairness, and data protection when deploying AI systems. The study recommends ensuring transparency in AI processes, conducting regularauditsto address biases, incorporating consumer feedback in AI development, and emphasizing consumer dataprivacy.","17 pages, 2 figures, 3 tables",2024,oct,https://doi.org/10.48550/arXiv.2410.15369,\url{https://arxiv.org/pdf/2410.15369},ARTICLE,Adanyin2024,no,not auditing OF AI
"Operationalizing the Blueprint for an AI Bill of Rights: Recommendations for Practitioners, Researchers, and Policy Makers","Oesterling, Alex AND Bhalla, Usha AND Venkatasubramanian, Suresh AND Lakkaraju, Himabindu","As Artificial Intelligence (AI) tools are increasingly employed in diverse real-world applications, there has been significant interest in regulating these tools. To this end, several regulatory frameworks have been introduced by different countries worldwide. For example, the European Union recently passed the AI Act, the White House issued an Executive Order on safe, secure, and trustworthy AI, and the White House Office of Science and Technology Policy issued the Blueprint for an AI Bill of Rights (AI BoR). Many of these frameworks emphasize the need forauditingand improving the trustworthiness of AI tools, underscoring the importance of safety,privacy, explainability,fairness, and human fallback options. Although these regulatory frameworks highlight the necessity of enforcement, practitioners often lack detailed guidance on implementing them. Furthermore, the extensive research on operationalizing each of these aspects is frequently buried in technical papers that are difficult for practitioners to parse. In this write-up, we address this shortcoming by providing an accessible overview of existing literature related to operationalizing regulatory principles. We provide easy-to-understand summaries of state-of-the-art literature and highlight various gaps that exist between regulatory guidelines and existing AI research, including the trade-offs that emerge during operationalization. We hope that this work not only serves as a starting point for practitioners interested in learning more about operationalizing the regulatory guidelines outlined in the Blueprint for an AI BoR but also provides researchers with a list of critical open problems and gaps between regulations and state-of-the-art AI research. Finally, we note that this is a working paper and we invite feedback in line with the purpose of this document as described in the introduction.",15 pages,2024,jul,https://doi.org/10.48550/arXiv.2407.08689,\url{https://arxiv.org/pdf/2407.08689},ARTICLE,Oesterling2024,no,non-paper
"Navigating LLM Ethics: Advancements, Challenges, and Future Directions","Jiao, Junfeng AND Afroogh, Saleh AND Xu, Yiming AND Phillips, Connor","This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such asprivacyandfairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamicauditingsystems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.",,2025,jun,https://doi.org/10.48550/arXiv.2406.18841,\url{https://arxiv.org/pdf/2406.18841},ARTICLE,Jiao2025,no,not auditing OF AI
ExploringPrivacyandFairnessRisks in Sharing Diffusion Models: An Adversarial Perspective,"Luo, Xinjian AND Jiang, Yangfan AND Wei, Fei AND Wu, Yuncheng AND Xiao, Xiaokui AND Ooi, Chin, Beng","Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancingprivacyprotection by avoiding sharingprivatedata directly. However, the potential risks associated with such an approach have not been comprehensively examined.
  In this paper, we take an adversarial perspective to investigate the potentialprivacyandfairnessrisks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model usingprivatedata and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can executefairnesspoisoning attacks to undermine the receiver's downstream models by manipulating the training data distribution of the diffusion model. Meanwhile, the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer's dataset. Our experiments conducted on real-world datasets demonstrate remarkable attack performance on different types of diffusion models, which highlights the critical importance of robust dataauditingandprivacyprotection protocols in pertinent applications.",,2024,sep,https://doi.org/10.48550/arXiv.2402.18607,\url{https://arxiv.org/pdf/2402.18607},ARTICLE,Luo2024,no,not auditing OF AI
Publiclyauditableprivacy-preserving electoral rolls,"Agrawal, Prashant AND Jhanwar, Prasad, Mahabir AND Sharma, Vishnu, Subodh AND Banerjee, Subhashis","While existing literature on electronic voting has extensively addressed verifiability of voting protocols, the vulnerability of electoral rolls in large public elections remains a critical concern. To ensure integrity of electoral rolls, the current practice is to either make electoral rolls public or share them with the political parties. However, this enables construction of detailed voter profiles and selective targeting and manipulation of voters, thereby undermining the fundamental principle of free andfairelections. In this paper, we study the problem of designing publiclyauditableyetprivacy-preserving electoral rolls. We first formulate a threat model and provide formal security definitions. We then present a protocol for creation, maintenance and usage of electoral rolls that mitigates the threats. Eligible voters can verify their inclusion, whereas political parties andauditorscan statisticallyauditthe electoral roll. Further, theauditcan also detect polling-day ballot stuffing and denials to eligible voters by malicious polling officers. The entire electoral roll is never revealed, which prevents any large-scale systematic voter targeting and manipulation.",,2024,jun,https://doi.org/10.48550/arXiv.2402.11582,\url{https://arxiv.org/pdf/2402.11582},ARTICLE,Agrawal2024,no,not auditing OF AI
Is my Data in your AI Model? Membership Inference Test with Application to Face Images,"DeAlcala, Daniel AND Morales, Aythami AND Fierrez, Julian AND Mancera, Gonzalo AND Tolosana, Ruben AND Ortega-Garcia, Javier","This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when anAuditedModel is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90\% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforceprivacyandfairnessin several AI applications, e.g., revealing if sensitive orprivatedata was used for training or tuning Large Language Models (LLMs).",26 pages main text and 2 pages appendix,2025,jun,https://doi.org/10.48550/arXiv.2402.09225,\url{https://arxiv.org/pdf/2402.09225},ARTICLE,DeAlcala2025,no,not auditing OF AI
Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions,"Ganescu, Bianca-Mihaela AND Passerat-Palmbach, Jonathan","Generative AI, exemplified by models like transformers, has opened up new possibilities in various domains but also raised concerns aboutfairness, transparency and reliability, especially in fields like medicine and law. This paper emphasizes the urgency of ensuringfairnessand quality in these domains through generative AI. It explores using cryptographic techniques, particularly Zero-Knowledge Proofs (ZKPs), to address concerns regarding performancefairnessand accuracy while protecting modelprivacy. Applying ZKPs to Machine Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables independent validation of AI-generated content without revealing sensitive model information, promoting transparency and trust. ZKML enhances AIfairnessby providing cryptographicaudittrails for model predictions and ensuring uniform performance across users. We introduce snarkGPT, a practical ZKML implementation for transformers, to empower users to verify output accuracy and quality while preserving modelprivacy. We present a series of empirical results studying snarkGPT's scalability and performance to assess the feasibility and challenges of adopting a ZKML-powered approach to capture quality and performancefairnessproblems in generative AI models.",Accepted at PPAI-24: The 5th AAAI Workshop onPrivacy-Preserving Artificial Intelligence 2024,2024,feb,https://doi.org/10.48550/arXiv.2402.06414,\url{https://arxiv.org/pdf/2402.06414},ARTICLE,Ganescu2024,yes,
"SoK: Taming the Triangle -- On the Interplays betweenFairness, Interpretability andPrivacyin Machine Learning","Ferry, Julien AND AÃ¯vodji, Ulrich AND Gambs, SÃ©bastien AND Huguet, Marie-JosÃ© AND Siala, Mohamed","Machine learning techniques are increasingly used for high-stakes decision-making, such as college admissions, loan attribution or recidivism prediction. Thus, it is crucial to ensure that the models learnt can beauditedor understood by human users, do not create or reproduce discrimination or bias, and do not leak sensitive information regarding their training data. Indeed, interpretability,fairnessandprivacyare key requirements for the development of responsible machine learning, and all three have been studied extensively during the last decade. However, they were mainly considered in isolation, while in practice they interplay with each other, either positively or negatively. In this Systematization of Knowledge (SoK) paper, we survey the literature on the interactions between these three desiderata. More precisely, for each pairwise interaction, we summarize the identified synergies and tensions. These findings highlight several fundamental theoretical and empirical conflicts, while also demonstrating that jointly considering these different requirements is challenging when one aims at preserving a high level of utility. To solve this issue, we also discuss possible conciliation mechanisms, showing that a careful design can enable to successfully handle these different concerns in practice.",,2023,dec,https://doi.org/10.48550/arXiv.2312.16191,\url{https://arxiv.org/pdf/2312.16191},ARTICLE,Ferry2023,no,survey or review
VerifiableFairness:Privacy-preserving Computation ofFairnessfor Machine Learning Systems,"Toreini, Ehsan AND Mehrnezhad, Maryam AND Moorsel, van, Aad","Fairmachine learning is a thriving and vibrant research topic. In this paper, we proposeFairnessas a Service (FaaS), a secure, verifiable andprivacy-preserving protocol to computes and verify thefairnessof any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensureprivacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support variousfairnessmetrics; hence, it can be used as a service toauditthefairnessof any ML model. Our solution requires no trusted third party orprivatechannels for the computation of thefairnessmetric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g.,auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.","accepted in International Workshop onPrivate, Secure, and Trustworthy AI (PriST-AI), ESORICS'23 workshop",2023,sep,https://doi.org/10.48550/arXiv.2309.06061,\url{https://arxiv.org/pdf/2309.06061},ARTICLE,Toreini2023,yes,
APrivacy-Preserving Blockchain-based E-voting System,"Mukherjee, Arnab AND Majumdar, Souvik AND Kolya, Kumar, Anup AND Nandi, Saborni","Within a modern democratic nation, elections play a significant role in the nation's functioning. However, with the existing infrastructure for conducting elections using Electronic Voting Systems (EVMs), many loopholes exist, which illegitimate entities might leverage to cast false votes or even tamper with the EVMs after the voting session is complete. The need of the hour is to introduce a robust,auditable, transparent, and tamper-proof e-voting system, enabling a more reliable andfairelection process. To address such concerns, we propose a novel solution for blockchain-based e-voting, focusing on the security andprivacyaspects of the e-voting process. We consider the security risks and loopholes and aim to preserve the anonymity of the voters while ensuring that illegitimate votes are properly handled. Additionally, we develop a prototype as a proof of concept using the Ethereum blockchain platform. Finally, we perform experiments to demonstrate the performance of the system.",,2023,jul,https://doi.org/10.48550/arXiv.2307.08412,\url{https://arxiv.org/pdf/2307.08412},ARTICLE,Mukherjee2023,no,not auditing OF AI
Deploying ZKP Frameworks with Real-World Data: Challenges and Proposed Solutions,"Mallozzi, Piergiuseppe","Zero-knowledge proof (ZKP) frameworks have the potential to revolutionize the handling of sensitive data in various domains. However, deploying ZKP frameworks with real-world data presents several challenges, including scalability, usability, and interoperability. In this project, we present Fact Fortress, an end-to-end framework for designing and deploying zero-knowledge proofs of general statements. Our solution leverages proofs of data provenance andauditabledata access policies to ensure the trustworthiness of how sensitive data is handled and provide assurance of the computations that have been performed on it. ZKP is mostly associated with blockchain technology, where it enhances transactionprivacyand scalability through rollups, addressing the data inherent to the blockchain. Our approach focuses on safeguarding theprivacyof data external to the blockchain, with the blockchain serving as publiclyauditableinfrastructure to verify the validity of ZK proofs and track how data access has been granted without revealing the data itself. Additionally, our framework provides high-level abstractions that enable developers to express complex computations without worrying about the underlying arithmetic circuits and facilitates the deployment of on-chain verifiers. Although our approach demonstratedfairscalability for large datasets, there is still room for improvement, and further work is needed to enhance its scalability. By enabling on-chain verification of computation and data provenance without revealing any information about the data itself, our solution ensures the integrity of the computations on the data while preserving itsprivacy.",,2023,jul,https://doi.org/10.48550/arXiv.2307.06408,\url{https://arxiv.org/pdf/2307.06408},ARTICLE,Mallozzi2023,no,not auditing OF AI
Quantum PufferfishPrivacy: A FlexiblePrivacyFramework for Quantum Systems,"Nuradha, Theshani AND Goldfeld, Ziv AND Wilde, M., Mark","We propose a versatileprivacyframework for quantum systems, termed quantum pufferfishprivacy(QPP). Inspired by classical pufferfishprivacy, our formulation generalizes and addresses limitations of quantum differentialprivacyby offering flexibility in specifyingprivateinformation, feasible measurements, and domain knowledge. We show that QPP can be equivalently formulated in terms of the Datta-Leditzky information spectrum divergence, thus providing the first operational interpretation thereof. We reformulate this divergence as a semi-definite program and derive several properties of it, which are then used to prove convexity, composability, and post-processing of QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism are also derived. We analyze theprivacy-utility tradeoff of general QPP mechanisms and, again, study the depolarization mechanism as an explicit instance. The QPP framework is then applied toprivacyauditingfor identifyingprivacyviolations via a hypothesis testing pipeline that leverages quantum algorithms. Connections to quantumfairnessand other quantum divergences are also explored and several variants of QPP are examined.","v2: 33 pages, 9 figures, accepted to IEEE Transactions on Information Theory",2024,may,https://doi.org/10.48550/arXiv.2306.13054,\url{https://arxiv.org/pdf/2306.13054},ARTICLE,Nuradha2024,no,not auditing OF AI
"Datasheets for Machine Learning Sensors: Towards Transparency,Auditability, and Responsibility for Intelligent Sensing","Stewart, Matthew AND Warden, Pete AND Omri, Yasmine AND Prakash, Shvetank AND Santos, Joao AND Hymel, Shawn AND Brown, Benjamin AND MacArthur, Jim AND Jeffries, Nat AND Katti, Sachin AND Plancher, Brian AND Reddi, Janapa, Vijay","Machine learning (ML) sensors are enabling intelligence at the edge by empowering end-users with greater control over their data. ML sensors offer a new paradigm for sensing that moves the processing and analysis to the device itself rather than relying on the cloud, bringing benefits like lower latency and greater dataprivacy. The rise of these intelligent edge devices, while revolutionizing areas like the internet of things (IoT) and healthcare, also throws open critical questions aboutprivacy, security, and the opacity of AI decision-making. As ML sensors become more pervasive, it requires judicious governance regarding transparency, accountability, andfairness. To this end, we introduce a standard datasheet template for these ML sensors and discuss and evaluate the design and motivation for each section of the datasheet in detail including: standard dasheet components like the system's hardware specifications, IoT and AI components like the ML model and dataset attributes, as well as novel components like end-to-end performance metrics, and expanded environmental impact metrics. To provide a case study of the application of our datasheet template, we also designed and developed two examples for ML sensors performing computer vision-based person detection: one an open-source ML sensor designed and developed in-house, and a second commercial ML sensor developed by our industry collaborators. Together, ML sensors and their datasheets provide greaterprivacy, security, transparency, explainability,auditability, and user-friendliness for ML-enabled embedded systems. We conclude by emphasizing the need for standardization of datasheets across the broader ML community to ensure the responsible use of sensor data.",,2024,feb,https://doi.org/10.48550/arXiv.2306.08848,\url{https://arxiv.org/pdf/2306.08848},ARTICLE,Stewart2024,yes,
SEAL: A Strategy-Proof andPrivacy-Preserving UAV Computation Offloading Framework,"Wang, Yuntao AND Su, Zhou AND Luan, H., Tom AND Li, Jiliang AND Xu, Qichao AND Li, Ruidong","Due to the limited battery and computing resource, offloading unmanned aerial vehicles (UAVs)' computation tasks to ground infrastructure, e.g., vehicles, is a fundamental framework. Under such an open and untrusted environment, vehicles are reluctant to share their computing resource unless provisioning strong incentives,privacyprotection, andfairnessguarantee. Precisely, without strategy-proofness guarantee, the strategic vehicles can overclaim participation costs so as to conduct market manipulation. Without thefairnessprovision, vehicles can deliberately abort the assigned tasks without any punishments, and UAVs can refuse to pay by the end, causing an exchange dilemma. Lastly, the strategy-proofness andfairnessprovision typically require transparent payment/task results exchange under publicaudit, which may disclose sensitive information of vehicles and make theprivacypreservation a foremost issue. To achieve the three design goals, we propose SEAL, an integrated framework to address strategy-proof,fair, andprivacy-preserving UAV computation offloading. SEAL deploys a strategy-proof reverse combinatorial auction mechanism to optimize UAVs' task offloading under practical constraints while ensuring economic-robustness and polynomial-time efficiency. Based on smart contracts and hashchain micropayment, SEAL implements afairon-chain exchange protocol to realize the atomic completion of batch payments and computing results in multi-round auctions. In addition, aprivacy-preserving off-chain auction protocol is devised with the assistance of the trusted processor to efficiently protect vehicles' bidprivacy. Using rigorous theoretical analysis and extensive simulations, we validate that SEAL can effectively prevent vehicles from manipulating, ensureprivacyprotection andfairness, improve the offloading efficiency.",Accepted by IEEE Transactions on Information Forensics and Security (IEEE TIFS),2023,may,https://doi.org/10.48550/arXiv.2305.08691,\url{https://arxiv.org/pdf/2305.08691},ARTICLE,Wang2023,no,not auditing OF AI
"Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation","DÃ­az-RodrÃ­guez, Natalia AND Ser, Del, Javier AND Coeckelbergh, Mark AND Prado, de, LÃ³pez, Marcos AND Herrera-Viedma, Enrique AND Herrera, Francisco","Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety;privacyand data governance; transparency; diversity, non-discrimination andfairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a givenauditingprocess. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized throughauditingprocesses, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.","30 pages, 5 figures, under second review",2023,jun,https://doi.org/10.48550/arXiv.2305.02231,\url{https://arxiv.org/pdf/2305.02231},ARTICLE,DÃ­az-RodrÃ­guez2023,no,not auditing OF AI
PopSim: An Individual-level Population Simulator for Equitable Allocation of City Resources,"Nguyen, Duy, Khanh AND Shahbazi, Nima AND Asudeh, Abolfazl","Historical systematic exclusionary tactics based on race have forced people of certain demographic groups to congregate in specific urban areas. Aside from the ethical aspects of such segregation, these policies have implications for the allocation of urban resources including public transportation, healthcare, and education within the cities. The initial step towards addressing these issues involves conducting anauditto assess the status of equitable resource allocation. However, due toprivacyand confidentiality concerns, individual-level data containing demographic information cannot be made publicly available. By leveraging publicly available aggregated demographic statistics data, we introduce PopSim, a system for generating semi-synthetic individual-level population data with demographic information. We use PopSim to generate multiple benchmark datasets for the city of Chicago and conduct extensive statistical evaluations to validate those. We further use our datasets for several case studies that showcase the application of our system forauditingequitable allocation of city resources.","Published as part of the Workshop on AlgorithmicFairnessin Artificial Intelligence, Machine Learning, and Decision Making (AFair-AMLD) at the SIAM International Conference on Data Mining (SDM23)",2023,apr,https://doi.org/10.48550/arXiv.2305.02204,\url{https://arxiv.org/pdf/2305.02204},ARTICLE,Nguyen2023,no,not auditing OF AI
"Benchmark Dataset Dynamics, Bias andPrivacyChallenges in Voice Biometrics Research","Rusti, Casandra AND Leschanowsky, Anna AND Quinlan, Carolyn AND Pnacek, Michaela AND Gorce, Lauriane AND Hutiri, Wiebke","Speaker recognition is a widely used voice-based biometric technology with applications in various industries, including banking, education, recruitment, immigration, law enforcement, healthcare, and well-being. However, while dataset evaluations andauditshave improved data practices in face recognition and other computer vision tasks, the data practices in speaker recognition have gone largely unquestioned. Our research aims to address this gap by exploring how dataset usage has evolved over time and what implications this has on bias,fairnessandprivacyin speaker recognition systems. Previous studies have demonstrated the presence of historical, representation, and measurement biases in popular speaker recognition benchmarks. In this paper, we present a longitudinal study of speaker recognition datasets used for training and evaluation from 2012 to 2021. We survey close to 700 papers to investigate community adoption of datasets and changes in usage over a crucial time period where speaker recognition approaches transitioned to the widespread adoption of deep neural networks. Our study identifies the most commonly used datasets in the field, examines their usage patterns, and assesses their attributes that affect bias,fairness, and other ethical concerns. Our findings suggest areas for further research on the ethics andfairnessof speaker recognition technology.",8 pages (10 with References),2023,aug,https://doi.org/10.48550/arXiv.2304.03858,\url{https://arxiv.org/pdf/2304.03858},ARTICLE,Rusti2023,no,survey or review
Ethical Considerations for Responsible Data Curation,"Andrews, A., T., Jerone AND Zhao, Dora AND Thong, William AND Modas, Apostolos AND Papakyriakopoulos, Orestis AND Xiang, Alice","Human-centric computer vision (HCCV) data curation practices often neglectprivacyand bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensivefairnessand robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose,privacyand consent, and diversity, for curating HCCV evaluation datasets, addressingprivacyand bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, andaudits, to inform our considerations and recommendations.",NeurIPS 2023 Track on Datasets and Benchmarks (Oral),2023,dec,https://doi.org/10.48550/arXiv.2302.03629,\url{https://arxiv.org/pdf/2302.03629},ARTICLE,Andrews2023,no,not auditing OF AI
Much Ado About Gender: Current Practices and Future Recommendations for Appropriate Gender-Aware Information Access,"Pinney, Christine AND Raj, Amifa AND Hanna, Alex AND Ekstrand, D., Michael","Information access research (and development) sometimes makes use of gender, whether to report on the demographics of participants in a user study, as inputs to personalized results or recommendations, or to make systems gender-fair, amongst other purposes. This work makes a variety of assumptions about gender, however, that are not necessarily aligned with current understandings of what gender is, how it should be encoded, and how a gender variable should be ethically used. In this work, we present a systematic review of papers on information retrieval and recommender systems that mention gender in order to document how gender is currently being used in this field. We find that most papers mentioning gender do not use an explicit gender variable, but most of those that do either focus on contextualizing results of model performance, personalizing a system based on assumptions of user gender, orauditinga model's behavior forfairnessor otherprivacy-related issues. Moreover, most of the papers we review rely on a binary notion of gender, even if they acknowledge that gender cannot be split into two categories. We connect these findings with scholarship on gender theory and recent work on gender in human-computer interaction and natural language processing. We conclude by making recommendations for ethical and well-grounded use of gender in building and researching information access systems.",Published in CHIIR 2023,2023,jan,https://doi.org/10.48550/arXiv.2301.04780,\url{https://arxiv.org/pdf/2301.04780},ARTICLE,Pinney2023,no,survey or review
Can Querying for Bias Leak Protected Attributes? AchievingPrivacyWith Smooth Sensitivity,"Hamman, Faisal AND Chen, Jiahao AND Dutta, Sanghamitra","Existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting infairnessassessments on populations without knowing their protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset forauditingpurposes). However, the model developers might be allowed to test their models for bias by querying the compliance team for groupfairnessmetrics. In this paper, we first demonstrate that simply querying forfairnessmetrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. In particular, we show that one can reconstruct the protected attributes of all the individuals from O(Nk \log( n /Nk)) queries when Nk<<n using techniques from compressed sensing (n: size of the test dataset, Nk: size of smallest group). Our results pose an interesting debate in algorithmicfairness: should querying forfairnessmetrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations andprivacyif the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation, we also propose Attribute-Conceal, a novel technique that achieves differentialprivacyby calibrating noise to the smooth sensitivity of our bias query, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic data.","Published in 2023 ACM Conference onFairness, Accountability, and Transparency (FAccT2023)",2023,jun,https://doi.org/10.48550/arXiv.2211.02139,\url{https://arxiv.org/pdf/2211.02139},ARTICLE,Hamman2023,no,not auditing OF AI
A Comprehensive Survey on Trustworthy Recommender Systems,"Fan, Wenqi AND Zhao, Xiangyu AND Chen, Xiao AND Su, Jingran AND Gao, Jingtong AND Wang, Lin AND Liu, Qidong AND Wang, Yiqi AND Xu, Han AND Chen, Lei AND Li, Qing","As one of the most successful AI-powered applications, recommender systems aim to help people make appropriate decisions in an effective and efficient way, by providing personalized suggestions in many aspects of our lives, especially for various human-oriented online services such as e-commerce platforms and social media sites. In the past few decades, the rapid developments of recommender systems have significantly benefited human by creating economic value, saving time and effort, and promoting social good. However, recent studies have found that data-driven recommender systems can pose serious threats to users and society, such as spreading fake news to manipulate public opinion in social media sites, amplifying unfairness toward under-represented groups or individuals in job matching services, or inferringprivacyinformation from recommendation results. Therefore, systems' trustworthiness has been attracting increasing attention from various aspects for mitigating negative impacts caused by recommender systems, so as to enhance the public's trust towards recommender systems techniques. In this survey, we provide a comprehensive overview of Trustworthy Recommender systems (TRec) with a specific focus on six of the most important aspects; namely, Safety & Robustness, Nondiscrimination &Fairness, Explainability,Privacy, Environmental Well-being, and Accountability &Auditability. For each aspect, we summarize the recent related technologies and discuss potential research directions to help achieve trustworthy recommender systems in the future.",,2022,sep,https://doi.org/10.48550/arXiv.2209.10117,\url{https://arxiv.org/pdf/2209.10117},ARTICLE,Fan2022,no,survey or review
Having yourPrivacyCake and Eating it Too: Platform-supportedAuditingof Social Media Algorithms for Public Interest,"Imana, Basileal AND Korolova, Aleksandra AND Heidemann, John","Social media platforms curate access to information and opportunities, and so play a critical role in shaping public discourse today. The opaque nature of the algorithms these platforms use to curate content raises societal questions. Prior studies have used black-box methods to show that these algorithms can lead to biased or discriminatory outcomes. However, existingauditingmethods face fundamental limitations because they function independent of the platforms. Concerns of potential harm have prompted proposal of legislation in both the U.S. and the E.U. to mandate a new form ofauditingwhere vetted external researchers get privileged access to social media platforms. Unfortunately, to date there have been no concrete technical proposals to provide suchauditing, becauseauditingat scale risks disclosure of users'privatedata and platforms' proprietary algorithms. We propose a new method for platform-supportedauditingthat can meet the goals of the proposed legislation. Our first contribution is to enumerate the challenges of existingauditingmethods to implement these policies at scale. Second, we suggest that limited, privileged access to relevance estimators is the key to enabling generalizable platform-supportedauditingby external researchers. Third, we show platform-supportedauditingneed not risk userprivacynor disclosure of platforms' business interests by proposing anauditingframework that protects against these risks. For a particularfairnessmetric, we show that ensuringprivacyimposes only a small constant factor increase (6.34x as an upper bound, and 4x for typical parameters) in the number of samples required for accurateauditing. Our technical contributions, combined with ongoing legal and policy efforts, can enable public oversight into how social media platforms affect individuals and society by moving past theprivacy-vs-transparency hurdle.",,2023,mar,https://doi.org/10.48550/arXiv.2207.08773,\url{https://arxiv.org/pdf/2207.08773},ARTICLE,Imana2023,yes,
PASS: A ParameterAudit-based Secure andFairFederated Learning Scheme against Free-Rider Attack,"Wang, Jianhua AND Chang, Xiaolin AND MiÅ¡iÄ‡, Jelena AND MiÅ¡iÄ‡, B., Vojislav AND Wang, Yixiang","Federated Learning (FL) as a secure distributed learning framework gains interests in Internet of Things (IoT) due to its capability of protecting theprivacyof participant data. However, traditional FL systems are vulnerable to Free-Rider (FR) attacks, which causes unfairness,privacyleakage and inferior performance to FL systems. The prior defense mechanisms against FR attacks assumed that malicious clients (namely, adversaries) declare less than 50% of the total amount of clients. Moreover, they aimed for Anonymous FR (AFR) attacks and lost effectiveness in resisting Selfish FR (SFR) attacks. In this paper, we propose a ParameterAudit-based Secure andfairfederated learning Scheme (PASS) against FR attack. PASS has the following key features: (a) prevent fromprivacyleakage with less accuracy loss; (b) be effective in countering both AFR and SFR attacks; (c) work well no matter whether AFR and SFR adversaries occupy the majority of clients or not. Extensive experimental results validate that PASS: (a) has the same level as the State-Of-The-Art method in mean square error againstprivacyleakage; (b) defends against AFR and SFR attacks in terms of a higher defense success rate, lower false positive rate, and higher F1-score; (c) is still effective where adversaries exceed 50%, with F1-score 89% against AFR attack and F1-score 87% against SFR attack. Note that PASS produces no negative effect on FL accuracy when there is no FR adversary.","11 pages, 13 figures, 5 tables. Accepted by IoTJ. For associated file, see early access https://ieeexplore.ieee.org/document/10160204",2023,jun,https://doi.org/10.48550/arXiv.2207.07292,\url{https://arxiv.org/pdf/2207.07292},ARTICLE,Wang2023,no,is this a FL sheme even though it is called parameter audit
PrivFair: a Library forPrivacy-PreservingFairnessAuditing,"Pentyala, Sikha AND Melanson, David AND Cock, De, Martine AND Farnadi, Golnoosh","Machine learning (ML) has become prominent in applications that directly affect people's quality of life, including in healthcare, justice, and finance. ML models have been found to exhibit discrimination based on sensitive attributes such as gender, race, or disability. Assessing if an ML model is free of bias remains challenging to date, and by definition has to be done with sensitive user characteristics that are subject of anti-discrimination and data protection law. Existing libraries forfairnessauditingof ML models offer no mechanism to protect theprivacyof theauditdata. We present PrivFair, a library forprivacy-preservingfairnessauditsof ML models. Through the use of Secure Multiparty Computation (MPC), PrivFair protects the confidentiality of the model underauditand the sensitive data used for theaudit, hence it supports scenarios in which a proprietary classifier owned by a company isauditedusing sensitiveauditdata from an external investigator. We demonstrate the use of PrivFair for groupfairnessauditingwith tabular data or image data, without requiring the investigator to disclose their data to anyone in an unencrypted manner, or the model owner to reveal their model parameters to anyone in plaintext.",,2022,may,https://doi.org/10.48550/arXiv.2202.04058,\url{https://arxiv.org/pdf/2202.04058},ARTICLE,Pentyala2022,yes,
Dikaios:PrivacyAuditingof AlgorithmicFairnessvia Attribute Inference Attacks,"Aalmoes, Jan AND Duddu, Vasisht AND Boutet, Antoine","Machine learning (ML) models have been deployed for high-stakes applications. Due to class imbalance in the sensitive attribute observed in the datasets, ML models are unfair on minority subgroups identified by a sensitive attribute, such as race and sex. In-processingfairnessalgorithms ensure model predictions are independent of sensitive attribute. Furthermore, ML models are vulnerable to attribute inference attacks where an adversary can identify the values of sensitive attribute by exploiting their distinguishable model predictions. Despiteprivacyandfairnessbeing important pillars of trustworthy ML, theprivacyrisk introduced byfairnessalgorithms with respect to attribute leakage has not been studied. We identify attribute inference attacks as an effective measure forauditingblackboxfairnessalgorithms to enable model builder to account forprivacyandfairnessin the model design. We proposed Dikaios, aprivacyauditingtool forfairnessalgorithms for model builders which leveraged a new effective attribute inference attack that account for the class imbalance in sensitive attributes through an adaptive prediction threshold. We evaluated Dikaios to perform aprivacyauditof two in-processingfairnessalgorithms over five datasets. We show that our attribute inference attacks with adaptive prediction threshold significantly outperform prior attacks. We highlighted the limitations of in-processingfairnessalgorithms to ensure indistinguishable predictions across different values of sensitive attributes. Indeed, the attributeprivacyrisk of these in-processingfairnessschemes is highly variable according to the proportion of the sensitive attributes in the dataset. This unpredictable effect offairnessmechanisms on the attributeprivacyrisk is an important limitation on their utilization which has to be accounted by the model builder.",The paper's results and conclusions underwent significant changes. The updated paper can be found at arXiv:2211.10209,2022,nov,https://doi.org/10.48550/arXiv.2202.02242,\url{},ARTICLE,Aalmoes2022,yes,
Property Inference Attacks Against GANs,"Zhou, Junhao AND Chen, Yufei AND Shen, Chao AND Zhang, Yang","While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security andprivacyattacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security andprivacyrisks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN's training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as afairnessauditorto check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.",To Appear in NDSS 2022,2021,nov,https://doi.org/10.48550/arXiv.2111.07608,\url{https://arxiv.org/pdf/2111.07608},ARTICLE,Zhou2021,no,not auditing OF AI
Proposing an InteractiveAuditPipeline for VisualPrivacyResearch,"DeHart, Jasmine AND Xu, Chenguang AND Egede, Lisa AND Grant, Christan","In an ideal world, deployed machine learning models will enhance our society. We hope that those models will provide unbiased and ethical decisions that will benefit everyone. However, this is not always the case; issues arise during the data preparation process throughout the steps leading to the models' deployment. The continued use of biased datasets and processes will adversely damage communities and increase the cost of fixing the problem later. In this work, we walk through the decision-making process that a researcher should consider before, during, and after a system deployment to understand the broader impacts of their research in the community. Throughout this paper, we discussfairness,privacy, and ownership issues in the machine learning pipeline; we assert the need for a responsible human-over-the-loop methodology to bring accountability into the machine learning pipeline, and finally, reflect on the need to explore research agendas that have harmful societal impacts. We examine visualprivacyresearch and draw lessons that can apply broadly to artificial intelligence. Our goal is to systematically analyze the machine learning pipeline for visualprivacyand bias issues. We hope to raise stakeholder (e.g., researchers, modelers, corporations) awareness as these issues propagate in this pipeline's various machine learning phases.","Extended version of IEEE BigData 2021 Short Paper, 14 pages, grammar edits",2021,nov,https://doi.org/10.48550/arXiv.2111.03984,\url{https://arxiv.org/pdf/2111.03984},ARTICLE,DeHart2021,yes,
Trustworthy AI: A Computational Perspective,"Liu, Haochen AND Wang, Yiqi AND Fan, Wenqi AND Liu, Xiaorui AND Li, Yaxin AND Jain, Shaili AND Liu, Yunhao AND Jain, K., Anil AND Tang, Jiliang","In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention of developing AI is to benefit humans, by reducing human labor, bringing everyday convenience to human lives, and promoting social good. However, recent research and AI applications show that AI can cause unintentional harm to humans, such as making unreliable decisions in safety-critical scenarios or underminingfairnessby inadvertently discriminating against one group. Thus, trustworthy AI has attracted immense attention recently, which requires careful consideration to avoid the adverse effects that AI may bring to humans, so that humans can fully trust and live in harmony with AI technologies.
  Recent years have witnessed a tremendous amount of research on trustworthy AI. In this survey, we present a comprehensive survey of trustworthy AI from a computational perspective, to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex area, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Non-discrimination &Fairness, (iii) Explainability, (iv)Privacy, (v) Accountability &Auditability, and (vi) Environmental Well-Being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",55 pages,2021,aug,https://doi.org/10.48550/arXiv.2107.06641,\url{https://arxiv.org/pdf/2107.06641},ARTICLE,Liu2021,no,survey or review
When the Umpire is also a Player: Bias inPrivateLabel Product Recommendations on E-commerce Marketplaces,"Dash, Abhisek AND Chakraborty, Abhijnan AND Ghosh, Saptarshi AND Mukherjee, Animesh AND Gummadi, P., Krishna","Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about thefairnessof black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own `privatelabel' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace `organic' recommendations with ad-driven `sponsored' recommendations, which include their ownprivatelabels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any publicauditof these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematicauditof related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazonprivatelabel products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","This work has been accepted for presentation at the ACM Conference onFairness, Accountability, and Transparency 2021 (ACM FAccT 2021)",2021,feb,https://doi.org/10.48550/arXiv.2102.00141,\url{https://arxiv.org/pdf/2102.00141},ARTICLE,Dash2021,no,
TowardsPrivacy-assured and Lightweight On-chainAuditingof Decentralized Storage,"Du, Yuefeng AND Duan, Huayi AND Zhou, Anxin AND Wang, Cong AND Au, Ho, Man AND Wang, Qian","How toauditoutsourced data in centralized storage like cloud is well-studied, but it is largely under-explored for the rising decentralized storage network (DSN) that bodes well for a billion-dollar market. To realize DSN as a usable service in a truly decentralized manner, the blockchain comes in handy -- to record and verifyaudittrails in forms of proof of storage, and based on that, to handlefairpayments with necessary dispute resolution.
  Leaving theaudittrails on the blockchain offers transparency andfairness, yet it 1) sacrificesprivacy, as they may leak information about the data underaudit, and 2) overwhelms on-chain resources, as they may be practically large in size and expensive to verify. Priorauditingdesigns in centralized settings are not directly applicable here. A handful of proposals targeting DSN cannot satisfactorily address these issues either.
  We present anauditingsolution that addresses on-chainprivacyand efficiency, from a synergy of homomorphic linear authenticators with polynomial commitments for succinct proofs, and the sigma protocol for provableprivacy. The solution results in, peraudit, 288-byte proof written to the blockchain, and constant verification cost. It can sustain long-term operation and easily scale to thousands of users on Ethereum.",,2020,aug,https://doi.org/10.48550/arXiv.2005.05531,\url{https://arxiv.org/pdf/2005.05531},ARTICLE,Du2020,no,blockchain
Mechanism Design and Blockchains,"Mamageishvili, Akaki AND Schlegel, Christoph, Jan","Game theory is often used as a tool to analyze decentralized systems and their properties, in particular, blockchains. In this note, we take the opposite view. We argue that blockchains can and should be used to implement economic mechanisms because they can help to overcome problems that occur if trust in the mechanism designer cannot be assumed. Mechanism design deals with the allocation of resources to agents, often by extractingprivateinformation from them. Some mechanisms are immune to early information disclosure, while others may heavily depend on it. Some mechanisms have to randomize to achievefairnessand efficiency. Both issues, information disclosure, and randomness require trust in the mechanism designer. If there is no trust, mechanisms can be manipulated. We claim that mechanisms that use randomness or sequential information disclosure are much harder, if not impossible, toaudit. Therefore, centralized implementation is often not a good solution. We consider some of the most frequently used mechanisms in practice and identify circumstances under which manipulation is possible. We propose a decentralized implementation of such mechanisms, that can be, in practical terms, realized by blockchain technology. Moreover, we argue in which environments a decentralized implementation of a mechanism brings a significant advantage.",,2020,oct,https://doi.org/10.48550/arXiv.2005.02390,\url{https://arxiv.org/pdf/2005.02390},ARTICLE,Mamageishvili2020,no,blockchain
"PASTRAMI:Privacy-preserving,Auditable, Scalable & Trustworthy Auctions for Multiple Items","KrÃ³l, MichaÅ‚ AND Sonnino, Alberto AND Tasiopoulos, Argyrios AND Psaras, Ioannis AND RiviÃ¨re, Etienne","Decentralised cloud computing platforms enable individuals to offer and rent resources in a peer-to-peer fashion. They must assign resources from multiple sellers to multiple buyers and derive prices that match the interests and capacities of both parties. The assignment process must be decentralised,fairand transparent, but also protect theprivacyof buyers. We present PASTRAMI, a decentralised platform enabling trustworthy assignments of items and prices between a large number of sellers and bidders, through the support of multi-item auctions. PASTRAMI uses threshold blind signatures and commitment schemes to provide strongprivacyguarantees while making bidders accountable. It leverages the Ethereum blockchain forauditability, combining efficient off-chain computations with novel, on-chain proofs of misbehaviour. Our evaluation of PASTRAMI using Filecoin workloads show its ability to efficiently produce trustworthy assignments between thousands of buyers and sellers.",,2020,dec,https://doi.org/10.48550/arXiv.2004.06403,\url{https://arxiv.org/pdf/2004.06403},ARTICLE,KrÃ³l2020,no,not auditing OF AI
Audita: A Blockchain-basedAuditingFramework for Off-chain Storage,"Francati, Danilo AND Ateniese, Giuseppe AND Faye, Abdoulaye AND Milazzo, Maria, Andrea AND Perillo, Massimo, Angelo AND Schiatti, Luca AND Giordano, Giuseppe","The cloud changed the way we manage and store data. Today, cloud storage services offer clients an infrastructure that allows them a convenient source to store, replicate, and secure data online. However, with these new capabilities also come limitations, such as lack of transparency, limited decentralization, and challenges withprivacyand security. And, as the need for more agile,privateand secure data solutions continues to grow exponentially, rethinking the current structure of cloud storage is mission-critical for enterprises. By leveraging and building upon blockchain's unique attributes, including immutability, security to the data element level, distributed (no single point of failure), we have developed a solution prototype that allows data to be reliably stored while simultaneously being secured, with tamper-evidentauditability, via blockchain. The result,Audita, is a flexible solution that assures data protection and solves challenges such as scalability andprivacy.Auditaworks via an augmented blockchain network of participants that include storage-nodes and block-creators. In addition, it provides an automatic andfairchallenge system to assure that data is distributed and reliably and provably stored. While the prototype is built on Quorum, the solution framework can be used with any blockchain platform. The benefit is a system that is built to grow along with the data needs of enterprises, while continuing to build the network via incentives and solving for issues such asauditingand outsourcing.",,2020,jul,https://doi.org/10.48550/arXiv.1911.08515,\url{https://arxiv.org/pdf/1911.08515},ARTICLE,Francati2020,no,not auditing OF AI
A Blockchain Framework for Managing and Monitoring Data in Multi-Site Clinical Trials,"Choudhury, Olivia AND Fairoza, Noor AND Sylla, Issa AND Das, Amar","The cost of conducting multi-site clinical trials has significantly increased over time, with site monitoring, data management, and amendments being key drivers. Clinical trial data management approaches typically rely on a central database, and require manual efforts to encode and maintain data capture and reporting requirements. To reduce the administrative burden, time, and effort of ensuring data integrity andprivacyin multi-site trials, we propose a novel data management framework based on permissioned blockchain technology. We demonstrate how our framework, which uses smart contracts andprivatechannels, enables confidential data communication, protocol enforcement, and and an automatedaudittrail. We compare this framework with the traditional data management approach and evaluate its effectiveness in satisfying the major requirements of multi-site clinical trials. We show that our framework ensures enforcement of IRB-related regulatory requirements across multiple sites and stakeholders.",,2019,feb,https://doi.org/10.48550/arXiv.1902.03975,\url{https://arxiv.org/pdf/1902.03975},ARTICLE,Choudhury2019,no,not auditing OF AI
PBF: A NewPrivacy-Aware Billing Framework for Online Electric Vehicles with BidirectionalAuditability,"Hussain, Rasheed AND Kim, Donghyun AND Nogueira, Michele AND Son, Junggab AND Tokuta, O., Alade AND Oh, Heekuck","Recently an online electric vehicle (OLEV) concept has been introduced, where vehicles are propelled through the wirelessly transmitted electrical power from the infrastructure installed under the road while moving. The absence of secure-and-fairbilling is one main hurdle to widely adopt this promising technology. This paper introduces a secure andprivacy-awarefairbilling framework for OLEV on the move through the charging plates installed under the road. We first propose two extreme lightweight mutual authentication mechanisms, a direct authentication and a hash chain-based authentication between vehicles and the charging plates that can be used for different vehicular speeds on the road. Second we propose a secure andprivacy-aware wireless power transfer on move for the vehicles with bidirectionalauditabilityguarantee by leveraging game-theoretic approach. Each charging plate transfers a fixed amount of energy to the vehicle and bills the vehicle in aprivacy-aware way accordingly. Our protocol guarantees secure,privacy-aware, andfairbilling mechanism for the OLEVs while receiving electric power from the road. Moreover our proposed framework can play a vital role in eliminating the security andprivacychallenges in the deployment of power transfer technology to the OLEVs.","13 pages, 7 figures",2015,apr,https://doi.org/10.48550/arXiv.1504.05276,\url{https://arxiv.org/pdf/1504.05276},ARTICLE,Hussain2015,no,not auditing OF AI
